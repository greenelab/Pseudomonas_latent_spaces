{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with 1-hidden layer\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# To ensure reproducibility using Keras during development\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "randomState = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train_model_input.txt.xz\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0, header=0, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "epochs = 500\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_1layer_{}latent_stats1.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_1layer_{}latent_hist1.png\".format(latent_dim))\n",
    "\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"train_input_1layer_{}latent_encoded1.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}latent_encoder_model1.h5\".format(latent_dim))\n",
    "weights_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}latent_encoder_weights1.h5\".format(latent_dim))\n",
    "model_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}latent_decoder_model1.h5\".format(latent_dim))\n",
    "weights_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}latent_decoder_weights1.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent, random_state = randomState)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:66: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_to_reconstruct = Dense(original_dim,\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               activation='sigmoid')\n",
    "rnaseq_reconstruct = decoder_to_reconstruct(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1065 samples, validate on 118 samples\n",
      "Epoch 1/500\n",
      "1065/1065 [==============================] - 1s 762us/step - loss: 3830.9592 - val_loss: 3777.0667\n",
      "Epoch 2/500\n",
      "1065/1065 [==============================] - 1s 690us/step - loss: 3785.0205 - val_loss: 3751.1258\n",
      "Epoch 3/500\n",
      "1065/1065 [==============================] - 1s 673us/step - loss: 3740.3942 - val_loss: 3725.9802\n",
      "Epoch 4/500\n",
      "1065/1065 [==============================] - 1s 629us/step - loss: 3698.9803 - val_loss: 3701.1059\n",
      "Epoch 5/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3672.8605 - val_loss: 3706.9730\n",
      "Epoch 6/500\n",
      "1065/1065 [==============================] - 1s 626us/step - loss: 3647.5049 - val_loss: 3662.2820\n",
      "Epoch 7/500\n",
      "1065/1065 [==============================] - 1s 642us/step - loss: 3621.8589 - val_loss: 3636.2557\n",
      "Epoch 8/500\n",
      "1065/1065 [==============================] - 1s 625us/step - loss: 3602.7168 - val_loss: 3574.2886\n",
      "Epoch 9/500\n",
      "1065/1065 [==============================] - 1s 622us/step - loss: 3591.3344 - val_loss: 3599.2767\n",
      "Epoch 10/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3575.0879 - val_loss: 3555.2124\n",
      "Epoch 11/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3563.5591 - val_loss: 3528.3238\n",
      "Epoch 12/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3552.9225 - val_loss: 3524.9054\n",
      "Epoch 13/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3544.8739 - val_loss: 3543.6111\n",
      "Epoch 14/500\n",
      "1065/1065 [==============================] - 1s 661us/step - loss: 3539.3321 - val_loss: 3567.1200\n",
      "Epoch 15/500\n",
      "1065/1065 [==============================] - 1s 685us/step - loss: 3532.4922 - val_loss: 3532.4629\n",
      "Epoch 16/500\n",
      "1065/1065 [==============================] - 1s 659us/step - loss: 3525.6909 - val_loss: 3564.9576\n",
      "Epoch 17/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3517.8331 - val_loss: 3602.4099\n",
      "Epoch 18/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3516.8778 - val_loss: 3555.4715\n",
      "Epoch 19/500\n",
      "1065/1065 [==============================] - 1s 699us/step - loss: 3511.6765 - val_loss: 3525.0221\n",
      "Epoch 20/500\n",
      "1065/1065 [==============================] - 1s 654us/step - loss: 3505.6031 - val_loss: 3503.2595\n",
      "Epoch 21/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3502.9192 - val_loss: 3513.8424\n",
      "Epoch 22/500\n",
      "1065/1065 [==============================] - 1s 660us/step - loss: 3500.3860 - val_loss: 3496.4627\n",
      "Epoch 23/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3492.2803 - val_loss: 3491.6993\n",
      "Epoch 24/500\n",
      "1065/1065 [==============================] - 1s 596us/step - loss: 3493.2523 - val_loss: 3476.8464\n",
      "Epoch 25/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3491.6823 - val_loss: 3470.0410\n",
      "Epoch 26/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3484.8266 - val_loss: 3473.7513\n",
      "Epoch 27/500\n",
      "1065/1065 [==============================] - 1s 617us/step - loss: 3483.6436 - val_loss: 3471.4862\n",
      "Epoch 28/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3482.0556 - val_loss: 3487.2277\n",
      "Epoch 29/500\n",
      "1065/1065 [==============================] - 1s 634us/step - loss: 3476.9485 - val_loss: 3466.2351\n",
      "Epoch 30/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3477.3299 - val_loss: 3462.7486\n",
      "Epoch 31/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3471.2493 - val_loss: 3469.5025\n",
      "Epoch 32/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3466.0215 - val_loss: 3460.8067\n",
      "Epoch 33/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3466.6748 - val_loss: 3448.7445\n",
      "Epoch 34/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3467.8341 - val_loss: 3451.8075\n",
      "Epoch 35/500\n",
      "1065/1065 [==============================] - 1s 617us/step - loss: 3464.9369 - val_loss: 3461.6124\n",
      "Epoch 36/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3465.5373 - val_loss: 3458.2026\n",
      "Epoch 37/500\n",
      "1065/1065 [==============================] - 1s 615us/step - loss: 3462.0545 - val_loss: 3463.3812\n",
      "Epoch 38/500\n",
      "1065/1065 [==============================] - 1s 642us/step - loss: 3459.4973 - val_loss: 3457.5096\n",
      "Epoch 39/500\n",
      "1065/1065 [==============================] - 1s 625us/step - loss: 3459.3767 - val_loss: 3442.8798\n",
      "Epoch 40/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3456.2007 - val_loss: 3453.2875\n",
      "Epoch 41/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3455.6419 - val_loss: 3452.0916\n",
      "Epoch 42/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3454.2673 - val_loss: 3448.6541\n",
      "Epoch 43/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3451.2008 - val_loss: 3440.2479\n",
      "Epoch 44/500\n",
      "1065/1065 [==============================] - 1s 648us/step - loss: 3451.2656 - val_loss: 3457.4589\n",
      "Epoch 45/500\n",
      "1065/1065 [==============================] - 1s 642us/step - loss: 3451.6913 - val_loss: 3447.4616\n",
      "Epoch 46/500\n",
      "1065/1065 [==============================] - 1s 640us/step - loss: 3449.9904 - val_loss: 3446.1160\n",
      "Epoch 47/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3448.2179 - val_loss: 3454.9989\n",
      "Epoch 48/500\n",
      "1065/1065 [==============================] - 1s 621us/step - loss: 3446.0119 - val_loss: 3444.4218\n",
      "Epoch 49/500\n",
      "1065/1065 [==============================] - 1s 643us/step - loss: 3444.8350 - val_loss: 3434.0811\n",
      "Epoch 50/500\n",
      "1065/1065 [==============================] - 1s 638us/step - loss: 3443.8750 - val_loss: 3436.3973\n",
      "Epoch 51/500\n",
      "1065/1065 [==============================] - 1s 640us/step - loss: 3445.2482 - val_loss: 3435.4913\n",
      "Epoch 52/500\n",
      "1065/1065 [==============================] - 1s 654us/step - loss: 3444.1161 - val_loss: 3434.4788\n",
      "Epoch 53/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3442.1492 - val_loss: 3430.8132\n",
      "Epoch 54/500\n",
      "1065/1065 [==============================] - 1s 629us/step - loss: 3440.0187 - val_loss: 3431.4690\n",
      "Epoch 55/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3439.3363 - val_loss: 3434.7136\n",
      "Epoch 56/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3438.6061 - val_loss: 3437.8838\n",
      "Epoch 57/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3437.5446 - val_loss: 3425.5447\n",
      "Epoch 58/500\n",
      "1065/1065 [==============================] - 1s 615us/step - loss: 3438.4564 - val_loss: 3433.3084\n",
      "Epoch 59/500\n",
      "1065/1065 [==============================] - 1s 635us/step - loss: 3437.0871 - val_loss: 3437.1784\n",
      "Epoch 60/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3434.8968 - val_loss: 3435.0123\n",
      "Epoch 61/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3434.6919 - val_loss: 3426.9310\n",
      "Epoch 62/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3433.2197 - val_loss: 3426.7018\n",
      "Epoch 63/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3432.1491 - val_loss: 3433.6408\n",
      "Epoch 64/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3430.3997 - val_loss: 3427.4846\n",
      "Epoch 65/500\n",
      "1065/1065 [==============================] - 1s 629us/step - loss: 3431.3941 - val_loss: 3424.0952\n",
      "Epoch 66/500\n",
      "1065/1065 [==============================] - 1s 631us/step - loss: 3429.5702 - val_loss: 3418.0791\n",
      "Epoch 67/500\n",
      "1065/1065 [==============================] - 1s 637us/step - loss: 3428.5327 - val_loss: 3419.5993\n",
      "Epoch 68/500\n",
      "1065/1065 [==============================] - 1s 625us/step - loss: 3429.0768 - val_loss: 3436.9412\n",
      "Epoch 69/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3429.0191 - val_loss: 3423.0055\n",
      "Epoch 70/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3427.8337 - val_loss: 3420.2875\n",
      "Epoch 71/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3427.4697 - val_loss: 3426.7255\n",
      "Epoch 72/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3425.5210 - val_loss: 3420.9910\n",
      "Epoch 73/500\n",
      "1065/1065 [==============================] - 1s 617us/step - loss: 3424.4014 - val_loss: 3418.3882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/500\n",
      "1065/1065 [==============================] - 1s 644us/step - loss: 3425.5200 - val_loss: 3414.6533\n",
      "Epoch 75/500\n",
      "1065/1065 [==============================] - 1s 645us/step - loss: 3423.7450 - val_loss: 3417.0832\n",
      "Epoch 76/500\n",
      "1065/1065 [==============================] - 1s 647us/step - loss: 3423.7747 - val_loss: 3426.7991\n",
      "Epoch 77/500\n",
      "1065/1065 [==============================] - 1s 655us/step - loss: 3422.0067 - val_loss: 3417.0408\n",
      "Epoch 78/500\n",
      "1065/1065 [==============================] - 1s 624us/step - loss: 3421.6522 - val_loss: 3413.3907\n",
      "Epoch 79/500\n",
      "1065/1065 [==============================] - 1s 630us/step - loss: 3421.9631 - val_loss: 3419.4811\n",
      "Epoch 80/500\n",
      "1065/1065 [==============================] - 1s 639us/step - loss: 3421.9721 - val_loss: 3416.7973\n",
      "Epoch 81/500\n",
      "1065/1065 [==============================] - 1s 657us/step - loss: 3421.7315 - val_loss: 3419.8304\n",
      "Epoch 82/500\n",
      "1065/1065 [==============================] - 1s 641us/step - loss: 3419.2361 - val_loss: 3415.8596\n",
      "Epoch 83/500\n",
      "1065/1065 [==============================] - 1s 634us/step - loss: 3420.5217 - val_loss: 3410.3405\n",
      "Epoch 84/500\n",
      "1065/1065 [==============================] - 1s 655us/step - loss: 3417.4117 - val_loss: 3415.5117\n",
      "Epoch 85/500\n",
      "1065/1065 [==============================] - 1s 631us/step - loss: 3418.0795 - val_loss: 3416.4883\n",
      "Epoch 86/500\n",
      "1065/1065 [==============================] - 1s 667us/step - loss: 3417.9353 - val_loss: 3419.3621\n",
      "Epoch 87/500\n",
      "1065/1065 [==============================] - 1s 655us/step - loss: 3416.2043 - val_loss: 3420.6388\n",
      "Epoch 88/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3416.1114 - val_loss: 3415.7771\n",
      "Epoch 89/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3415.5960 - val_loss: 3410.3548\n",
      "Epoch 90/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3415.8995 - val_loss: 3410.5251\n",
      "Epoch 91/500\n",
      "1065/1065 [==============================] - 1s 708us/step - loss: 3415.2599 - val_loss: 3416.4921\n",
      "Epoch 92/500\n",
      "1065/1065 [==============================] - 1s 627us/step - loss: 3412.8396 - val_loss: 3417.0240\n",
      "Epoch 93/500\n",
      "1065/1065 [==============================] - 1s 669us/step - loss: 3414.4059 - val_loss: 3414.8667\n",
      "Epoch 94/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3413.9804 - val_loss: 3408.6314\n",
      "Epoch 95/500\n",
      "1065/1065 [==============================] - 1s 600us/step - loss: 3412.9562 - val_loss: 3406.1489\n",
      "Epoch 96/500\n",
      "1065/1065 [==============================] - 1s 688us/step - loss: 3412.4185 - val_loss: 3404.9590\n",
      "Epoch 97/500\n",
      "1065/1065 [==============================] - 1s 652us/step - loss: 3411.7497 - val_loss: 3410.8461\n",
      "Epoch 98/500\n",
      "1065/1065 [==============================] - 1s 617us/step - loss: 3412.9338 - val_loss: 3409.9334\n",
      "Epoch 99/500\n",
      "1065/1065 [==============================] - 1s 578us/step - loss: 3410.2625 - val_loss: 3408.7511\n",
      "Epoch 100/500\n",
      "1065/1065 [==============================] - 1s 587us/step - loss: 3411.4190 - val_loss: 3406.6651\n",
      "Epoch 101/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3410.2862 - val_loss: 3411.2392\n",
      "Epoch 102/500\n",
      "1065/1065 [==============================] - 1s 653us/step - loss: 3409.2162 - val_loss: 3418.1548\n",
      "Epoch 103/500\n",
      "1065/1065 [==============================] - 1s 676us/step - loss: 3410.5946 - val_loss: 3415.4555\n",
      "Epoch 104/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3409.1762 - val_loss: 3408.8818\n",
      "Epoch 105/500\n",
      "1065/1065 [==============================] - 1s 622us/step - loss: 3410.4159 - val_loss: 3409.8199\n",
      "Epoch 106/500\n",
      "1065/1065 [==============================] - 1s 634us/step - loss: 3408.5508 - val_loss: 3405.6730\n",
      "Epoch 107/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3408.7023 - val_loss: 3411.0532\n",
      "Epoch 108/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3408.1036 - val_loss: 3407.2318\n",
      "Epoch 109/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3408.0114 - val_loss: 3412.4718\n",
      "Epoch 110/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3407.0996 - val_loss: 3411.7121\n",
      "Epoch 111/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3406.4350 - val_loss: 3407.6866\n",
      "Epoch 112/500\n",
      "1065/1065 [==============================] - 1s 625us/step - loss: 3407.2342 - val_loss: 3408.1077\n",
      "Epoch 113/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3405.9725 - val_loss: 3403.8629\n",
      "Epoch 114/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3407.6872 - val_loss: 3402.6251\n",
      "Epoch 115/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3406.3438 - val_loss: 3402.7882\n",
      "Epoch 116/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3404.1990 - val_loss: 3397.5532\n",
      "Epoch 117/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3405.5806 - val_loss: 3402.5680\n",
      "Epoch 118/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3404.3498 - val_loss: 3398.6733\n",
      "Epoch 119/500\n",
      "1065/1065 [==============================] - 1s 626us/step - loss: 3404.3185 - val_loss: 3401.1572\n",
      "Epoch 120/500\n",
      "1065/1065 [==============================] - 1s 635us/step - loss: 3403.8242 - val_loss: 3397.2605\n",
      "Epoch 121/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3404.0970 - val_loss: 3396.4424\n",
      "Epoch 122/500\n",
      "1065/1065 [==============================] - 1s 601us/step - loss: 3403.6314 - val_loss: 3400.7985\n",
      "Epoch 123/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3403.0859 - val_loss: 3397.4014\n",
      "Epoch 124/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3402.7154 - val_loss: 3404.9108\n",
      "Epoch 125/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3403.6472 - val_loss: 3397.9686\n",
      "Epoch 126/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3403.1419 - val_loss: 3400.1768\n",
      "Epoch 127/500\n",
      "1065/1065 [==============================] - 1s 634us/step - loss: 3402.3480 - val_loss: 3397.8718\n",
      "Epoch 128/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3401.7626 - val_loss: 3399.3431\n",
      "Epoch 129/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3401.8268 - val_loss: 3394.3327\n",
      "Epoch 130/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3401.0359 - val_loss: 3398.0148\n",
      "Epoch 131/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3400.3868 - val_loss: 3399.2988\n",
      "Epoch 132/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3399.6335 - val_loss: 3396.4215\n",
      "Epoch 133/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3401.5905 - val_loss: 3395.5923\n",
      "Epoch 134/500\n",
      "1065/1065 [==============================] - 1s 635us/step - loss: 3399.7822 - val_loss: 3397.4002\n",
      "Epoch 135/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3399.9296 - val_loss: 3392.0953\n",
      "Epoch 136/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3397.3644 - val_loss: 3398.0717\n",
      "Epoch 137/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3400.5911 - val_loss: 3393.6046\n",
      "Epoch 138/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3399.7400 - val_loss: 3393.8625\n",
      "Epoch 139/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3399.8882 - val_loss: 3396.6934\n",
      "Epoch 140/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3400.0182 - val_loss: 3398.7622\n",
      "Epoch 141/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3398.3981 - val_loss: 3398.9237\n",
      "Epoch 142/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3398.8431 - val_loss: 3394.1827\n",
      "Epoch 143/500\n",
      "1065/1065 [==============================] - 1s 615us/step - loss: 3398.1231 - val_loss: 3395.7275\n",
      "Epoch 144/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3397.7888 - val_loss: 3403.0527\n",
      "Epoch 145/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3398.9913 - val_loss: 3392.2407\n",
      "Epoch 146/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065/1065 [==============================] - 1s 607us/step - loss: 3397.0853 - val_loss: 3399.4228\n",
      "Epoch 147/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3397.4280 - val_loss: 3398.1810\n",
      "Epoch 148/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3396.8586 - val_loss: 3398.0953\n",
      "Epoch 149/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3395.0773 - val_loss: 3393.2169\n",
      "Epoch 150/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3396.6806 - val_loss: 3392.1510\n",
      "Epoch 151/500\n",
      "1065/1065 [==============================] - 1s 637us/step - loss: 3397.0449 - val_loss: 3393.5653\n",
      "Epoch 152/500\n",
      "1065/1065 [==============================] - 1s 625us/step - loss: 3395.2159 - val_loss: 3393.0224\n",
      "Epoch 153/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3395.6230 - val_loss: 3392.3598\n",
      "Epoch 154/500\n",
      "1065/1065 [==============================] - 1s 615us/step - loss: 3394.7651 - val_loss: 3393.7542\n",
      "Epoch 155/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3396.6621 - val_loss: 3391.2623\n",
      "Epoch 156/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3395.1383 - val_loss: 3389.9796\n",
      "Epoch 157/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3392.9792 - val_loss: 3391.2605\n",
      "Epoch 158/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3394.5459 - val_loss: 3393.2040\n",
      "Epoch 159/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3393.5926 - val_loss: 3391.8431\n",
      "Epoch 160/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3395.3659 - val_loss: 3390.7840\n",
      "Epoch 161/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3394.3101 - val_loss: 3386.9855\n",
      "Epoch 162/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3394.9851 - val_loss: 3391.9822\n",
      "Epoch 163/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3393.7422 - val_loss: 3388.9827\n",
      "Epoch 164/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3392.4953 - val_loss: 3385.5369\n",
      "Epoch 165/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3393.3479 - val_loss: 3391.3584\n",
      "Epoch 166/500\n",
      "1065/1065 [==============================] - 1s 625us/step - loss: 3394.4834 - val_loss: 3392.6408\n",
      "Epoch 167/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3393.5330 - val_loss: 3389.2461\n",
      "Epoch 168/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3392.5449 - val_loss: 3393.6829\n",
      "Epoch 169/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3392.4870 - val_loss: 3390.2014\n",
      "Epoch 170/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3393.8560 - val_loss: 3393.6914\n",
      "Epoch 171/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3393.5906 - val_loss: 3390.9726\n",
      "Epoch 172/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3392.5219 - val_loss: 3388.4781\n",
      "Epoch 173/500\n",
      "1065/1065 [==============================] - 1s 640us/step - loss: 3392.5315 - val_loss: 3398.2726\n",
      "Epoch 174/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3392.4944 - val_loss: 3394.2044\n",
      "Epoch 175/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3392.1087 - val_loss: 3388.4600\n",
      "Epoch 176/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3392.2612 - val_loss: 3389.0605\n",
      "Epoch 177/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3392.6157 - val_loss: 3387.4580\n",
      "Epoch 178/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3392.5807 - val_loss: 3388.0256\n",
      "Epoch 179/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3392.1715 - val_loss: 3386.2898\n",
      "Epoch 180/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3392.5221 - val_loss: 3387.6926\n",
      "Epoch 181/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3390.5704 - val_loss: 3386.3877\n",
      "Epoch 182/500\n",
      "1065/1065 [==============================] - 1s 633us/step - loss: 3390.4214 - val_loss: 3388.7586\n",
      "Epoch 183/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3389.1068 - val_loss: 3386.9677\n",
      "Epoch 184/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3391.5780 - val_loss: 3385.9829\n",
      "Epoch 185/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3389.8302 - val_loss: 3385.2391\n",
      "Epoch 186/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3390.0555 - val_loss: 3382.8626\n",
      "Epoch 187/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3390.2187 - val_loss: 3383.4898\n",
      "Epoch 188/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3389.7999 - val_loss: 3387.2667\n",
      "Epoch 189/500\n",
      "1065/1065 [==============================] - 1s 641us/step - loss: 3389.1517 - val_loss: 3389.9334\n",
      "Epoch 190/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3390.2760 - val_loss: 3384.9700\n",
      "Epoch 191/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3390.5561 - val_loss: 3387.0804\n",
      "Epoch 192/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3389.8927 - val_loss: 3389.0629\n",
      "Epoch 193/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3389.1296 - val_loss: 3383.7327\n",
      "Epoch 194/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3389.3267 - val_loss: 3385.9957\n",
      "Epoch 195/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3389.5879 - val_loss: 3383.5364\n",
      "Epoch 196/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3389.3400 - val_loss: 3386.2976\n",
      "Epoch 197/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3387.9159 - val_loss: 3386.9121\n",
      "Epoch 198/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3387.9846 - val_loss: 3383.2398\n",
      "Epoch 199/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3386.9610 - val_loss: 3383.6345\n",
      "Epoch 200/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3387.6064 - val_loss: 3385.2028\n",
      "Epoch 201/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3387.8961 - val_loss: 3387.6646\n",
      "Epoch 202/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3388.2633 - val_loss: 3384.7925\n",
      "Epoch 203/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3387.2214 - val_loss: 3388.0203\n",
      "Epoch 204/500\n",
      "1065/1065 [==============================] - 1s 617us/step - loss: 3387.7795 - val_loss: 3384.4252\n",
      "Epoch 205/500\n",
      "1065/1065 [==============================] - 1s 633us/step - loss: 3386.6767 - val_loss: 3386.2386\n",
      "Epoch 206/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3387.9379 - val_loss: 3385.5265\n",
      "Epoch 207/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3388.1412 - val_loss: 3386.8794\n",
      "Epoch 208/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3387.9902 - val_loss: 3383.2936\n",
      "Epoch 209/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3387.1451 - val_loss: 3382.4746\n",
      "Epoch 210/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3387.2144 - val_loss: 3382.8451\n",
      "Epoch 211/500\n",
      "1065/1065 [==============================] - 1s 597us/step - loss: 3387.8327 - val_loss: 3384.8204\n",
      "Epoch 212/500\n",
      "1065/1065 [==============================] - 1s 601us/step - loss: 3387.1107 - val_loss: 3387.0412\n",
      "Epoch 213/500\n",
      "1065/1065 [==============================] - 1s 627us/step - loss: 3386.7951 - val_loss: 3383.4117\n",
      "Epoch 214/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3386.2692 - val_loss: 3383.7520\n",
      "Epoch 215/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3387.1239 - val_loss: 3384.0948\n",
      "Epoch 216/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3386.5739 - val_loss: 3382.0551\n",
      "Epoch 217/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3386.8831 - val_loss: 3383.1391\n",
      "Epoch 218/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065/1065 [==============================] - 1s 604us/step - loss: 3385.1418 - val_loss: 3381.4949\n",
      "Epoch 219/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3384.9443 - val_loss: 3382.3949\n",
      "Epoch 220/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3386.0632 - val_loss: 3382.9192\n",
      "Epoch 221/500\n",
      "1065/1065 [==============================] - 1s 622us/step - loss: 3384.9838 - val_loss: 3383.1840\n",
      "Epoch 222/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3385.3742 - val_loss: 3381.7533\n",
      "Epoch 223/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3385.8557 - val_loss: 3379.9538\n",
      "Epoch 224/500\n",
      "1065/1065 [==============================] - 1s 622us/step - loss: 3385.5778 - val_loss: 3379.2857\n",
      "Epoch 225/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3386.4222 - val_loss: 3381.4619\n",
      "Epoch 226/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3385.1381 - val_loss: 3380.1001\n",
      "Epoch 227/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3384.8237 - val_loss: 3383.5017\n",
      "Epoch 228/500\n",
      "1065/1065 [==============================] - 1s 626us/step - loss: 3384.3032 - val_loss: 3381.5646\n",
      "Epoch 229/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3385.2230 - val_loss: 3377.2410\n",
      "Epoch 230/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3383.9639 - val_loss: 3379.4648\n",
      "Epoch 231/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3384.8332 - val_loss: 3378.6433\n",
      "Epoch 232/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3384.5970 - val_loss: 3380.3342\n",
      "Epoch 233/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3385.4921 - val_loss: 3381.9672\n",
      "Epoch 234/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3384.4936 - val_loss: 3381.8379\n",
      "Epoch 235/500\n",
      "1065/1065 [==============================] - 1s 630us/step - loss: 3382.9557 - val_loss: 3380.9905\n",
      "Epoch 236/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3383.6711 - val_loss: 3377.2463\n",
      "Epoch 237/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3383.1578 - val_loss: 3380.7537\n",
      "Epoch 238/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3383.7961 - val_loss: 3379.3046\n",
      "Epoch 239/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3384.4437 - val_loss: 3380.3494\n",
      "Epoch 240/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3382.5575 - val_loss: 3380.5912\n",
      "Epoch 241/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3383.2434 - val_loss: 3376.8554\n",
      "Epoch 242/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3383.8840 - val_loss: 3378.7507\n",
      "Epoch 243/500\n",
      "1065/1065 [==============================] - 1s 631us/step - loss: 3382.6455 - val_loss: 3377.9459\n",
      "Epoch 244/500\n",
      "1065/1065 [==============================] - 1s 617us/step - loss: 3382.6558 - val_loss: 3377.6345\n",
      "Epoch 245/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3382.7010 - val_loss: 3376.2324\n",
      "Epoch 246/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3383.6696 - val_loss: 3379.5774\n",
      "Epoch 247/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3383.4759 - val_loss: 3377.5130\n",
      "Epoch 248/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3384.6525 - val_loss: 3379.7449\n",
      "Epoch 249/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3383.4170 - val_loss: 3377.2923\n",
      "Epoch 250/500\n",
      "1065/1065 [==============================] - 1s 622us/step - loss: 3382.6757 - val_loss: 3376.2066\n",
      "Epoch 251/500\n",
      "1065/1065 [==============================] - 1s 626us/step - loss: 3382.9664 - val_loss: 3378.1721\n",
      "Epoch 252/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3383.2127 - val_loss: 3378.5265\n",
      "Epoch 253/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3382.2534 - val_loss: 3379.4175\n",
      "Epoch 254/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3382.0274 - val_loss: 3377.7270\n",
      "Epoch 255/500\n",
      "1065/1065 [==============================] - 1s 621us/step - loss: 3381.7800 - val_loss: 3377.4602\n",
      "Epoch 256/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3381.5205 - val_loss: 3380.5638\n",
      "Epoch 257/500\n",
      "1065/1065 [==============================] - 1s 601us/step - loss: 3382.3337 - val_loss: 3376.6852\n",
      "Epoch 258/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3382.2016 - val_loss: 3377.6009\n",
      "Epoch 259/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3382.5106 - val_loss: 3376.4181\n",
      "Epoch 260/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3382.1873 - val_loss: 3375.7939\n",
      "Epoch 261/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3382.1707 - val_loss: 3375.7026\n",
      "Epoch 262/500\n",
      "1065/1065 [==============================] - 1s 627us/step - loss: 3381.8933 - val_loss: 3376.8362\n",
      "Epoch 263/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3380.6698 - val_loss: 3374.8334\n",
      "Epoch 264/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3381.3772 - val_loss: 3373.9689\n",
      "Epoch 265/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3381.7698 - val_loss: 3377.3953\n",
      "Epoch 266/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3380.8899 - val_loss: 3375.7960\n",
      "Epoch 267/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3380.1626 - val_loss: 3376.7574\n",
      "Epoch 268/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3381.0484 - val_loss: 3377.6075\n",
      "Epoch 269/500\n",
      "1065/1065 [==============================] - 1s 633us/step - loss: 3380.3683 - val_loss: 3375.6393\n",
      "Epoch 270/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3380.2526 - val_loss: 3375.8393\n",
      "Epoch 271/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3381.0141 - val_loss: 3376.7059\n",
      "Epoch 272/500\n",
      "1065/1065 [==============================] - 1s 629us/step - loss: 3380.2708 - val_loss: 3377.0356\n",
      "Epoch 273/500\n",
      "1065/1065 [==============================] - 1s 615us/step - loss: 3380.7616 - val_loss: 3375.5072\n",
      "Epoch 274/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3381.1041 - val_loss: 3374.9000\n",
      "Epoch 275/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3379.7195 - val_loss: 3374.4937\n",
      "Epoch 276/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3380.3203 - val_loss: 3376.1826\n",
      "Epoch 277/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3380.0224 - val_loss: 3373.8558\n",
      "Epoch 278/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3380.5752 - val_loss: 3375.3919\n",
      "Epoch 279/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3380.1274 - val_loss: 3374.9378\n",
      "Epoch 280/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3380.3157 - val_loss: 3374.8768\n",
      "Epoch 281/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3379.7126 - val_loss: 3373.5739\n",
      "Epoch 282/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3379.3101 - val_loss: 3375.5456\n",
      "Epoch 283/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3379.5362 - val_loss: 3374.6874\n",
      "Epoch 284/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3378.5013 - val_loss: 3376.0955\n",
      "Epoch 285/500\n",
      "1065/1065 [==============================] - 1s 621us/step - loss: 3379.4893 - val_loss: 3373.2638\n",
      "Epoch 286/500\n",
      "1065/1065 [==============================] - 1s 637us/step - loss: 3378.9109 - val_loss: 3375.2079\n",
      "Epoch 287/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3379.4016 - val_loss: 3375.1085\n",
      "Epoch 288/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3378.7982 - val_loss: 3375.7449\n",
      "Epoch 289/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3378.5287 - val_loss: 3377.0025\n",
      "Epoch 290/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065/1065 [==============================] - 1s 604us/step - loss: 3378.3640 - val_loss: 3375.9093\n",
      "Epoch 291/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3379.0691 - val_loss: 3375.7045\n",
      "Epoch 292/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3378.2255 - val_loss: 3374.3410\n",
      "Epoch 293/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3379.6820 - val_loss: 3373.1216\n",
      "Epoch 294/500\n",
      "1065/1065 [==============================] - 1s 626us/step - loss: 3379.7846 - val_loss: 3373.9791\n",
      "Epoch 295/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3378.5135 - val_loss: 3373.5987\n",
      "Epoch 296/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3379.2753 - val_loss: 3373.6595\n",
      "Epoch 297/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3378.6146 - val_loss: 3373.7676\n",
      "Epoch 298/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3377.8338 - val_loss: 3376.7030\n",
      "Epoch 299/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3378.6754 - val_loss: 3377.8968\n",
      "Epoch 300/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3377.9673 - val_loss: 3374.6618\n",
      "Epoch 301/500\n",
      "1065/1065 [==============================] - 1s 635us/step - loss: 3378.3604 - val_loss: 3376.1523\n",
      "Epoch 302/500\n",
      "1065/1065 [==============================] - 1s 624us/step - loss: 3377.8943 - val_loss: 3373.7490\n",
      "Epoch 303/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3377.4240 - val_loss: 3373.7579\n",
      "Epoch 304/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3377.1288 - val_loss: 3371.7125\n",
      "Epoch 305/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3377.9220 - val_loss: 3370.6877\n",
      "Epoch 306/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3377.8264 - val_loss: 3372.8258\n",
      "Epoch 307/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3377.6057 - val_loss: 3371.1169\n",
      "Epoch 308/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3377.3577 - val_loss: 3374.6968\n",
      "Epoch 309/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3377.9114 - val_loss: 3373.8734\n",
      "Epoch 310/500\n",
      "1065/1065 [==============================] - 1s 625us/step - loss: 3377.2545 - val_loss: 3374.1131\n",
      "Epoch 311/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3379.2158 - val_loss: 3371.5938\n",
      "Epoch 312/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3377.7937 - val_loss: 3372.1625\n",
      "Epoch 313/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3377.5655 - val_loss: 3374.9215\n",
      "Epoch 314/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3376.4711 - val_loss: 3374.2718\n",
      "Epoch 315/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3377.6076 - val_loss: 3372.0559\n",
      "Epoch 316/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3376.4733 - val_loss: 3371.9418\n",
      "Epoch 317/500\n",
      "1065/1065 [==============================] - 1s 629us/step - loss: 3376.8103 - val_loss: 3370.2847\n",
      "Epoch 318/500\n",
      "1065/1065 [==============================] - 1s 617us/step - loss: 3377.1419 - val_loss: 3372.3618\n",
      "Epoch 319/500\n",
      "1065/1065 [==============================] - 1s 627us/step - loss: 3377.1528 - val_loss: 3370.7131\n",
      "Epoch 320/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3377.5020 - val_loss: 3372.8479\n",
      "Epoch 321/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3376.1335 - val_loss: 3373.0749\n",
      "Epoch 322/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3376.4721 - val_loss: 3371.2513\n",
      "Epoch 323/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3376.0670 - val_loss: 3372.0448\n",
      "Epoch 324/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3376.1503 - val_loss: 3372.0162\n",
      "Epoch 325/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3377.0942 - val_loss: 3372.1817\n",
      "Epoch 326/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3375.8095 - val_loss: 3372.4310\n",
      "Epoch 327/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3375.5259 - val_loss: 3372.8173\n",
      "Epoch 328/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3375.0903 - val_loss: 3369.7151\n",
      "Epoch 329/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3375.4233 - val_loss: 3371.4078\n",
      "Epoch 330/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3376.4285 - val_loss: 3372.0509\n",
      "Epoch 331/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3375.7009 - val_loss: 3370.6072\n",
      "Epoch 332/500\n",
      "1065/1065 [==============================] - 1s 624us/step - loss: 3376.2052 - val_loss: 3369.9683\n",
      "Epoch 333/500\n",
      "1065/1065 [==============================] - 1s 655us/step - loss: 3375.8455 - val_loss: 3373.1463\n",
      "Epoch 334/500\n",
      "1065/1065 [==============================] - 1s 634us/step - loss: 3376.5162 - val_loss: 3372.2913\n",
      "Epoch 335/500\n",
      "1065/1065 [==============================] - 1s 620us/step - loss: 3375.3082 - val_loss: 3370.9162\n",
      "Epoch 336/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3375.5865 - val_loss: 3369.5013\n",
      "Epoch 337/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3376.1916 - val_loss: 3369.7159\n",
      "Epoch 338/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3375.3922 - val_loss: 3369.1394\n",
      "Epoch 339/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3374.8459 - val_loss: 3369.3426\n",
      "Epoch 340/500\n",
      "1065/1065 [==============================] - 1s 635us/step - loss: 3375.0128 - val_loss: 3370.2483\n",
      "Epoch 341/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3375.3733 - val_loss: 3369.6671\n",
      "Epoch 342/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3375.4032 - val_loss: 3371.2154\n",
      "Epoch 343/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3374.9460 - val_loss: 3369.5433\n",
      "Epoch 344/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3375.0061 - val_loss: 3368.5932\n",
      "Epoch 345/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3375.0599 - val_loss: 3370.3117\n",
      "Epoch 346/500\n",
      "1065/1065 [==============================] - 1s 629us/step - loss: 3375.2456 - val_loss: 3369.7135\n",
      "Epoch 347/500\n",
      "1065/1065 [==============================] - 1s 662us/step - loss: 3374.7684 - val_loss: 3369.2726\n",
      "Epoch 348/500\n",
      "1065/1065 [==============================] - 1s 642us/step - loss: 3374.7354 - val_loss: 3370.3738\n",
      "Epoch 349/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3374.7145 - val_loss: 3370.2848\n",
      "Epoch 350/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3373.7095 - val_loss: 3370.3949\n",
      "Epoch 351/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3375.7107 - val_loss: 3370.0876\n",
      "Epoch 352/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3374.0140 - val_loss: 3371.7085\n",
      "Epoch 353/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3375.5735 - val_loss: 3370.0418\n",
      "Epoch 354/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3374.1146 - val_loss: 3368.7364\n",
      "Epoch 355/500\n",
      "1065/1065 [==============================] - 1s 621us/step - loss: 3374.5659 - val_loss: 3367.8205\n",
      "Epoch 356/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3374.9669 - val_loss: 3368.1887\n",
      "Epoch 357/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3375.7075 - val_loss: 3367.2024\n",
      "Epoch 358/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3374.0059 - val_loss: 3368.1475\n",
      "Epoch 359/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3373.3963 - val_loss: 3368.8915\n",
      "Epoch 360/500\n",
      "1065/1065 [==============================] - 1s 600us/step - loss: 3373.7581 - val_loss: 3370.2587\n",
      "Epoch 361/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3373.6814 - val_loss: 3368.9890\n",
      "Epoch 362/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065/1065 [==============================] - 1s 623us/step - loss: 3374.4978 - val_loss: 3370.0830\n",
      "Epoch 363/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3374.0095 - val_loss: 3370.4844\n",
      "Epoch 364/500\n",
      "1065/1065 [==============================] - 1s 633us/step - loss: 3373.1873 - val_loss: 3368.9640\n",
      "Epoch 365/500\n",
      "1065/1065 [==============================] - 1s 616us/step - loss: 3374.7787 - val_loss: 3370.9588\n",
      "Epoch 366/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3374.0277 - val_loss: 3369.3810\n",
      "Epoch 367/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3373.6089 - val_loss: 3367.0624\n",
      "Epoch 368/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3373.9661 - val_loss: 3368.0499\n",
      "Epoch 369/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3373.9313 - val_loss: 3367.5202\n",
      "Epoch 370/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3373.9741 - val_loss: 3368.8891\n",
      "Epoch 371/500\n",
      "1065/1065 [==============================] - 1s 621us/step - loss: 3373.1444 - val_loss: 3366.9087\n",
      "Epoch 372/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3373.7496 - val_loss: 3367.2979\n",
      "Epoch 373/500\n",
      "1065/1065 [==============================] - 1s 599us/step - loss: 3373.2867 - val_loss: 3369.4690\n",
      "Epoch 374/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3372.1035 - val_loss: 3369.1740\n",
      "Epoch 375/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3372.8221 - val_loss: 3368.6882\n",
      "Epoch 376/500\n",
      "1065/1065 [==============================] - 1s 613us/step - loss: 3373.2408 - val_loss: 3369.4751\n",
      "Epoch 377/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3372.6743 - val_loss: 3369.4744\n",
      "Epoch 378/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3372.3579 - val_loss: 3367.6361\n",
      "Epoch 379/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3373.2494 - val_loss: 3367.7245\n",
      "Epoch 380/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3372.8047 - val_loss: 3369.1773\n",
      "Epoch 381/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3372.9697 - val_loss: 3365.3658\n",
      "Epoch 382/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3372.6423 - val_loss: 3366.3026\n",
      "Epoch 383/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3372.0203 - val_loss: 3367.5456\n",
      "Epoch 384/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3371.7656 - val_loss: 3368.4973\n",
      "Epoch 385/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3371.8632 - val_loss: 3368.0622\n",
      "Epoch 386/500\n",
      "1065/1065 [==============================] - 1s 626us/step - loss: 3372.2481 - val_loss: 3365.7439\n",
      "Epoch 387/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3372.2099 - val_loss: 3367.3276\n",
      "Epoch 388/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3372.9628 - val_loss: 3368.3118\n",
      "Epoch 389/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3371.9134 - val_loss: 3367.8834\n",
      "Epoch 390/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3371.7091 - val_loss: 3365.1094\n",
      "Epoch 391/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3373.2027 - val_loss: 3366.9001\n",
      "Epoch 392/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3372.2541 - val_loss: 3369.3227\n",
      "Epoch 393/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3372.0756 - val_loss: 3366.4666\n",
      "Epoch 394/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3372.3642 - val_loss: 3367.2593\n",
      "Epoch 395/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3372.2504 - val_loss: 3367.5305\n",
      "Epoch 396/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3373.1758 - val_loss: 3366.6137\n",
      "Epoch 397/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3371.4521 - val_loss: 3367.4437\n",
      "Epoch 398/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3371.7722 - val_loss: 3366.7171\n",
      "Epoch 399/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3372.7943 - val_loss: 3366.5726\n",
      "Epoch 400/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3371.1898 - val_loss: 3366.8940\n",
      "Epoch 401/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3370.8067 - val_loss: 3367.0384\n",
      "Epoch 402/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3371.4470 - val_loss: 3367.2883\n",
      "Epoch 403/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3371.7365 - val_loss: 3366.6712\n",
      "Epoch 404/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3370.9543 - val_loss: 3367.1470\n",
      "Epoch 405/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3372.0812 - val_loss: 3366.5825\n",
      "Epoch 406/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3371.3259 - val_loss: 3365.6058\n",
      "Epoch 407/500\n",
      "1065/1065 [==============================] - 1s 618us/step - loss: 3370.5925 - val_loss: 3366.3239\n",
      "Epoch 408/500\n",
      "1065/1065 [==============================] - 1s 625us/step - loss: 3370.8218 - val_loss: 3365.9481\n",
      "Epoch 409/500\n",
      "1065/1065 [==============================] - 1s 632us/step - loss: 3371.5103 - val_loss: 3366.4136\n",
      "Epoch 410/500\n",
      "1065/1065 [==============================] - 1s 612us/step - loss: 3371.1675 - val_loss: 3366.6359\n",
      "Epoch 411/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3371.3978 - val_loss: 3368.1436\n",
      "Epoch 412/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3371.0684 - val_loss: 3366.1764\n",
      "Epoch 413/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3370.4203 - val_loss: 3365.5421\n",
      "Epoch 414/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3370.3038 - val_loss: 3364.2953\n",
      "Epoch 415/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3371.5142 - val_loss: 3364.6755\n",
      "Epoch 416/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3370.0064 - val_loss: 3363.5049\n",
      "Epoch 417/500\n",
      "1065/1065 [==============================] - 1s 617us/step - loss: 3370.5824 - val_loss: 3364.0089\n",
      "Epoch 418/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3370.1168 - val_loss: 3365.8285\n",
      "Epoch 419/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3371.2347 - val_loss: 3364.3548\n",
      "Epoch 420/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3370.2071 - val_loss: 3363.7438\n",
      "Epoch 421/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3370.6780 - val_loss: 3365.2431\n",
      "Epoch 422/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3370.0221 - val_loss: 3366.0956\n",
      "Epoch 423/500\n",
      "1065/1065 [==============================] - 1s 615us/step - loss: 3369.5341 - val_loss: 3365.6292\n",
      "Epoch 424/500\n",
      "1065/1065 [==============================] - 1s 629us/step - loss: 3370.3819 - val_loss: 3366.1187\n",
      "Epoch 425/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3371.0250 - val_loss: 3365.2478\n",
      "Epoch 426/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3370.5076 - val_loss: 3364.8483\n",
      "Epoch 427/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3369.6455 - val_loss: 3365.8260\n",
      "Epoch 428/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3369.7811 - val_loss: 3365.8506\n",
      "Epoch 429/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3370.3610 - val_loss: 3365.9219\n",
      "Epoch 430/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3371.0193 - val_loss: 3365.1973\n",
      "Epoch 431/500\n",
      "1065/1065 [==============================] - 1s 600us/step - loss: 3370.7131 - val_loss: 3365.8189\n",
      "Epoch 432/500\n",
      "1065/1065 [==============================] - 1s 626us/step - loss: 3371.1046 - val_loss: 3364.8645\n",
      "Epoch 433/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3369.9961 - val_loss: 3363.3806\n",
      "Epoch 434/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065/1065 [==============================] - 1s 604us/step - loss: 3370.3618 - val_loss: 3364.8645\n",
      "Epoch 435/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3369.6209 - val_loss: 3363.7813\n",
      "Epoch 436/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3369.9715 - val_loss: 3366.0597\n",
      "Epoch 437/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3369.7285 - val_loss: 3365.1820\n",
      "Epoch 438/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3370.1604 - val_loss: 3364.8283\n",
      "Epoch 439/500\n",
      "1065/1065 [==============================] - 1s 626us/step - loss: 3370.2778 - val_loss: 3367.3836\n",
      "Epoch 440/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3369.6566 - val_loss: 3363.5942\n",
      "Epoch 441/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3369.7794 - val_loss: 3364.2009\n",
      "Epoch 442/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3369.9029 - val_loss: 3364.0392\n",
      "Epoch 443/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3369.2659 - val_loss: 3365.4608\n",
      "Epoch 444/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3370.4647 - val_loss: 3364.0374\n",
      "Epoch 445/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3369.2515 - val_loss: 3363.0931\n",
      "Epoch 446/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3368.7640 - val_loss: 3363.9366\n",
      "Epoch 447/500\n",
      "1065/1065 [==============================] - 1s 621us/step - loss: 3368.7578 - val_loss: 3365.2508\n",
      "Epoch 448/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3369.6176 - val_loss: 3364.7046\n",
      "Epoch 449/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3369.8109 - val_loss: 3363.3321\n",
      "Epoch 450/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3369.7030 - val_loss: 3364.9789\n",
      "Epoch 451/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3369.7846 - val_loss: 3365.2484\n",
      "Epoch 452/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3368.7957 - val_loss: 3363.9780\n",
      "Epoch 453/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3369.3342 - val_loss: 3364.5218\n",
      "Epoch 454/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3368.2577 - val_loss: 3365.6229\n",
      "Epoch 455/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3369.0337 - val_loss: 3365.1076\n",
      "Epoch 456/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3368.2045 - val_loss: 3363.9690\n",
      "Epoch 457/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3369.3838 - val_loss: 3363.4020\n",
      "Epoch 458/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3368.7256 - val_loss: 3364.6614\n",
      "Epoch 459/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3368.7443 - val_loss: 3364.3559\n",
      "Epoch 460/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3368.2554 - val_loss: 3363.4939\n",
      "Epoch 461/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3368.4280 - val_loss: 3362.6939\n",
      "Epoch 462/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3369.2201 - val_loss: 3362.9214\n",
      "Epoch 463/500\n",
      "1065/1065 [==============================] - 1s 627us/step - loss: 3367.6861 - val_loss: 3365.1479\n",
      "Epoch 464/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3369.0199 - val_loss: 3362.4748\n",
      "Epoch 465/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3368.5197 - val_loss: 3363.5130\n",
      "Epoch 466/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3369.4651 - val_loss: 3363.3981\n",
      "Epoch 467/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3369.4674 - val_loss: 3363.8282\n",
      "Epoch 468/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3368.7943 - val_loss: 3363.8662\n",
      "Epoch 469/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3367.9296 - val_loss: 3363.2617\n",
      "Epoch 470/500\n",
      "1065/1065 [==============================] - 1s 630us/step - loss: 3368.5906 - val_loss: 3364.4937\n",
      "Epoch 471/500\n",
      "1065/1065 [==============================] - 1s 619us/step - loss: 3368.7699 - val_loss: 3363.6083\n",
      "Epoch 472/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3368.7089 - val_loss: 3363.3660\n",
      "Epoch 473/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3368.3838 - val_loss: 3364.4905\n",
      "Epoch 474/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3367.7545 - val_loss: 3364.0795\n",
      "Epoch 475/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3368.2226 - val_loss: 3362.9192\n",
      "Epoch 476/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3368.8643 - val_loss: 3363.8368\n",
      "Epoch 477/500\n",
      "1065/1065 [==============================] - 1s 610us/step - loss: 3368.2343 - val_loss: 3362.5709\n",
      "Epoch 478/500\n",
      "1065/1065 [==============================] - 1s 628us/step - loss: 3368.6003 - val_loss: 3364.4084\n",
      "Epoch 479/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3368.7907 - val_loss: 3363.4021\n",
      "Epoch 480/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3368.6349 - val_loss: 3362.9117\n",
      "Epoch 481/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3368.1227 - val_loss: 3363.2736\n",
      "Epoch 482/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3367.7084 - val_loss: 3363.5064\n",
      "Epoch 483/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3367.9575 - val_loss: 3363.5286\n",
      "Epoch 484/500\n",
      "1065/1065 [==============================] - 1s 607us/step - loss: 3367.1691 - val_loss: 3362.3962\n",
      "Epoch 485/500\n",
      "1065/1065 [==============================] - 1s 632us/step - loss: 3367.1073 - val_loss: 3362.1116\n",
      "Epoch 486/500\n",
      "1065/1065 [==============================] - 1s 623us/step - loss: 3367.6010 - val_loss: 3362.5177\n",
      "Epoch 487/500\n",
      "1065/1065 [==============================] - 1s 602us/step - loss: 3367.3316 - val_loss: 3362.6245\n",
      "Epoch 488/500\n",
      "1065/1065 [==============================] - 1s 609us/step - loss: 3367.4332 - val_loss: 3361.4299\n",
      "Epoch 489/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3367.8447 - val_loss: 3362.7070\n",
      "Epoch 490/500\n",
      "1065/1065 [==============================] - 1s 606us/step - loss: 3368.3419 - val_loss: 3362.3317\n",
      "Epoch 491/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3367.0256 - val_loss: 3362.2773\n",
      "Epoch 492/500\n",
      "1065/1065 [==============================] - 1s 604us/step - loss: 3366.7181 - val_loss: 3362.2671\n",
      "Epoch 493/500\n",
      "1065/1065 [==============================] - 1s 614us/step - loss: 3367.1126 - val_loss: 3362.4373\n",
      "Epoch 494/500\n",
      "1065/1065 [==============================] - 1s 624us/step - loss: 3369.0088 - val_loss: 3362.9790\n",
      "Epoch 495/500\n",
      "1065/1065 [==============================] - 1s 608us/step - loss: 3368.1083 - val_loss: 3362.5942\n",
      "Epoch 496/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3367.8583 - val_loss: 3363.3651\n",
      "Epoch 497/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3367.5032 - val_loss: 3362.1977\n",
      "Epoch 498/500\n",
      "1065/1065 [==============================] - 1s 603us/step - loss: 3367.5412 - val_loss: 3362.2460\n",
      "Epoch 499/500\n",
      "1065/1065 [==============================] - 1s 605us/step - loss: 3367.4136 - val_loss: 3363.0583\n",
      "Epoch 500/500\n",
      "1065/1065 [==============================] - 1s 611us/step - loss: 3367.3432 - val_loss: 3361.9666\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# Separate encoder and decoder components of the model\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Model includes all layers in the computation of rnaseq_input GIVEN z_mean_encoded\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5054ce0128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_to_reconstruct(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "decoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
