{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with single hidden layer\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train_model_input.txt.xz\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0, header=0, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_1layer_{}_train_stats.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_1layer_{}_train_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"tybalt_1layer_{}_train_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}_train_encoder_model.h5\".format(latent_dim))\n",
    "weights_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}_train_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}_train_decoder_model.h5\".format(latent_dim))\n",
    "weights_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}_train_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:66: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_to_reconstruct = Dense(original_dim,\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               activation='sigmoid')\n",
    "rnaseq_reconstruct = decoder_to_reconstruct(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1065 samples, validate on 118 samples\n",
      "Epoch 1/100\n",
      "1065/1065 [==============================] - 0s 451us/step - loss: 3811.2504 - val_loss: 3755.5165\n",
      "Epoch 2/100\n",
      "1065/1065 [==============================] - 0s 281us/step - loss: 3727.4614 - val_loss: 3706.1196\n",
      "Epoch 3/100\n",
      "1065/1065 [==============================] - 0s 281us/step - loss: 3669.4457 - val_loss: 3696.5237\n",
      "Epoch 4/100\n",
      "1065/1065 [==============================] - 0s 325us/step - loss: 3610.9974 - val_loss: 3678.1073\n",
      "Epoch 5/100\n",
      "1065/1065 [==============================] - 0s 300us/step - loss: 3586.5709 - val_loss: 3651.2338\n",
      "Epoch 6/100\n",
      "1065/1065 [==============================] - 0s 268us/step - loss: 3565.8494 - val_loss: 3587.4175\n",
      "Epoch 7/100\n",
      "1065/1065 [==============================] - 0s 266us/step - loss: 3546.2525 - val_loss: 3531.9824\n",
      "Epoch 8/100\n",
      "1065/1065 [==============================] - 0s 270us/step - loss: 3530.6122 - val_loss: 3531.5291\n",
      "Epoch 9/100\n",
      "1065/1065 [==============================] - 0s 286us/step - loss: 3519.9819 - val_loss: 3553.7956\n",
      "Epoch 10/100\n",
      "1065/1065 [==============================] - 0s 296us/step - loss: 3514.7044 - val_loss: 3515.3949\n",
      "Epoch 11/100\n",
      "1065/1065 [==============================] - 0s 269us/step - loss: 3504.8325 - val_loss: 3517.0518\n",
      "Epoch 12/100\n",
      "1065/1065 [==============================] - 0s 278us/step - loss: 3497.8174 - val_loss: 3500.0233\n",
      "Epoch 13/100\n",
      "1065/1065 [==============================] - 0s 277us/step - loss: 3492.5878 - val_loss: 3525.0856\n",
      "Epoch 14/100\n",
      "1065/1065 [==============================] - 0s 267us/step - loss: 3488.0282 - val_loss: 3485.3895\n",
      "Epoch 15/100\n",
      "1065/1065 [==============================] - 0s 266us/step - loss: 3481.0890 - val_loss: 3506.6850\n",
      "Epoch 16/100\n",
      "1065/1065 [==============================] - 0s 266us/step - loss: 3477.2106 - val_loss: 3487.6068\n",
      "Epoch 17/100\n",
      "1065/1065 [==============================] - 0s 267us/step - loss: 3472.4480 - val_loss: 3471.5381\n",
      "Epoch 18/100\n",
      "1065/1065 [==============================] - 0s 267us/step - loss: 3467.1198 - val_loss: 3470.4190\n",
      "Epoch 19/100\n",
      "1065/1065 [==============================] - 0s 267us/step - loss: 3466.3394 - val_loss: 3484.9303\n",
      "Epoch 20/100\n",
      "1065/1065 [==============================] - 0s 267us/step - loss: 3466.3430 - val_loss: 3466.7206\n",
      "Epoch 21/100\n",
      "1065/1065 [==============================] - 0s 277us/step - loss: 3459.2114 - val_loss: 3470.2497\n",
      "Epoch 22/100\n",
      "1065/1065 [==============================] - 0s 274us/step - loss: 3459.8179 - val_loss: 3484.3769\n",
      "Epoch 23/100\n",
      "1065/1065 [==============================] - 0s 276us/step - loss: 3453.0461 - val_loss: 3472.2536\n",
      "Epoch 24/100\n",
      "1065/1065 [==============================] - 0s 269us/step - loss: 3453.9394 - val_loss: 3463.3255\n",
      "Epoch 25/100\n",
      "1065/1065 [==============================] - 0s 309us/step - loss: 3449.6938 - val_loss: 3466.9674\n",
      "Epoch 26/100\n",
      "1065/1065 [==============================] - 0s 305us/step - loss: 3449.3827 - val_loss: 3451.8691\n",
      "Epoch 27/100\n",
      "1065/1065 [==============================] - 0s 290us/step - loss: 3444.5283 - val_loss: 3456.0871\n",
      "Epoch 28/100\n",
      "1065/1065 [==============================] - 0s 291us/step - loss: 3444.8467 - val_loss: 3454.4892\n",
      "Epoch 29/100\n",
      "1065/1065 [==============================] - 0s 275us/step - loss: 3443.5841 - val_loss: 3462.8343\n",
      "Epoch 30/100\n",
      "1065/1065 [==============================] - 0s 286us/step - loss: 3440.2697 - val_loss: 3452.3749\n",
      "Epoch 31/100\n",
      "1065/1065 [==============================] - 0s 284us/step - loss: 3439.0310 - val_loss: 3452.8259\n",
      "Epoch 32/100\n",
      "1065/1065 [==============================] - 0s 299us/step - loss: 3437.6271 - val_loss: 3440.4070\n",
      "Epoch 33/100\n",
      "1065/1065 [==============================] - 0s 295us/step - loss: 3433.8231 - val_loss: 3446.0419\n",
      "Epoch 34/100\n",
      "1065/1065 [==============================] - 0s 310us/step - loss: 3433.6733 - val_loss: 3448.0886\n",
      "Epoch 35/100\n",
      "1065/1065 [==============================] - 0s 304us/step - loss: 3434.1928 - val_loss: 3447.0050\n",
      "Epoch 36/100\n",
      "1065/1065 [==============================] - 0s 297us/step - loss: 3432.3936 - val_loss: 3441.7651\n",
      "Epoch 37/100\n",
      "1065/1065 [==============================] - 0s 313us/step - loss: 3431.0273 - val_loss: 3445.5012\n",
      "Epoch 38/100\n",
      "1065/1065 [==============================] - 0s 292us/step - loss: 3429.2248 - val_loss: 3436.8298\n",
      "Epoch 39/100\n",
      "1065/1065 [==============================] - 0s 283us/step - loss: 3427.6030 - val_loss: 3429.1106\n",
      "Epoch 40/100\n",
      "1065/1065 [==============================] - 0s 319us/step - loss: 3428.4366 - val_loss: 3433.8783\n",
      "Epoch 41/100\n",
      "1065/1065 [==============================] - 0s 322us/step - loss: 3426.1638 - val_loss: 3435.4232\n",
      "Epoch 42/100\n",
      "1065/1065 [==============================] - 0s 311us/step - loss: 3423.3976 - val_loss: 3441.5048\n",
      "Epoch 43/100\n",
      "1065/1065 [==============================] - 0s 285us/step - loss: 3423.7505 - val_loss: 3429.3047\n",
      "Epoch 44/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3424.6197 - val_loss: 3426.6727\n",
      "Epoch 45/100\n",
      "1065/1065 [==============================] - 0s 276us/step - loss: 3421.6968 - val_loss: 3427.5153\n",
      "Epoch 46/100\n",
      "1065/1065 [==============================] - 0s 295us/step - loss: 3422.1169 - val_loss: 3432.7034\n",
      "Epoch 47/100\n",
      "1065/1065 [==============================] - 0s 354us/step - loss: 3420.1535 - val_loss: 3432.1528\n",
      "Epoch 48/100\n",
      "1065/1065 [==============================] - 0s 315us/step - loss: 3417.8867 - val_loss: 3440.9391\n",
      "Epoch 49/100\n",
      "1065/1065 [==============================] - 0s 317us/step - loss: 3418.4182 - val_loss: 3430.3520\n",
      "Epoch 50/100\n",
      "1065/1065 [==============================] - 0s 302us/step - loss: 3416.1000 - val_loss: 3437.0955\n",
      "Epoch 51/100\n",
      "1065/1065 [==============================] - 0s 311us/step - loss: 3416.5696 - val_loss: 3420.9607\n",
      "Epoch 52/100\n",
      "1065/1065 [==============================] - 0s 300us/step - loss: 3416.9461 - val_loss: 3433.3490\n",
      "Epoch 53/100\n",
      "1065/1065 [==============================] - 0s 344us/step - loss: 3414.1784 - val_loss: 3424.0947\n",
      "Epoch 54/100\n",
      "1065/1065 [==============================] - 0s 278us/step - loss: 3412.3505 - val_loss: 3417.6796\n",
      "Epoch 55/100\n",
      "1065/1065 [==============================] - 0s 309us/step - loss: 3412.3679 - val_loss: 3416.8888\n",
      "Epoch 56/100\n",
      "1065/1065 [==============================] - 0s 318us/step - loss: 3411.8929 - val_loss: 3413.2689\n",
      "Epoch 57/100\n",
      "1065/1065 [==============================] - 0s 314us/step - loss: 3412.4383 - val_loss: 3416.1327\n",
      "Epoch 58/100\n",
      "1065/1065 [==============================] - 0s 292us/step - loss: 3410.7069 - val_loss: 3416.5473\n",
      "Epoch 59/100\n",
      "1065/1065 [==============================] - 0s 275us/step - loss: 3409.8685 - val_loss: 3415.2698\n",
      "Epoch 60/100\n",
      "1065/1065 [==============================] - 0s 276us/step - loss: 3409.6745 - val_loss: 3421.2915\n",
      "Epoch 61/100\n",
      "1065/1065 [==============================] - 0s 286us/step - loss: 3408.3462 - val_loss: 3412.2521\n",
      "Epoch 62/100\n",
      "1065/1065 [==============================] - 0s 270us/step - loss: 3410.2532 - val_loss: 3412.0840\n",
      "Epoch 63/100\n",
      "1065/1065 [==============================] - 0s 297us/step - loss: 3407.3286 - val_loss: 3406.1489\n",
      "Epoch 64/100\n",
      "1065/1065 [==============================] - 0s 302us/step - loss: 3408.5009 - val_loss: 3411.2640\n",
      "Epoch 65/100\n",
      "1065/1065 [==============================] - 0s 327us/step - loss: 3406.2867 - val_loss: 3411.7966\n",
      "Epoch 66/100\n",
      "1065/1065 [==============================] - 0s 303us/step - loss: 3406.9472 - val_loss: 3410.4772\n",
      "Epoch 67/100\n",
      "1065/1065 [==============================] - 0s 286us/step - loss: 3404.9893 - val_loss: 3414.5343\n",
      "Epoch 68/100\n",
      "1065/1065 [==============================] - 0s 270us/step - loss: 3404.1953 - val_loss: 3417.8439\n",
      "Epoch 69/100\n",
      "1065/1065 [==============================] - 0s 287us/step - loss: 3404.8377 - val_loss: 3410.6140\n",
      "Epoch 70/100\n",
      "1065/1065 [==============================] - 0s 302us/step - loss: 3403.5419 - val_loss: 3416.8609\n",
      "Epoch 71/100\n",
      "1065/1065 [==============================] - 0s 311us/step - loss: 3402.6818 - val_loss: 3410.8441\n",
      "Epoch 72/100\n",
      "1065/1065 [==============================] - 0s 302us/step - loss: 3402.1373 - val_loss: 3414.8312\n",
      "Epoch 73/100\n",
      "1065/1065 [==============================] - 0s 339us/step - loss: 3402.3251 - val_loss: 3413.6423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "1065/1065 [==============================] - 0s 329us/step - loss: 3401.5131 - val_loss: 3416.3508\n",
      "Epoch 75/100\n",
      "1065/1065 [==============================] - 0s 321us/step - loss: 3401.6303 - val_loss: 3409.6249\n",
      "Epoch 76/100\n",
      "1065/1065 [==============================] - 0s 294us/step - loss: 3400.5498 - val_loss: 3409.6306\n",
      "Epoch 77/100\n",
      "1065/1065 [==============================] - 0s 284us/step - loss: 3398.4396 - val_loss: 3406.5146\n",
      "Epoch 78/100\n",
      "1065/1065 [==============================] - 0s 297us/step - loss: 3401.3762 - val_loss: 3409.8418\n",
      "Epoch 79/100\n",
      "1065/1065 [==============================] - 0s 302us/step - loss: 3400.5101 - val_loss: 3406.8629\n",
      "Epoch 80/100\n",
      "1065/1065 [==============================] - 0s 322us/step - loss: 3399.7613 - val_loss: 3410.9499\n",
      "Epoch 81/100\n",
      "1065/1065 [==============================] - 0s 303us/step - loss: 3400.9558 - val_loss: 3409.6067\n",
      "Epoch 82/100\n",
      "1065/1065 [==============================] - 0s 293us/step - loss: 3396.9431 - val_loss: 3406.6912\n",
      "Epoch 83/100\n",
      "1065/1065 [==============================] - 0s 269us/step - loss: 3398.9320 - val_loss: 3406.6659\n",
      "Epoch 84/100\n",
      "1065/1065 [==============================] - 0s 272us/step - loss: 3398.8338 - val_loss: 3407.6956\n",
      "Epoch 85/100\n",
      "1065/1065 [==============================] - 0s 270us/step - loss: 3398.2669 - val_loss: 3404.3120\n",
      "Epoch 86/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3395.3539 - val_loss: 3407.2929\n",
      "Epoch 87/100\n",
      "1065/1065 [==============================] - 0s 277us/step - loss: 3396.6764 - val_loss: 3403.3335\n",
      "Epoch 88/100\n",
      "1065/1065 [==============================] - 0s 315us/step - loss: 3397.0593 - val_loss: 3402.9465\n",
      "Epoch 89/100\n",
      "1065/1065 [==============================] - 0s 305us/step - loss: 3397.3352 - val_loss: 3403.2800\n",
      "Epoch 90/100\n",
      "1065/1065 [==============================] - 0s 307us/step - loss: 3395.2430 - val_loss: 3402.7673\n",
      "Epoch 91/100\n",
      "1065/1065 [==============================] - 0s 276us/step - loss: 3395.2198 - val_loss: 3402.7107\n",
      "Epoch 92/100\n",
      "1065/1065 [==============================] - 0s 289us/step - loss: 3395.1290 - val_loss: 3401.4811\n",
      "Epoch 93/100\n",
      "1065/1065 [==============================] - 0s 290us/step - loss: 3394.8611 - val_loss: 3399.9211\n",
      "Epoch 94/100\n",
      "1065/1065 [==============================] - 0s 295us/step - loss: 3393.2030 - val_loss: 3402.5874\n",
      "Epoch 95/100\n",
      "1065/1065 [==============================] - 0s 278us/step - loss: 3394.5241 - val_loss: 3400.8114\n",
      "Epoch 96/100\n",
      "1065/1065 [==============================] - 0s 273us/step - loss: 3393.6052 - val_loss: 3401.3663\n",
      "Epoch 97/100\n",
      "1065/1065 [==============================] - 0s 290us/step - loss: 3393.4276 - val_loss: 3406.1543\n",
      "Epoch 98/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3392.3922 - val_loss: 3398.6211\n",
      "Epoch 99/100\n",
      "1065/1065 [==============================] - 0s 274us/step - loss: 3392.8274 - val_loss: 3402.4077\n",
      "Epoch 100/100\n",
      "1065/1065 [==============================] - 0s 276us/step - loss: 3392.0599 - val_loss: 3402.0296\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# Separate encoder and decoder components of the model\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Model includes all layers in the computation of rnaseq_input GIVEN z_mean_encoded\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1210127b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_to_reconstruct(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "encoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
