{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with single hidden layer\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train_model_input.txt.xz\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0, header=0, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_1layer_{}_train_stats.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_1layer_{}_train_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"tybalt_1layer_{}_train_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}_train_encoder_model.json\".format(latent_dim))\n",
    "weights_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}_train_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}_train_decoder_model.json\".format(latent_dim))\n",
    "weights_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_{}_train_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:66: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_to_reconstruct = Dense(original_dim,\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               activation='sigmoid')\n",
    "rnaseq_reconstruct = decoder_to_reconstruct(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1065 samples, validate on 118 samples\n",
      "Epoch 1/100\n",
      "1065/1065 [==============================] - 1s 482us/step - loss: 3811.2677 - val_loss: 3757.9316\n",
      "Epoch 2/100\n",
      "1065/1065 [==============================] - 0s 319us/step - loss: 3727.6634 - val_loss: 3700.9366\n",
      "Epoch 3/100\n",
      "1065/1065 [==============================] - 0s 314us/step - loss: 3669.9888 - val_loss: 3696.7410\n",
      "Epoch 4/100\n",
      "1065/1065 [==============================] - 0s 322us/step - loss: 3611.5542 - val_loss: 3660.9321\n",
      "Epoch 5/100\n",
      "1065/1065 [==============================] - 0s 268us/step - loss: 3586.9983 - val_loss: 3628.2450\n",
      "Epoch 6/100\n",
      "1065/1065 [==============================] - 0s 282us/step - loss: 3565.5055 - val_loss: 3581.8079\n",
      "Epoch 7/100\n",
      "1065/1065 [==============================] - 0s 300us/step - loss: 3546.1167 - val_loss: 3531.0025\n",
      "Epoch 8/100\n",
      "1065/1065 [==============================] - 0s 310us/step - loss: 3531.0139 - val_loss: 3535.7230\n",
      "Epoch 9/100\n",
      "1065/1065 [==============================] - 0s 303us/step - loss: 3520.1478 - val_loss: 3566.2574\n",
      "Epoch 10/100\n",
      "1065/1065 [==============================] - 0s 312us/step - loss: 3514.7775 - val_loss: 3513.7740\n",
      "Epoch 11/100\n",
      "1065/1065 [==============================] - 0s 293us/step - loss: 3504.7318 - val_loss: 3519.8049\n",
      "Epoch 12/100\n",
      "1065/1065 [==============================] - 0s 269us/step - loss: 3498.4382 - val_loss: 3500.5584\n",
      "Epoch 13/100\n",
      "1065/1065 [==============================] - 0s 275us/step - loss: 3493.0559 - val_loss: 3521.3936\n",
      "Epoch 14/100\n",
      "1065/1065 [==============================] - 0s 268us/step - loss: 3488.3248 - val_loss: 3483.6098\n",
      "Epoch 15/100\n",
      "1065/1065 [==============================] - 0s 272us/step - loss: 3481.6230 - val_loss: 3505.1714\n",
      "Epoch 16/100\n",
      "1065/1065 [==============================] - 0s 272us/step - loss: 3477.8329 - val_loss: 3487.9098\n",
      "Epoch 17/100\n",
      "1065/1065 [==============================] - 0s 289us/step - loss: 3472.4531 - val_loss: 3472.6081\n",
      "Epoch 18/100\n",
      "1065/1065 [==============================] - 0s 278us/step - loss: 3467.5475 - val_loss: 3468.3965\n",
      "Epoch 19/100\n",
      "1065/1065 [==============================] - 0s 269us/step - loss: 3466.3942 - val_loss: 3487.2059\n",
      "Epoch 20/100\n",
      "1065/1065 [==============================] - 0s 275us/step - loss: 3466.3434 - val_loss: 3465.7783\n",
      "Epoch 21/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3459.4676 - val_loss: 3470.8678\n",
      "Epoch 22/100\n",
      "1065/1065 [==============================] - 0s 294us/step - loss: 3459.9149 - val_loss: 3480.7572\n",
      "Epoch 23/100\n",
      "1065/1065 [==============================] - 0s 265us/step - loss: 3452.6963 - val_loss: 3468.6065\n",
      "Epoch 24/100\n",
      "1065/1065 [==============================] - 0s 338us/step - loss: 3453.0926 - val_loss: 3455.2449\n",
      "Epoch 25/100\n",
      "1065/1065 [==============================] - 0s 322us/step - loss: 3449.1235 - val_loss: 3462.6367\n",
      "Epoch 26/100\n",
      "1065/1065 [==============================] - 0s 319us/step - loss: 3448.7371 - val_loss: 3451.6990\n",
      "Epoch 27/100\n",
      "1065/1065 [==============================] - 0s 321us/step - loss: 3444.0329 - val_loss: 3456.1756\n",
      "Epoch 28/100\n",
      "1065/1065 [==============================] - 0s 282us/step - loss: 3444.4053 - val_loss: 3454.1290\n",
      "Epoch 29/100\n",
      "1065/1065 [==============================] - 0s 297us/step - loss: 3443.2244 - val_loss: 3460.9300\n",
      "Epoch 30/100\n",
      "1065/1065 [==============================] - 0s 317us/step - loss: 3439.3820 - val_loss: 3451.0742\n",
      "Epoch 31/100\n",
      "1065/1065 [==============================] - 0s 345us/step - loss: 3438.7259 - val_loss: 3449.5586\n",
      "Epoch 32/100\n",
      "1065/1065 [==============================] - 0s 309us/step - loss: 3437.4611 - val_loss: 3439.1420\n",
      "Epoch 33/100\n",
      "1065/1065 [==============================] - 0s 310us/step - loss: 3433.2810 - val_loss: 3448.3161\n",
      "Epoch 34/100\n",
      "1065/1065 [==============================] - 0s 290us/step - loss: 3433.3339 - val_loss: 3446.9698\n",
      "Epoch 35/100\n",
      "1065/1065 [==============================] - 0s 266us/step - loss: 3433.9848 - val_loss: 3444.5297\n",
      "Epoch 36/100\n",
      "1065/1065 [==============================] - 0s 273us/step - loss: 3431.6208 - val_loss: 3442.3916\n",
      "Epoch 37/100\n",
      "1065/1065 [==============================] - 0s 273us/step - loss: 3430.4676 - val_loss: 3444.3366\n",
      "Epoch 38/100\n",
      "1065/1065 [==============================] - 0s 305us/step - loss: 3428.3841 - val_loss: 3437.3598\n",
      "Epoch 39/100\n",
      "1065/1065 [==============================] - 0s 308us/step - loss: 3426.3288 - val_loss: 3429.6959\n",
      "Epoch 40/100\n",
      "1065/1065 [==============================] - 0s 305us/step - loss: 3427.3196 - val_loss: 3433.9017\n",
      "Epoch 41/100\n",
      "1065/1065 [==============================] - 0s 281us/step - loss: 3425.1554 - val_loss: 3434.2137\n",
      "Epoch 42/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3422.4891 - val_loss: 3438.2673\n",
      "Epoch 43/100\n",
      "1065/1065 [==============================] - 0s 280us/step - loss: 3423.1257 - val_loss: 3430.4001\n",
      "Epoch 44/100\n",
      "1065/1065 [==============================] - 0s 277us/step - loss: 3423.5313 - val_loss: 3429.8104\n",
      "Epoch 45/100\n",
      "1065/1065 [==============================] - 0s 291us/step - loss: 3421.1122 - val_loss: 3430.2001\n",
      "Epoch 46/100\n",
      "1065/1065 [==============================] - 0s 291us/step - loss: 3420.9829 - val_loss: 3432.5292\n",
      "Epoch 47/100\n",
      "1065/1065 [==============================] - 0s 273us/step - loss: 3419.2256 - val_loss: 3430.6638\n",
      "Epoch 48/100\n",
      "1065/1065 [==============================] - 0s 284us/step - loss: 3417.2811 - val_loss: 3437.8264\n",
      "Epoch 49/100\n",
      "1065/1065 [==============================] - 0s 279us/step - loss: 3417.5482 - val_loss: 3431.0869\n",
      "Epoch 50/100\n",
      "1065/1065 [==============================] - 0s 275us/step - loss: 3415.2162 - val_loss: 3434.0768\n",
      "Epoch 51/100\n",
      "1065/1065 [==============================] - 0s 276us/step - loss: 3415.8955 - val_loss: 3420.9281\n",
      "Epoch 52/100\n",
      "1065/1065 [==============================] - 0s 277us/step - loss: 3416.2004 - val_loss: 3432.9693\n",
      "Epoch 53/100\n",
      "1065/1065 [==============================] - 0s 278us/step - loss: 3413.8700 - val_loss: 3421.5133\n",
      "Epoch 54/100\n",
      "1065/1065 [==============================] - 0s 274us/step - loss: 3411.7696 - val_loss: 3416.8631\n",
      "Epoch 55/100\n",
      "1065/1065 [==============================] - 0s 274us/step - loss: 3411.9020 - val_loss: 3416.7228\n",
      "Epoch 56/100\n",
      "1065/1065 [==============================] - 0s 270us/step - loss: 3411.4574 - val_loss: 3413.2693\n",
      "Epoch 57/100\n",
      "1065/1065 [==============================] - 0s 278us/step - loss: 3411.7998 - val_loss: 3415.6815\n",
      "Epoch 58/100\n",
      "1065/1065 [==============================] - 0s 275us/step - loss: 3410.6663 - val_loss: 3416.8503\n",
      "Epoch 59/100\n",
      "1065/1065 [==============================] - 0s 266us/step - loss: 3410.1519 - val_loss: 3416.9434\n",
      "Epoch 60/100\n",
      "1065/1065 [==============================] - 0s 272us/step - loss: 3409.8445 - val_loss: 3421.1789\n",
      "Epoch 61/100\n",
      "1065/1065 [==============================] - 0s 284us/step - loss: 3408.1150 - val_loss: 3413.6221\n",
      "Epoch 62/100\n",
      "1065/1065 [==============================] - 0s 305us/step - loss: 3410.5619 - val_loss: 3413.3904\n",
      "Epoch 63/100\n",
      "1065/1065 [==============================] - 0s 275us/step - loss: 3407.5168 - val_loss: 3408.0440\n",
      "Epoch 64/100\n",
      "1065/1065 [==============================] - 0s 282us/step - loss: 3408.8679 - val_loss: 3412.3006\n",
      "Epoch 65/100\n",
      "1065/1065 [==============================] - 0s 300us/step - loss: 3406.4198 - val_loss: 3412.7957\n",
      "Epoch 66/100\n",
      "1065/1065 [==============================] - 0s 297us/step - loss: 3407.1027 - val_loss: 3412.3110\n",
      "Epoch 67/100\n",
      "1065/1065 [==============================] - 0s 277us/step - loss: 3405.6104 - val_loss: 3415.0058\n",
      "Epoch 68/100\n",
      "1065/1065 [==============================] - 0s 294us/step - loss: 3404.8119 - val_loss: 3416.5366\n",
      "Epoch 69/100\n",
      "1065/1065 [==============================] - 0s 297us/step - loss: 3405.2872 - val_loss: 3411.2562\n",
      "Epoch 70/100\n",
      "1065/1065 [==============================] - 0s 281us/step - loss: 3403.5954 - val_loss: 3416.6001\n",
      "Epoch 71/100\n",
      "1065/1065 [==============================] - 0s 294us/step - loss: 3402.6262 - val_loss: 3411.0973\n",
      "Epoch 72/100\n",
      "1065/1065 [==============================] - 0s 270us/step - loss: 3402.5030 - val_loss: 3413.9398\n",
      "Epoch 73/100\n",
      "1065/1065 [==============================] - 0s 281us/step - loss: 3402.7494 - val_loss: 3414.4203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "1065/1065 [==============================] - 0s 265us/step - loss: 3401.3039 - val_loss: 3418.9302\n",
      "Epoch 75/100\n",
      "1065/1065 [==============================] - 0s 265us/step - loss: 3402.0089 - val_loss: 3412.3593\n",
      "Epoch 76/100\n",
      "1065/1065 [==============================] - 0s 270us/step - loss: 3400.9176 - val_loss: 3411.0192\n",
      "Epoch 77/100\n",
      "1065/1065 [==============================] - 0s 264us/step - loss: 3398.9742 - val_loss: 3407.2113\n",
      "Epoch 78/100\n",
      "1065/1065 [==============================] - 0s 266us/step - loss: 3401.8321 - val_loss: 3409.2232\n",
      "Epoch 79/100\n",
      "1065/1065 [==============================] - 0s 270us/step - loss: 3400.8940 - val_loss: 3407.1153\n",
      "Epoch 80/100\n",
      "1065/1065 [==============================] - 0s 274us/step - loss: 3400.5941 - val_loss: 3411.2410\n",
      "Epoch 81/100\n",
      "1065/1065 [==============================] - 0s 266us/step - loss: 3401.3043 - val_loss: 3410.3701\n",
      "Epoch 82/100\n",
      "1065/1065 [==============================] - 0s 278us/step - loss: 3397.5385 - val_loss: 3406.2681\n",
      "Epoch 83/100\n",
      "1065/1065 [==============================] - 0s 278us/step - loss: 3399.2005 - val_loss: 3408.4146\n",
      "Epoch 84/100\n",
      "1065/1065 [==============================] - 0s 285us/step - loss: 3399.3931 - val_loss: 3406.8815\n",
      "Epoch 85/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3398.9397 - val_loss: 3405.1724\n",
      "Epoch 86/100\n",
      "1065/1065 [==============================] - 0s 277us/step - loss: 3395.9540 - val_loss: 3409.3790\n",
      "Epoch 87/100\n",
      "1065/1065 [==============================] - 0s 269us/step - loss: 3396.8833 - val_loss: 3405.6139\n",
      "Epoch 88/100\n",
      "1065/1065 [==============================] - 0s 280us/step - loss: 3397.3803 - val_loss: 3405.6889\n",
      "Epoch 89/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3397.6460 - val_loss: 3404.8835\n",
      "Epoch 90/100\n",
      "1065/1065 [==============================] - 0s 267us/step - loss: 3396.0421 - val_loss: 3405.0660\n",
      "Epoch 91/100\n",
      "1065/1065 [==============================] - 0s 277us/step - loss: 3396.2252 - val_loss: 3405.2256\n",
      "Epoch 92/100\n",
      "1065/1065 [==============================] - 0s 267us/step - loss: 3395.4894 - val_loss: 3402.9096\n",
      "Epoch 93/100\n",
      "1065/1065 [==============================] - 0s 273us/step - loss: 3395.3711 - val_loss: 3401.8476\n",
      "Epoch 94/100\n",
      "1065/1065 [==============================] - 0s 269us/step - loss: 3393.7390 - val_loss: 3404.4784\n",
      "Epoch 95/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3394.7933 - val_loss: 3402.5610\n",
      "Epoch 96/100\n",
      "1065/1065 [==============================] - 0s 271us/step - loss: 3393.9564 - val_loss: 3403.0447\n",
      "Epoch 97/100\n",
      "1065/1065 [==============================] - 0s 264us/step - loss: 3394.2883 - val_loss: 3405.2847\n",
      "Epoch 98/100\n",
      "1065/1065 [==============================] - 0s 279us/step - loss: 3393.1733 - val_loss: 3398.0792\n",
      "Epoch 99/100\n",
      "1065/1065 [==============================] - 0s 315us/step - loss: 3393.0567 - val_loss: 3402.2906\n",
      "Epoch 100/100\n",
      "1065/1065 [==============================] - 0s 285us/step - loss: 3392.4890 - val_loss: 3403.4099\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# Separate encoder and decoder components of the model\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Model includes all layers in the computation of rnaseq_input GIVEN z_mean_encoded\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76029481d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "model_encoder_json = encoder.to_json()\n",
    "with open(model_encoder_file, \"w\") as json_file:\n",
    "    json_file.write(model_encoder_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_to_reconstruct(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "model_decoder_json = decoder.to_json()\n",
    "with open(model_decoder_file, \"w\") as json_file:\n",
    "    json_file.write(model_decoder_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
