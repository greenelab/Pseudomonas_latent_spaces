{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with 2-hidden layers\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# To ensure reproducibility using Keras during development\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "randomState = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"oxygen_level\", \"train_model_input.txt.xz\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0, header=0, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "epochs = 500\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "intermediate_dim = 100\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"oxygen_level\", \"tybalt_2layer_{}latent_stats.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"oxygen_level\", \"tybalt_2layer_{}latent_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"oxygen_level\", \"train_input_2layer_{}latent_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"oxygen_level\", \"tybalt_2layer_{}latent_encoder_model.h5\".format(latent_dim))\n",
    "weights_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"oxygen_level\", \"tybalt_2layer_{}latent_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"oxygen_level\", \"tybalt_2layer_{}latent_decoder_model.h5\".format(latent_dim))\n",
    "weights_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"oxygen_level\", \"tybalt_2layer_{}latent_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent, random_state = randomState)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:74: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "hidden_dense_linear = Dense(intermediate_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "hidden_dense_batchnorm = BatchNormalization()(hidden_dense_linear)\n",
    "hidden_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "# Note:\n",
    "# Normalize and relu filter at each layer adds non-linear component (relu is non-linear function)\n",
    "# If architecture is layer-layer-normalization-relu then the computation is still linear\n",
    "# Add additional layers in triplicate\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_model = Sequential()\n",
    "decoder_model.add(Dense(intermediate_dim, activation='relu', input_dim=latent_dim))\n",
    "decoder_model.add(Dense(original_dim, activation='sigmoid'))\n",
    "rnaseq_reconstruct = decoder_model(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1061 samples, validate on 118 samples\n",
      "Epoch 1/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3750.1025 - val_loss: 3595.9475\n",
      "Epoch 2/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3587.8393 - val_loss: 3580.5758\n",
      "Epoch 3/500\n",
      "1061/1061 [==============================] - 1s 901us/step - loss: 3526.6957 - val_loss: 3648.0841\n",
      "Epoch 4/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3506.5969 - val_loss: 3629.4877\n",
      "Epoch 5/500\n",
      "1061/1061 [==============================] - 1s 888us/step - loss: 3493.7799 - val_loss: 3550.2845\n",
      "Epoch 6/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3485.8599 - val_loss: 3505.7038\n",
      "Epoch 7/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3483.1250 - val_loss: 3476.6030\n",
      "Epoch 8/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3478.9141 - val_loss: 3454.1565\n",
      "Epoch 9/500\n",
      "1061/1061 [==============================] - 1s 882us/step - loss: 3473.7207 - val_loss: 3454.5432\n",
      "Epoch 10/500\n",
      "1061/1061 [==============================] - 1s 875us/step - loss: 3471.9749 - val_loss: 3453.4960\n",
      "Epoch 11/500\n",
      "1061/1061 [==============================] - 1s 895us/step - loss: 3468.4967 - val_loss: 3448.8599\n",
      "Epoch 12/500\n",
      "1061/1061 [==============================] - 1s 877us/step - loss: 3465.3804 - val_loss: 3439.0716\n",
      "Epoch 13/500\n",
      "1061/1061 [==============================] - 1s 959us/step - loss: 3465.8902 - val_loss: 3447.0363\n",
      "Epoch 14/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3463.0393 - val_loss: 3436.2091\n",
      "Epoch 15/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3459.9244 - val_loss: 3437.7550\n",
      "Epoch 16/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3461.6819 - val_loss: 3440.4119\n",
      "Epoch 17/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3455.5473 - val_loss: 3452.0314\n",
      "Epoch 18/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3454.7469 - val_loss: 3447.6753\n",
      "Epoch 19/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3450.6541 - val_loss: 3446.6025\n",
      "Epoch 20/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3449.9905 - val_loss: 3433.8542\n",
      "Epoch 21/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3448.0175 - val_loss: 3439.5120\n",
      "Epoch 22/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3445.4763 - val_loss: 3432.3469\n",
      "Epoch 23/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3444.5690 - val_loss: 3427.1974\n",
      "Epoch 24/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3442.7840 - val_loss: 3429.6591\n",
      "Epoch 25/500\n",
      "1061/1061 [==============================] - 1s 944us/step - loss: 3441.3960 - val_loss: 3422.3923\n",
      "Epoch 26/500\n",
      "1061/1061 [==============================] - 1s 914us/step - loss: 3443.4304 - val_loss: 3415.9091\n",
      "Epoch 27/500\n",
      "1061/1061 [==============================] - 1s 937us/step - loss: 3436.4053 - val_loss: 3418.0921\n",
      "Epoch 28/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3436.4298 - val_loss: 3417.4655\n",
      "Epoch 29/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3435.2082 - val_loss: 3422.2404\n",
      "Epoch 30/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3432.4744 - val_loss: 3415.6731\n",
      "Epoch 31/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3435.7982 - val_loss: 3416.4001\n",
      "Epoch 32/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3432.0989 - val_loss: 3418.1349\n",
      "Epoch 33/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3429.9948 - val_loss: 3410.9026\n",
      "Epoch 34/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3427.7039 - val_loss: 3404.3780\n",
      "Epoch 35/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3429.2542 - val_loss: 3405.4156\n",
      "Epoch 36/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3426.1292 - val_loss: 3406.1458\n",
      "Epoch 37/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3425.7543 - val_loss: 3409.6050\n",
      "Epoch 38/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3424.5764 - val_loss: 3400.8707\n",
      "Epoch 39/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3422.3644 - val_loss: 3400.0701\n",
      "Epoch 40/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3421.0409 - val_loss: 3399.1817\n",
      "Epoch 41/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3421.3204 - val_loss: 3393.3737\n",
      "Epoch 42/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3418.3845 - val_loss: 3391.7826\n",
      "Epoch 43/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3419.0286 - val_loss: 3390.2317\n",
      "Epoch 44/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3416.0642 - val_loss: 3390.8616\n",
      "Epoch 45/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3414.6960 - val_loss: 3386.9777\n",
      "Epoch 46/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3413.9091 - val_loss: 3390.5419\n",
      "Epoch 47/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3411.7814 - val_loss: 3389.5731\n",
      "Epoch 48/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3412.4623 - val_loss: 3387.5127\n",
      "Epoch 49/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3410.0335 - val_loss: 3387.1318\n",
      "Epoch 50/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3409.7216 - val_loss: 3386.1331\n",
      "Epoch 51/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3408.7361 - val_loss: 3386.3389\n",
      "Epoch 52/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3407.2594 - val_loss: 3387.8533\n",
      "Epoch 53/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3406.9114 - val_loss: 3383.2415\n",
      "Epoch 54/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3405.9922 - val_loss: 3382.6658\n",
      "Epoch 55/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3405.9104 - val_loss: 3377.4485\n",
      "Epoch 56/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3404.4000 - val_loss: 3379.3378\n",
      "Epoch 57/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3403.0259 - val_loss: 3380.0972\n",
      "Epoch 58/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3403.8336 - val_loss: 3381.1675\n",
      "Epoch 59/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3401.6485 - val_loss: 3376.8523\n",
      "Epoch 60/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3401.1337 - val_loss: 3378.0942\n",
      "Epoch 61/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3401.5393 - val_loss: 3378.1159\n",
      "Epoch 62/500\n",
      "1061/1061 [==============================] - 1s 963us/step - loss: 3399.9803 - val_loss: 3379.0491\n",
      "Epoch 63/500\n",
      "1061/1061 [==============================] - 1s 937us/step - loss: 3400.0030 - val_loss: 3374.6573\n",
      "Epoch 64/500\n",
      "1061/1061 [==============================] - 1s 923us/step - loss: 3398.8460 - val_loss: 3375.7828\n",
      "Epoch 65/500\n",
      "1061/1061 [==============================] - 1s 951us/step - loss: 3397.7049 - val_loss: 3373.6357\n",
      "Epoch 66/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3397.1413 - val_loss: 3374.2449\n",
      "Epoch 67/500\n",
      "1061/1061 [==============================] - 1s 973us/step - loss: 3396.5685 - val_loss: 3376.1242\n",
      "Epoch 68/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3394.7727 - val_loss: 3372.5504\n",
      "Epoch 69/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3396.1247 - val_loss: 3374.2948\n",
      "Epoch 70/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3395.2879 - val_loss: 3372.3520\n",
      "Epoch 71/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3393.8848 - val_loss: 3369.5069\n",
      "Epoch 72/500\n",
      "1061/1061 [==============================] - 1s 987us/step - loss: 3394.0738 - val_loss: 3368.4355\n",
      "Epoch 73/500\n",
      "1061/1061 [==============================] - 1s 987us/step - loss: 3394.1891 - val_loss: 3369.1392\n",
      "Epoch 74/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061/1061 [==============================] - 1s 959us/step - loss: 3390.4417 - val_loss: 3366.2920\n",
      "Epoch 75/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3392.4213 - val_loss: 3366.3049\n",
      "Epoch 76/500\n",
      "1061/1061 [==============================] - 1s 979us/step - loss: 3391.8411 - val_loss: 3368.0148\n",
      "Epoch 77/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3388.9897 - val_loss: 3370.8780\n",
      "Epoch 78/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3389.2491 - val_loss: 3363.5315\n",
      "Epoch 79/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3389.5229 - val_loss: 3364.2948\n",
      "Epoch 80/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3388.1799 - val_loss: 3365.9687\n",
      "Epoch 81/500\n",
      "1061/1061 [==============================] - 1s 959us/step - loss: 3387.0526 - val_loss: 3361.7317\n",
      "Epoch 82/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3387.9043 - val_loss: 3363.1355\n",
      "Epoch 83/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3386.6129 - val_loss: 3366.4051\n",
      "Epoch 84/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3387.2369 - val_loss: 3366.5219\n",
      "Epoch 85/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3386.2843 - val_loss: 3367.6266\n",
      "Epoch 86/500\n",
      "1061/1061 [==============================] - 1s 972us/step - loss: 3384.2302 - val_loss: 3362.7641\n",
      "Epoch 87/500\n",
      "1061/1061 [==============================] - 1s 929us/step - loss: 3383.1321 - val_loss: 3363.2848\n",
      "Epoch 88/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3385.0497 - val_loss: 3360.8555\n",
      "Epoch 89/500\n",
      "1061/1061 [==============================] - 1s 940us/step - loss: 3382.0853 - val_loss: 3359.1011\n",
      "Epoch 90/500\n",
      "1061/1061 [==============================] - 1s 931us/step - loss: 3382.5611 - val_loss: 3358.4625\n",
      "Epoch 91/500\n",
      "1061/1061 [==============================] - 1s 953us/step - loss: 3382.5817 - val_loss: 3359.0652\n",
      "Epoch 92/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3381.9035 - val_loss: 3360.3820\n",
      "Epoch 93/500\n",
      "1061/1061 [==============================] - 1s 914us/step - loss: 3381.2962 - val_loss: 3361.3271\n",
      "Epoch 94/500\n",
      "1061/1061 [==============================] - 1s 904us/step - loss: 3381.9284 - val_loss: 3357.6816\n",
      "Epoch 95/500\n",
      "1061/1061 [==============================] - 1s 908us/step - loss: 3380.0971 - val_loss: 3359.2672\n",
      "Epoch 96/500\n",
      "1061/1061 [==============================] - 1s 979us/step - loss: 3377.5929 - val_loss: 3356.8091\n",
      "Epoch 97/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3378.9328 - val_loss: 3357.2072\n",
      "Epoch 98/500\n",
      "1061/1061 [==============================] - 1s 929us/step - loss: 3377.7461 - val_loss: 3356.5138\n",
      "Epoch 99/500\n",
      "1061/1061 [==============================] - 1s 908us/step - loss: 3378.2583 - val_loss: 3355.5736\n",
      "Epoch 100/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3377.4951 - val_loss: 3356.9233\n",
      "Epoch 101/500\n",
      "1061/1061 [==============================] - 1s 977us/step - loss: 3377.8473 - val_loss: 3352.6508\n",
      "Epoch 102/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3375.5064 - val_loss: 3353.9579\n",
      "Epoch 103/500\n",
      "1061/1061 [==============================] - 1s 988us/step - loss: 3376.2284 - val_loss: 3351.7262\n",
      "Epoch 104/500\n",
      "1061/1061 [==============================] - 1s 979us/step - loss: 3374.5697 - val_loss: 3351.0165\n",
      "Epoch 105/500\n",
      "1061/1061 [==============================] - 1s 955us/step - loss: 3375.0635 - val_loss: 3353.3721\n",
      "Epoch 106/500\n",
      "1061/1061 [==============================] - 1s 978us/step - loss: 3375.0218 - val_loss: 3352.4366\n",
      "Epoch 107/500\n",
      "1061/1061 [==============================] - 1s 936us/step - loss: 3373.4113 - val_loss: 3350.5963\n",
      "Epoch 108/500\n",
      "1061/1061 [==============================] - 1s 958us/step - loss: 3373.1342 - val_loss: 3349.4984\n",
      "Epoch 109/500\n",
      "1061/1061 [==============================] - 1s 954us/step - loss: 3372.6590 - val_loss: 3352.9435\n",
      "Epoch 110/500\n",
      "1061/1061 [==============================] - 1s 956us/step - loss: 3372.1294 - val_loss: 3351.6163\n",
      "Epoch 111/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3372.3808 - val_loss: 3350.8869\n",
      "Epoch 112/500\n",
      "1061/1061 [==============================] - 1s 977us/step - loss: 3372.4891 - val_loss: 3354.7106\n",
      "Epoch 113/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3371.3075 - val_loss: 3353.0579\n",
      "Epoch 114/500\n",
      "1061/1061 [==============================] - 1s 988us/step - loss: 3371.7301 - val_loss: 3354.2748\n",
      "Epoch 115/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3371.7382 - val_loss: 3351.0614\n",
      "Epoch 116/500\n",
      "1061/1061 [==============================] - 1s 912us/step - loss: 3370.0828 - val_loss: 3348.0615\n",
      "Epoch 117/500\n",
      "1061/1061 [==============================] - 1s 922us/step - loss: 3369.7358 - val_loss: 3348.0088\n",
      "Epoch 118/500\n",
      "1061/1061 [==============================] - 1s 927us/step - loss: 3369.4837 - val_loss: 3348.9500\n",
      "Epoch 119/500\n",
      "1061/1061 [==============================] - 1s 984us/step - loss: 3368.9010 - val_loss: 3351.7494\n",
      "Epoch 120/500\n",
      "1061/1061 [==============================] - 1s 913us/step - loss: 3369.6785 - val_loss: 3348.7185\n",
      "Epoch 121/500\n",
      "1061/1061 [==============================] - 1s 940us/step - loss: 3368.8775 - val_loss: 3347.9323\n",
      "Epoch 122/500\n",
      "1061/1061 [==============================] - 1s 930us/step - loss: 3368.3640 - val_loss: 3346.4417\n",
      "Epoch 123/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3367.2863 - val_loss: 3346.6779\n",
      "Epoch 124/500\n",
      "1061/1061 [==============================] - 1s 934us/step - loss: 3367.4568 - val_loss: 3349.0878\n",
      "Epoch 125/500\n",
      "1061/1061 [==============================] - 1s 924us/step - loss: 3366.7739 - val_loss: 3349.9187\n",
      "Epoch 126/500\n",
      "1061/1061 [==============================] - 1s 884us/step - loss: 3366.9259 - val_loss: 3348.3611\n",
      "Epoch 127/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3365.8745 - val_loss: 3348.0555\n",
      "Epoch 128/500\n",
      "1061/1061 [==============================] - 1s 923us/step - loss: 3366.5268 - val_loss: 3351.2794\n",
      "Epoch 129/500\n",
      "1061/1061 [==============================] - 1s 951us/step - loss: 3365.9892 - val_loss: 3351.5543\n",
      "Epoch 130/500\n",
      "1061/1061 [==============================] - 1s 931us/step - loss: 3365.4962 - val_loss: 3350.0115\n",
      "Epoch 131/500\n",
      "1061/1061 [==============================] - 1s 933us/step - loss: 3364.0700 - val_loss: 3346.9631\n",
      "Epoch 132/500\n",
      "1061/1061 [==============================] - 1s 907us/step - loss: 3364.8439 - val_loss: 3346.1863\n",
      "Epoch 133/500\n",
      "1061/1061 [==============================] - 1s 932us/step - loss: 3364.5983 - val_loss: 3344.5253\n",
      "Epoch 134/500\n",
      "1061/1061 [==============================] - 1s 924us/step - loss: 3362.8709 - val_loss: 3345.1083\n",
      "Epoch 135/500\n",
      "1061/1061 [==============================] - 1s 920us/step - loss: 3363.5154 - val_loss: 3346.5492\n",
      "Epoch 136/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3361.4742 - val_loss: 3348.1548\n",
      "Epoch 137/500\n",
      "1061/1061 [==============================] - 1s 918us/step - loss: 3363.2524 - val_loss: 3345.8270\n",
      "Epoch 138/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3363.4814 - val_loss: 3343.8450\n",
      "Epoch 139/500\n",
      "1061/1061 [==============================] - 1s 951us/step - loss: 3362.2276 - val_loss: 3340.3384\n",
      "Epoch 140/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3360.6523 - val_loss: 3342.1882\n",
      "Epoch 141/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3361.1437 - val_loss: 3344.1963\n",
      "Epoch 142/500\n",
      "1061/1061 [==============================] - 1s 831us/step - loss: 3361.2683 - val_loss: 3345.6766\n",
      "Epoch 143/500\n",
      "1061/1061 [==============================] - 1s 891us/step - loss: 3360.9997 - val_loss: 3346.5629\n",
      "Epoch 144/500\n",
      "1061/1061 [==============================] - 1s 914us/step - loss: 3361.1840 - val_loss: 3340.0877\n",
      "Epoch 145/500\n",
      "1061/1061 [==============================] - 1s 922us/step - loss: 3359.8878 - val_loss: 3341.4547\n",
      "Epoch 146/500\n",
      "1061/1061 [==============================] - 1s 907us/step - loss: 3361.0309 - val_loss: 3342.2400\n",
      "Epoch 147/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061/1061 [==============================] - 1s 943us/step - loss: 3361.0503 - val_loss: 3340.8426\n",
      "Epoch 148/500\n",
      "1061/1061 [==============================] - 1s 913us/step - loss: 3359.0168 - val_loss: 3340.3242\n",
      "Epoch 149/500\n",
      "1061/1061 [==============================] - 1s 915us/step - loss: 3359.6738 - val_loss: 3344.3498\n",
      "Epoch 150/500\n",
      "1061/1061 [==============================] - 1s 906us/step - loss: 3358.5594 - val_loss: 3342.1230\n",
      "Epoch 151/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3359.0117 - val_loss: 3343.7702\n",
      "Epoch 152/500\n",
      "1061/1061 [==============================] - 1s 894us/step - loss: 3358.1419 - val_loss: 3342.4411\n",
      "Epoch 153/500\n",
      "1061/1061 [==============================] - 1s 929us/step - loss: 3357.8195 - val_loss: 3341.9423\n",
      "Epoch 154/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3358.1234 - val_loss: 3339.4992\n",
      "Epoch 155/500\n",
      "1061/1061 [==============================] - 1s 914us/step - loss: 3357.5398 - val_loss: 3340.3730\n",
      "Epoch 156/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3358.6479 - val_loss: 3337.6552\n",
      "Epoch 157/500\n",
      "1061/1061 [==============================] - 1s 911us/step - loss: 3357.5563 - val_loss: 3342.1409\n",
      "Epoch 158/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3356.3552 - val_loss: 3339.1093\n",
      "Epoch 159/500\n",
      "1061/1061 [==============================] - 1s 904us/step - loss: 3356.9067 - val_loss: 3339.8159\n",
      "Epoch 160/500\n",
      "1061/1061 [==============================] - 1s 940us/step - loss: 3356.8397 - val_loss: 3338.8474\n",
      "Epoch 161/500\n",
      "1061/1061 [==============================] - 1s 939us/step - loss: 3356.3641 - val_loss: 3336.5254\n",
      "Epoch 162/500\n",
      "1061/1061 [==============================] - 1s 957us/step - loss: 3355.6661 - val_loss: 3336.9027\n",
      "Epoch 163/500\n",
      "1061/1061 [==============================] - 1s 930us/step - loss: 3355.1248 - val_loss: 3337.2324\n",
      "Epoch 164/500\n",
      "1061/1061 [==============================] - 1s 953us/step - loss: 3354.8839 - val_loss: 3338.9090\n",
      "Epoch 165/500\n",
      "1061/1061 [==============================] - 1s 921us/step - loss: 3354.2035 - val_loss: 3339.8575\n",
      "Epoch 166/500\n",
      "1061/1061 [==============================] - 1s 886us/step - loss: 3355.2223 - val_loss: 3336.0192\n",
      "Epoch 167/500\n",
      "1061/1061 [==============================] - 1s 866us/step - loss: 3354.4715 - val_loss: 3338.1927\n",
      "Epoch 168/500\n",
      "1061/1061 [==============================] - 1s 864us/step - loss: 3353.2039 - val_loss: 3339.1537\n",
      "Epoch 169/500\n",
      "1061/1061 [==============================] - 1s 915us/step - loss: 3353.3603 - val_loss: 3339.1341\n",
      "Epoch 170/500\n",
      "1061/1061 [==============================] - 1s 930us/step - loss: 3354.2685 - val_loss: 3339.1688\n",
      "Epoch 171/500\n",
      "1061/1061 [==============================] - 1s 906us/step - loss: 3352.4829 - val_loss: 3339.1807\n",
      "Epoch 172/500\n",
      "1061/1061 [==============================] - 1s 867us/step - loss: 3353.9887 - val_loss: 3335.8703\n",
      "Epoch 173/500\n",
      "1061/1061 [==============================] - 1s 884us/step - loss: 3353.6087 - val_loss: 3335.0933\n",
      "Epoch 174/500\n",
      "1061/1061 [==============================] - 1s 863us/step - loss: 3351.6610 - val_loss: 3334.1881\n",
      "Epoch 175/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3354.2500 - val_loss: 3335.5059\n",
      "Epoch 176/500\n",
      "1061/1061 [==============================] - 1s 911us/step - loss: 3352.8395 - val_loss: 3334.6091\n",
      "Epoch 177/500\n",
      "1061/1061 [==============================] - 1s 879us/step - loss: 3351.6445 - val_loss: 3336.0276\n",
      "Epoch 178/500\n",
      "1061/1061 [==============================] - 1s 879us/step - loss: 3352.8880 - val_loss: 3334.7082\n",
      "Epoch 179/500\n",
      "1061/1061 [==============================] - 1s 843us/step - loss: 3352.3473 - val_loss: 3332.7122\n",
      "Epoch 180/500\n",
      "1061/1061 [==============================] - 1s 897us/step - loss: 3351.1131 - val_loss: 3334.7052\n",
      "Epoch 181/500\n",
      "1061/1061 [==============================] - 1s 919us/step - loss: 3352.3176 - val_loss: 3333.8608\n",
      "Epoch 182/500\n",
      "1061/1061 [==============================] - 1s 893us/step - loss: 3351.7687 - val_loss: 3335.6013\n",
      "Epoch 183/500\n",
      "1061/1061 [==============================] - 1s 879us/step - loss: 3350.4604 - val_loss: 3331.1547\n",
      "Epoch 184/500\n",
      "1061/1061 [==============================] - 1s 864us/step - loss: 3350.8279 - val_loss: 3335.0259\n",
      "Epoch 185/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3350.1443 - val_loss: 3331.6209\n",
      "Epoch 186/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3350.3231 - val_loss: 3331.8608\n",
      "Epoch 187/500\n",
      "1061/1061 [==============================] - 1s 972us/step - loss: 3349.3562 - val_loss: 3333.0576\n",
      "Epoch 188/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3350.2774 - val_loss: 3329.1083\n",
      "Epoch 189/500\n",
      "1061/1061 [==============================] - 1s 984us/step - loss: 3350.6220 - val_loss: 3334.0729\n",
      "Epoch 190/500\n",
      "1061/1061 [==============================] - 1s 886us/step - loss: 3349.2547 - val_loss: 3331.5748\n",
      "Epoch 191/500\n",
      "1061/1061 [==============================] - 1s 913us/step - loss: 3348.3686 - val_loss: 3330.6745\n",
      "Epoch 192/500\n",
      "1061/1061 [==============================] - 1s 897us/step - loss: 3348.3777 - val_loss: 3331.4101\n",
      "Epoch 193/500\n",
      "1061/1061 [==============================] - 1s 888us/step - loss: 3347.0952 - val_loss: 3331.9489\n",
      "Epoch 194/500\n",
      "1061/1061 [==============================] - 1s 912us/step - loss: 3348.3585 - val_loss: 3332.0739\n",
      "Epoch 195/500\n",
      "1061/1061 [==============================] - 1s 914us/step - loss: 3348.5367 - val_loss: 3331.8239\n",
      "Epoch 196/500\n",
      "1061/1061 [==============================] - 1s 885us/step - loss: 3347.0593 - val_loss: 3330.3744\n",
      "Epoch 197/500\n",
      "1061/1061 [==============================] - 1s 909us/step - loss: 3347.5152 - val_loss: 3332.0135\n",
      "Epoch 198/500\n",
      "1061/1061 [==============================] - 1s 860us/step - loss: 3347.5405 - val_loss: 3333.8079\n",
      "Epoch 199/500\n",
      "1061/1061 [==============================] - 1s 868us/step - loss: 3346.1262 - val_loss: 3330.4337\n",
      "Epoch 200/500\n",
      "1061/1061 [==============================] - 1s 906us/step - loss: 3349.5615 - val_loss: 3331.3489\n",
      "Epoch 201/500\n",
      "1061/1061 [==============================] - 1s 897us/step - loss: 3348.0615 - val_loss: 3329.1451\n",
      "Epoch 202/500\n",
      "1061/1061 [==============================] - 1s 915us/step - loss: 3346.8707 - val_loss: 3329.6143\n",
      "Epoch 203/500\n",
      "1061/1061 [==============================] - 1s 898us/step - loss: 3346.2919 - val_loss: 3331.8052\n",
      "Epoch 204/500\n",
      "1061/1061 [==============================] - 1s 859us/step - loss: 3345.9007 - val_loss: 3330.0626\n",
      "Epoch 205/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3346.8886 - val_loss: 3329.6044\n",
      "Epoch 206/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3345.3721 - val_loss: 3329.3681\n",
      "Epoch 207/500\n",
      "1061/1061 [==============================] - 1s 927us/step - loss: 3345.9191 - val_loss: 3327.4977\n",
      "Epoch 208/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3346.5522 - val_loss: 3329.5843\n",
      "Epoch 209/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3345.2977 - val_loss: 3329.6993\n",
      "Epoch 210/500\n",
      "1061/1061 [==============================] - 1s 922us/step - loss: 3344.8911 - val_loss: 3327.9712\n",
      "Epoch 211/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3344.3393 - val_loss: 3332.3790\n",
      "Epoch 212/500\n",
      "1061/1061 [==============================] - 1s 928us/step - loss: 3346.0519 - val_loss: 3331.4484\n",
      "Epoch 213/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3343.8817 - val_loss: 3325.5149\n",
      "Epoch 214/500\n",
      "1061/1061 [==============================] - 1s 906us/step - loss: 3343.8072 - val_loss: 3328.7241\n",
      "Epoch 215/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3344.3237 - val_loss: 3330.0969\n",
      "Epoch 216/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3343.6354 - val_loss: 3329.0287\n",
      "Epoch 217/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3344.6766 - val_loss: 3328.2171\n",
      "Epoch 218/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3345.0180 - val_loss: 3330.2094\n",
      "Epoch 219/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061/1061 [==============================] - 1s 890us/step - loss: 3343.7503 - val_loss: 3328.2617\n",
      "Epoch 220/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3343.8693 - val_loss: 3324.4758\n",
      "Epoch 221/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3344.7925 - val_loss: 3324.5782\n",
      "Epoch 222/500\n",
      "1061/1061 [==============================] - 1s 906us/step - loss: 3341.8474 - val_loss: 3325.0796\n",
      "Epoch 223/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3343.5876 - val_loss: 3328.4225\n",
      "Epoch 224/500\n",
      "1061/1061 [==============================] - 1s 884us/step - loss: 3342.6914 - val_loss: 3325.1673\n",
      "Epoch 225/500\n",
      "1061/1061 [==============================] - 1s 879us/step - loss: 3342.7899 - val_loss: 3326.5098\n",
      "Epoch 226/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3342.3244 - val_loss: 3326.4549\n",
      "Epoch 227/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3343.1666 - val_loss: 3326.3680\n",
      "Epoch 228/500\n",
      "1061/1061 [==============================] - 1s 911us/step - loss: 3341.9427 - val_loss: 3327.2266\n",
      "Epoch 229/500\n",
      "1061/1061 [==============================] - 1s 879us/step - loss: 3344.0445 - val_loss: 3327.5361\n",
      "Epoch 230/500\n",
      "1061/1061 [==============================] - 1s 871us/step - loss: 3343.9904 - val_loss: 3326.7781\n",
      "Epoch 231/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3341.8502 - val_loss: 3326.2661\n",
      "Epoch 232/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3342.3471 - val_loss: 3324.2922\n",
      "Epoch 233/500\n",
      "1061/1061 [==============================] - 1s 933us/step - loss: 3342.2363 - val_loss: 3324.5721\n",
      "Epoch 234/500\n",
      "1061/1061 [==============================] - 1s 907us/step - loss: 3340.9151 - val_loss: 3326.3493\n",
      "Epoch 235/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3340.7307 - val_loss: 3324.0344\n",
      "Epoch 236/500\n",
      "1061/1061 [==============================] - 1s 942us/step - loss: 3341.6582 - val_loss: 3328.9385\n",
      "Epoch 237/500\n",
      "1061/1061 [==============================] - 1s 938us/step - loss: 3340.9741 - val_loss: 3325.3133\n",
      "Epoch 238/500\n",
      "1061/1061 [==============================] - 1s 864us/step - loss: 3340.5567 - val_loss: 3323.2970\n",
      "Epoch 239/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3340.4844 - val_loss: 3325.4781\n",
      "Epoch 240/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3339.4869 - val_loss: 3324.5485\n",
      "Epoch 241/500\n",
      "1061/1061 [==============================] - 1s 840us/step - loss: 3340.6039 - val_loss: 3326.2561\n",
      "Epoch 242/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3340.7839 - val_loss: 3322.1251\n",
      "Epoch 243/500\n",
      "1061/1061 [==============================] - 1s 911us/step - loss: 3340.3226 - val_loss: 3322.4052\n",
      "Epoch 244/500\n",
      "1061/1061 [==============================] - 1s 918us/step - loss: 3340.2162 - val_loss: 3327.1424\n",
      "Epoch 245/500\n",
      "1061/1061 [==============================] - 1s 897us/step - loss: 3339.1884 - val_loss: 3321.6208\n",
      "Epoch 246/500\n",
      "1061/1061 [==============================] - 1s 867us/step - loss: 3339.7951 - val_loss: 3323.4035\n",
      "Epoch 247/500\n",
      "1061/1061 [==============================] - 1s 895us/step - loss: 3342.1338 - val_loss: 3322.9832\n",
      "Epoch 248/500\n",
      "1061/1061 [==============================] - 1s 909us/step - loss: 3339.6538 - val_loss: 3321.6992\n",
      "Epoch 249/500\n",
      "1061/1061 [==============================] - 1s 915us/step - loss: 3337.1569 - val_loss: 3322.5095\n",
      "Epoch 250/500\n",
      "1061/1061 [==============================] - 1s 929us/step - loss: 3338.3616 - val_loss: 3322.5712\n",
      "Epoch 251/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3337.5994 - val_loss: 3321.8674\n",
      "Epoch 252/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3337.8269 - val_loss: 3321.2386\n",
      "Epoch 253/500\n",
      "1061/1061 [==============================] - 1s 909us/step - loss: 3338.1609 - val_loss: 3324.0120\n",
      "Epoch 254/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3337.2168 - val_loss: 3319.5390\n",
      "Epoch 255/500\n",
      "1061/1061 [==============================] - 1s 907us/step - loss: 3337.6705 - val_loss: 3321.4421\n",
      "Epoch 256/500\n",
      "1061/1061 [==============================] - 1s 907us/step - loss: 3336.8829 - val_loss: 3320.5486\n",
      "Epoch 257/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3336.2481 - val_loss: 3321.3201\n",
      "Epoch 258/500\n",
      "1061/1061 [==============================] - 1s 893us/step - loss: 3336.7586 - val_loss: 3320.0915\n",
      "Epoch 259/500\n",
      "1061/1061 [==============================] - 1s 888us/step - loss: 3338.7605 - val_loss: 3323.0193\n",
      "Epoch 260/500\n",
      "1061/1061 [==============================] - 1s 933us/step - loss: 3337.9260 - val_loss: 3324.6466\n",
      "Epoch 261/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3336.3273 - val_loss: 3320.5763\n",
      "Epoch 262/500\n",
      "1061/1061 [==============================] - 1s 927us/step - loss: 3339.0357 - val_loss: 3321.4708\n",
      "Epoch 263/500\n",
      "1061/1061 [==============================] - 1s 896us/step - loss: 3337.3545 - val_loss: 3320.5394\n",
      "Epoch 264/500\n",
      "1061/1061 [==============================] - 1s 898us/step - loss: 3338.1831 - val_loss: 3320.6001\n",
      "Epoch 265/500\n",
      "1061/1061 [==============================] - 1s 824us/step - loss: 3336.8047 - val_loss: 3319.2230\n",
      "Epoch 266/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3336.3542 - val_loss: 3319.7875\n",
      "Epoch 267/500\n",
      "1061/1061 [==============================] - 1s 871us/step - loss: 3335.0287 - val_loss: 3322.5833\n",
      "Epoch 268/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3336.8664 - val_loss: 3317.1131\n",
      "Epoch 269/500\n",
      "1061/1061 [==============================] - 1s 885us/step - loss: 3334.5964 - val_loss: 3319.1037\n",
      "Epoch 270/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3336.3904 - val_loss: 3318.4526\n",
      "Epoch 271/500\n",
      "1061/1061 [==============================] - 1s 928us/step - loss: 3336.3255 - val_loss: 3322.1316\n",
      "Epoch 272/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3337.0534 - val_loss: 3323.2731\n",
      "Epoch 273/500\n",
      "1061/1061 [==============================] - 1s 920us/step - loss: 3337.1839 - val_loss: 3320.8325\n",
      "Epoch 274/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3336.2735 - val_loss: 3322.2910\n",
      "Epoch 275/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3334.7523 - val_loss: 3319.4591\n",
      "Epoch 276/500\n",
      "1061/1061 [==============================] - 1s 885us/step - loss: 3336.3470 - val_loss: 3320.6459\n",
      "Epoch 277/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3335.2218 - val_loss: 3317.8402\n",
      "Epoch 278/500\n",
      "1061/1061 [==============================] - 1s 897us/step - loss: 3335.2858 - val_loss: 3320.4055\n",
      "Epoch 279/500\n",
      "1061/1061 [==============================] - 1s 894us/step - loss: 3334.9756 - val_loss: 3321.5647\n",
      "Epoch 280/500\n",
      "1061/1061 [==============================] - 1s 887us/step - loss: 3334.9312 - val_loss: 3319.7247\n",
      "Epoch 281/500\n",
      "1061/1061 [==============================] - 1s 871us/step - loss: 3333.6005 - val_loss: 3319.0553\n",
      "Epoch 282/500\n",
      "1061/1061 [==============================] - 1s 954us/step - loss: 3334.6741 - val_loss: 3316.5338\n",
      "Epoch 283/500\n",
      "1061/1061 [==============================] - 1s 863us/step - loss: 3334.3801 - val_loss: 3319.2089\n",
      "Epoch 284/500\n",
      "1061/1061 [==============================] - 1s 875us/step - loss: 3334.2180 - val_loss: 3318.7646\n",
      "Epoch 285/500\n",
      "1061/1061 [==============================] - 1s 901us/step - loss: 3333.7408 - val_loss: 3319.9607\n",
      "Epoch 286/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3332.3236 - val_loss: 3321.7497\n",
      "Epoch 287/500\n",
      "1061/1061 [==============================] - 1s 913us/step - loss: 3332.9575 - val_loss: 3317.9009\n",
      "Epoch 288/500\n",
      "1061/1061 [==============================] - 1s 842us/step - loss: 3333.8173 - val_loss: 3315.7022\n",
      "Epoch 289/500\n",
      "1061/1061 [==============================] - 1s 886us/step - loss: 3333.8713 - val_loss: 3318.1772\n",
      "Epoch 290/500\n",
      "1061/1061 [==============================] - 1s 865us/step - loss: 3334.3250 - val_loss: 3317.1060\n",
      "Epoch 291/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061/1061 [==============================] - 1s 919us/step - loss: 3333.3062 - val_loss: 3317.5975\n",
      "Epoch 292/500\n",
      "1061/1061 [==============================] - 1s 947us/step - loss: 3331.7955 - val_loss: 3318.4345\n",
      "Epoch 293/500\n",
      "1061/1061 [==============================] - 1s 897us/step - loss: 3333.4940 - val_loss: 3318.1449\n",
      "Epoch 294/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3332.4035 - val_loss: 3319.6803\n",
      "Epoch 295/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3333.9346 - val_loss: 3316.8204\n",
      "Epoch 296/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3331.7283 - val_loss: 3316.1605\n",
      "Epoch 297/500\n",
      "1061/1061 [==============================] - 1s 859us/step - loss: 3332.1473 - val_loss: 3317.8885\n",
      "Epoch 298/500\n",
      "1061/1061 [==============================] - 1s 884us/step - loss: 3332.7644 - val_loss: 3313.5045\n",
      "Epoch 299/500\n",
      "1061/1061 [==============================] - 1s 814us/step - loss: 3331.9197 - val_loss: 3317.6154\n",
      "Epoch 300/500\n",
      "1061/1061 [==============================] - 1s 904us/step - loss: 3331.8801 - val_loss: 3317.6623\n",
      "Epoch 301/500\n",
      "1061/1061 [==============================] - 1s 932us/step - loss: 3333.2269 - val_loss: 3316.2689\n",
      "Epoch 302/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3330.8374 - val_loss: 3316.9208\n",
      "Epoch 303/500\n",
      "1061/1061 [==============================] - 1s 930us/step - loss: 3332.9415 - val_loss: 3318.6703\n",
      "Epoch 304/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3331.7551 - val_loss: 3316.8043\n",
      "Epoch 305/500\n",
      "1061/1061 [==============================] - 1s 959us/step - loss: 3331.0845 - val_loss: 3316.3410\n",
      "Epoch 306/500\n",
      "1061/1061 [==============================] - 1s 933us/step - loss: 3330.8963 - val_loss: 3317.1711\n",
      "Epoch 307/500\n",
      "1061/1061 [==============================] - 1s 864us/step - loss: 3332.9149 - val_loss: 3314.8788\n",
      "Epoch 308/500\n",
      "1061/1061 [==============================] - 1s 858us/step - loss: 3330.8218 - val_loss: 3315.6306\n",
      "Epoch 309/500\n",
      "1061/1061 [==============================] - 1s 857us/step - loss: 3331.0259 - val_loss: 3314.1903\n",
      "Epoch 310/500\n",
      "1061/1061 [==============================] - 1s 864us/step - loss: 3331.7505 - val_loss: 3317.5053\n",
      "Epoch 311/500\n",
      "1061/1061 [==============================] - 1s 861us/step - loss: 3330.9681 - val_loss: 3317.0831\n",
      "Epoch 312/500\n",
      "1061/1061 [==============================] - 1s 869us/step - loss: 3330.8535 - val_loss: 3313.4895\n",
      "Epoch 313/500\n",
      "1061/1061 [==============================] - 1s 880us/step - loss: 3330.7925 - val_loss: 3315.8912\n",
      "Epoch 314/500\n",
      "1061/1061 [==============================] - 1s 867us/step - loss: 3330.2355 - val_loss: 3316.6104\n",
      "Epoch 315/500\n",
      "1061/1061 [==============================] - 1s 859us/step - loss: 3331.2788 - val_loss: 3313.9408\n",
      "Epoch 316/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3330.0987 - val_loss: 3313.3824\n",
      "Epoch 317/500\n",
      "1061/1061 [==============================] - 1s 870us/step - loss: 3330.0732 - val_loss: 3317.1758\n",
      "Epoch 318/500\n",
      "1061/1061 [==============================] - 1s 907us/step - loss: 3330.5587 - val_loss: 3313.8134\n",
      "Epoch 319/500\n",
      "1061/1061 [==============================] - 1s 869us/step - loss: 3329.6184 - val_loss: 3315.5629\n",
      "Epoch 320/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3330.0990 - val_loss: 3312.5328\n",
      "Epoch 321/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3328.9332 - val_loss: 3312.2661\n",
      "Epoch 322/500\n",
      "1061/1061 [==============================] - 1s 861us/step - loss: 3329.5706 - val_loss: 3315.6545\n",
      "Epoch 323/500\n",
      "1061/1061 [==============================] - 1s 887us/step - loss: 3330.0381 - val_loss: 3316.1977\n",
      "Epoch 324/500\n",
      "1061/1061 [==============================] - 1s 931us/step - loss: 3330.2583 - val_loss: 3314.1734\n",
      "Epoch 325/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3329.6025 - val_loss: 3315.5729\n",
      "Epoch 326/500\n",
      "1061/1061 [==============================] - 1s 886us/step - loss: 3328.6931 - val_loss: 3312.8681\n",
      "Epoch 327/500\n",
      "1061/1061 [==============================] - 1s 875us/step - loss: 3328.4752 - val_loss: 3313.2359\n",
      "Epoch 328/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3330.1468 - val_loss: 3314.1066\n",
      "Epoch 329/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3328.3888 - val_loss: 3315.7713\n",
      "Epoch 330/500\n",
      "1061/1061 [==============================] - 1s 879us/step - loss: 3327.8917 - val_loss: 3314.0897\n",
      "Epoch 331/500\n",
      "1061/1061 [==============================] - 1s 893us/step - loss: 3328.6382 - val_loss: 3311.0213\n",
      "Epoch 332/500\n",
      "1061/1061 [==============================] - 1s 891us/step - loss: 3328.9595 - val_loss: 3311.9011\n",
      "Epoch 333/500\n",
      "1061/1061 [==============================] - 1s 888us/step - loss: 3329.4304 - val_loss: 3314.6209\n",
      "Epoch 334/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3329.9030 - val_loss: 3315.4745\n",
      "Epoch 335/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3328.5224 - val_loss: 3312.4083\n",
      "Epoch 336/500\n",
      "1061/1061 [==============================] - 1s 935us/step - loss: 3328.7268 - val_loss: 3315.4291\n",
      "Epoch 337/500\n",
      "1061/1061 [==============================] - 1s 864us/step - loss: 3330.4351 - val_loss: 3311.3820\n",
      "Epoch 338/500\n",
      "1061/1061 [==============================] - 1s 928us/step - loss: 3328.6354 - val_loss: 3312.2413\n",
      "Epoch 339/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3328.6258 - val_loss: 3315.1416\n",
      "Epoch 340/500\n",
      "1061/1061 [==============================] - 1s 888us/step - loss: 3328.3626 - val_loss: 3312.0097\n",
      "Epoch 341/500\n",
      "1061/1061 [==============================] - 1s 884us/step - loss: 3327.7113 - val_loss: 3315.7975\n",
      "Epoch 342/500\n",
      "1061/1061 [==============================] - 1s 906us/step - loss: 3327.6059 - val_loss: 3315.1479\n",
      "Epoch 343/500\n",
      "1061/1061 [==============================] - 1s 980us/step - loss: 3328.3547 - val_loss: 3315.4799\n",
      "Epoch 344/500\n",
      "1061/1061 [==============================] - 1s 915us/step - loss: 3327.5006 - val_loss: 3312.9361\n",
      "Epoch 345/500\n",
      "1061/1061 [==============================] - 1s 988us/step - loss: 3328.1690 - val_loss: 3313.8913\n",
      "Epoch 346/500\n",
      "1061/1061 [==============================] - 1s 996us/step - loss: 3326.8957 - val_loss: 3313.5650\n",
      "Epoch 347/500\n",
      "1061/1061 [==============================] - 1s 907us/step - loss: 3327.8265 - val_loss: 3312.6135\n",
      "Epoch 348/500\n",
      "1061/1061 [==============================] - 1s 977us/step - loss: 3326.3449 - val_loss: 3312.6812\n",
      "Epoch 349/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3327.7349 - val_loss: 3312.8908\n",
      "Epoch 350/500\n",
      "1061/1061 [==============================] - 1s 913us/step - loss: 3326.5396 - val_loss: 3312.6438\n",
      "Epoch 351/500\n",
      "1061/1061 [==============================] - 1s 963us/step - loss: 3327.9510 - val_loss: 3313.3415\n",
      "Epoch 352/500\n",
      "1061/1061 [==============================] - 1s 895us/step - loss: 3326.6783 - val_loss: 3311.2848\n",
      "Epoch 353/500\n",
      "1061/1061 [==============================] - 1s 894us/step - loss: 3327.9006 - val_loss: 3309.9264\n",
      "Epoch 354/500\n",
      "1061/1061 [==============================] - 1s 870us/step - loss: 3326.6309 - val_loss: 3312.6364\n",
      "Epoch 355/500\n",
      "1061/1061 [==============================] - 1s 896us/step - loss: 3327.5471 - val_loss: 3312.2508\n",
      "Epoch 356/500\n",
      "1061/1061 [==============================] - 1s 887us/step - loss: 3326.3649 - val_loss: 3313.0505\n",
      "Epoch 357/500\n",
      "1061/1061 [==============================] - 1s 871us/step - loss: 3326.1955 - val_loss: 3312.0497\n",
      "Epoch 358/500\n",
      "1061/1061 [==============================] - 1s 885us/step - loss: 3325.6134 - val_loss: 3312.2177\n",
      "Epoch 359/500\n",
      "1061/1061 [==============================] - 1s 908us/step - loss: 3325.8529 - val_loss: 3311.9640\n",
      "Epoch 360/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3326.9803 - val_loss: 3312.6041\n",
      "Epoch 361/500\n",
      "1061/1061 [==============================] - 1s 1ms/step - loss: 3326.8922 - val_loss: 3312.6067\n",
      "Epoch 362/500\n",
      "1061/1061 [==============================] - 1s 918us/step - loss: 3326.2524 - val_loss: 3312.5060\n",
      "Epoch 363/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061/1061 [==============================] - 1s 949us/step - loss: 3326.1248 - val_loss: 3311.9689\n",
      "Epoch 364/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3325.3518 - val_loss: 3314.1212\n",
      "Epoch 365/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3325.5406 - val_loss: 3312.2366\n",
      "Epoch 366/500\n",
      "1061/1061 [==============================] - 1s 896us/step - loss: 3325.2545 - val_loss: 3313.1704\n",
      "Epoch 367/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3324.7645 - val_loss: 3309.9215\n",
      "Epoch 368/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3325.0292 - val_loss: 3310.0257\n",
      "Epoch 369/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3326.4422 - val_loss: 3314.5290\n",
      "Epoch 370/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3325.8013 - val_loss: 3311.5540\n",
      "Epoch 371/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3325.0688 - val_loss: 3309.4532\n",
      "Epoch 372/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3324.8030 - val_loss: 3310.9312\n",
      "Epoch 373/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3325.1449 - val_loss: 3310.1601\n",
      "Epoch 374/500\n",
      "1061/1061 [==============================] - 1s 894us/step - loss: 3325.4882 - val_loss: 3311.5773\n",
      "Epoch 375/500\n",
      "1061/1061 [==============================] - 1s 896us/step - loss: 3325.7461 - val_loss: 3312.8703\n",
      "Epoch 376/500\n",
      "1061/1061 [==============================] - 1s 872us/step - loss: 3324.8817 - val_loss: 3312.5756\n",
      "Epoch 377/500\n",
      "1061/1061 [==============================] - 1s 870us/step - loss: 3325.6393 - val_loss: 3309.9191\n",
      "Epoch 378/500\n",
      "1061/1061 [==============================] - 1s 885us/step - loss: 3325.7962 - val_loss: 3310.4842\n",
      "Epoch 379/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3324.3246 - val_loss: 3311.1029\n",
      "Epoch 380/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3323.8804 - val_loss: 3310.0601\n",
      "Epoch 381/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3324.1650 - val_loss: 3310.6206\n",
      "Epoch 382/500\n",
      "1061/1061 [==============================] - 1s 901us/step - loss: 3324.2303 - val_loss: 3309.6792\n",
      "Epoch 383/500\n",
      "1061/1061 [==============================] - 1s 914us/step - loss: 3323.7098 - val_loss: 3312.0597\n",
      "Epoch 384/500\n",
      "1061/1061 [==============================] - 1s 898us/step - loss: 3323.5600 - val_loss: 3308.4798\n",
      "Epoch 385/500\n",
      "1061/1061 [==============================] - 1s 895us/step - loss: 3324.9495 - val_loss: 3310.5564\n",
      "Epoch 386/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3325.6227 - val_loss: 3314.1577\n",
      "Epoch 387/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3324.5950 - val_loss: 3310.5376\n",
      "Epoch 388/500\n",
      "1061/1061 [==============================] - 1s 886us/step - loss: 3324.5901 - val_loss: 3310.5250\n",
      "Epoch 389/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3324.5424 - val_loss: 3308.3733\n",
      "Epoch 390/500\n",
      "1061/1061 [==============================] - 1s 901us/step - loss: 3323.8726 - val_loss: 3311.8271\n",
      "Epoch 391/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3324.6946 - val_loss: 3309.7603\n",
      "Epoch 392/500\n",
      "1061/1061 [==============================] - 1s 901us/step - loss: 3324.0240 - val_loss: 3307.5392\n",
      "Epoch 393/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3323.2778 - val_loss: 3308.5589\n",
      "Epoch 394/500\n",
      "1061/1061 [==============================] - 1s 873us/step - loss: 3323.4393 - val_loss: 3309.2107\n",
      "Epoch 395/500\n",
      "1061/1061 [==============================] - 1s 877us/step - loss: 3323.4374 - val_loss: 3310.9420\n",
      "Epoch 396/500\n",
      "1061/1061 [==============================] - 1s 913us/step - loss: 3322.1977 - val_loss: 3309.1729\n",
      "Epoch 397/500\n",
      "1061/1061 [==============================] - 1s 817us/step - loss: 3322.8267 - val_loss: 3310.0556\n",
      "Epoch 398/500\n",
      "1061/1061 [==============================] - 1s 898us/step - loss: 3323.2831 - val_loss: 3308.7852\n",
      "Epoch 399/500\n",
      "1061/1061 [==============================] - 1s 880us/step - loss: 3322.2038 - val_loss: 3310.8209\n",
      "Epoch 400/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3322.4123 - val_loss: 3309.5015\n",
      "Epoch 401/500\n",
      "1061/1061 [==============================] - 1s 931us/step - loss: 3321.7916 - val_loss: 3309.3421\n",
      "Epoch 402/500\n",
      "1061/1061 [==============================] - 1s 880us/step - loss: 3321.3521 - val_loss: 3305.7305\n",
      "Epoch 403/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3323.4910 - val_loss: 3308.9598\n",
      "Epoch 404/500\n",
      "1061/1061 [==============================] - 1s 887us/step - loss: 3322.5877 - val_loss: 3308.7540\n",
      "Epoch 405/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3321.7082 - val_loss: 3308.3122\n",
      "Epoch 406/500\n",
      "1061/1061 [==============================] - 1s 889us/step - loss: 3321.3198 - val_loss: 3306.6746\n",
      "Epoch 407/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3324.2341 - val_loss: 3312.2732\n",
      "Epoch 408/500\n",
      "1061/1061 [==============================] - 1s 886us/step - loss: 3323.1570 - val_loss: 3307.9112\n",
      "Epoch 409/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3321.2515 - val_loss: 3308.7175\n",
      "Epoch 410/500\n",
      "1061/1061 [==============================] - 1s 879us/step - loss: 3322.4022 - val_loss: 3307.3432\n",
      "Epoch 411/500\n",
      "1061/1061 [==============================] - 1s 896us/step - loss: 3320.5049 - val_loss: 3309.8990\n",
      "Epoch 412/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3322.3481 - val_loss: 3307.8062\n",
      "Epoch 413/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3321.8557 - val_loss: 3308.3995\n",
      "Epoch 414/500\n",
      "1061/1061 [==============================] - 1s 885us/step - loss: 3321.2902 - val_loss: 3306.3933\n",
      "Epoch 415/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3320.9825 - val_loss: 3306.6035\n",
      "Epoch 416/500\n",
      "1061/1061 [==============================] - 1s 887us/step - loss: 3322.9735 - val_loss: 3309.2405\n",
      "Epoch 417/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3322.3274 - val_loss: 3311.4170\n",
      "Epoch 418/500\n",
      "1061/1061 [==============================] - 1s 910us/step - loss: 3322.9494 - val_loss: 3305.8108\n",
      "Epoch 419/500\n",
      "1061/1061 [==============================] - 1s 901us/step - loss: 3322.1426 - val_loss: 3308.6587\n",
      "Epoch 420/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3322.6675 - val_loss: 3311.3544\n",
      "Epoch 421/500\n",
      "1061/1061 [==============================] - 1s 912us/step - loss: 3320.5106 - val_loss: 3308.8090\n",
      "Epoch 422/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3321.5621 - val_loss: 3307.4253\n",
      "Epoch 423/500\n",
      "1061/1061 [==============================] - 1s 886us/step - loss: 3321.8333 - val_loss: 3306.2984\n",
      "Epoch 424/500\n",
      "1061/1061 [==============================] - 1s 898us/step - loss: 3320.7365 - val_loss: 3308.9925\n",
      "Epoch 425/500\n",
      "1061/1061 [==============================] - 1s 882us/step - loss: 3322.1382 - val_loss: 3308.9102\n",
      "Epoch 426/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3320.7469 - val_loss: 3309.2873\n",
      "Epoch 427/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3321.5159 - val_loss: 3305.8550\n",
      "Epoch 428/500\n",
      "1061/1061 [==============================] - 1s 898us/step - loss: 3320.6856 - val_loss: 3308.7254\n",
      "Epoch 429/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3320.9024 - val_loss: 3309.4057\n",
      "Epoch 430/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3321.7031 - val_loss: 3309.2428\n",
      "Epoch 431/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3322.3715 - val_loss: 3308.0483\n",
      "Epoch 432/500\n",
      "1061/1061 [==============================] - 1s 866us/step - loss: 3320.5982 - val_loss: 3306.9734\n",
      "Epoch 433/500\n",
      "1061/1061 [==============================] - 1s 898us/step - loss: 3319.4125 - val_loss: 3305.9629\n",
      "Epoch 434/500\n",
      "1061/1061 [==============================] - 1s 893us/step - loss: 3319.2785 - val_loss: 3306.6917\n",
      "Epoch 435/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061/1061 [==============================] - 1s 880us/step - loss: 3319.8198 - val_loss: 3307.1973\n",
      "Epoch 436/500\n",
      "1061/1061 [==============================] - 1s 884us/step - loss: 3319.7819 - val_loss: 3307.0967\n",
      "Epoch 437/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3320.2328 - val_loss: 3309.4351\n",
      "Epoch 438/500\n",
      "1061/1061 [==============================] - 1s 913us/step - loss: 3320.3531 - val_loss: 3307.1361\n",
      "Epoch 439/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3320.3333 - val_loss: 3307.9234\n",
      "Epoch 440/500\n",
      "1061/1061 [==============================] - 1s 893us/step - loss: 3319.6201 - val_loss: 3307.0996\n",
      "Epoch 441/500\n",
      "1061/1061 [==============================] - 1s 891us/step - loss: 3320.2125 - val_loss: 3306.1893\n",
      "Epoch 442/500\n",
      "1061/1061 [==============================] - 1s 829us/step - loss: 3320.7294 - val_loss: 3306.8900\n",
      "Epoch 443/500\n",
      "1061/1061 [==============================] - 1s 911us/step - loss: 3319.8058 - val_loss: 3306.1545\n",
      "Epoch 444/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3320.8265 - val_loss: 3304.4791\n",
      "Epoch 445/500\n",
      "1061/1061 [==============================] - 1s 865us/step - loss: 3318.9025 - val_loss: 3309.6448\n",
      "Epoch 446/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3320.0450 - val_loss: 3305.1543\n",
      "Epoch 447/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3319.8107 - val_loss: 3308.9322\n",
      "Epoch 448/500\n",
      "1061/1061 [==============================] - 1s 833us/step - loss: 3318.8305 - val_loss: 3304.5099\n",
      "Epoch 449/500\n",
      "1061/1061 [==============================] - 1s 916us/step - loss: 3319.5044 - val_loss: 3306.7588\n",
      "Epoch 450/500\n",
      "1061/1061 [==============================] - 1s 904us/step - loss: 3319.7237 - val_loss: 3308.3847\n",
      "Epoch 451/500\n",
      "1061/1061 [==============================] - 1s 908us/step - loss: 3319.1815 - val_loss: 3305.4865\n",
      "Epoch 452/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3317.9398 - val_loss: 3305.4676\n",
      "Epoch 453/500\n",
      "1061/1061 [==============================] - 1s 882us/step - loss: 3318.8205 - val_loss: 3307.2907\n",
      "Epoch 454/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3319.7230 - val_loss: 3306.2582\n",
      "Epoch 455/500\n",
      "1061/1061 [==============================] - 1s 872us/step - loss: 3319.7516 - val_loss: 3305.6215\n",
      "Epoch 456/500\n",
      "1061/1061 [==============================] - 1s 873us/step - loss: 3317.6639 - val_loss: 3307.0561\n",
      "Epoch 457/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3318.9413 - val_loss: 3305.2679\n",
      "Epoch 458/500\n",
      "1061/1061 [==============================] - 1s 876us/step - loss: 3318.5789 - val_loss: 3303.7774\n",
      "Epoch 459/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3318.9420 - val_loss: 3307.4269\n",
      "Epoch 460/500\n",
      "1061/1061 [==============================] - 1s 883us/step - loss: 3318.3845 - val_loss: 3306.9937\n",
      "Epoch 461/500\n",
      "1061/1061 [==============================] - 1s 904us/step - loss: 3319.3429 - val_loss: 3304.5383\n",
      "Epoch 462/500\n",
      "1061/1061 [==============================] - 1s 902us/step - loss: 3319.5445 - val_loss: 3308.5450\n",
      "Epoch 463/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3318.5728 - val_loss: 3307.2041\n",
      "Epoch 464/500\n",
      "1061/1061 [==============================] - 1s 912us/step - loss: 3318.7598 - val_loss: 3304.8489\n",
      "Epoch 465/500\n",
      "1061/1061 [==============================] - 1s 897us/step - loss: 3317.6830 - val_loss: 3306.9109\n",
      "Epoch 466/500\n",
      "1061/1061 [==============================] - 1s 887us/step - loss: 3317.9351 - val_loss: 3306.0531\n",
      "Epoch 467/500\n",
      "1061/1061 [==============================] - 1s 909us/step - loss: 3318.0759 - val_loss: 3305.5404\n",
      "Epoch 468/500\n",
      "1061/1061 [==============================] - 1s 878us/step - loss: 3318.1190 - val_loss: 3306.5139\n",
      "Epoch 469/500\n",
      "1061/1061 [==============================] - 1s 904us/step - loss: 3318.6101 - val_loss: 3304.7649\n",
      "Epoch 470/500\n",
      "1061/1061 [==============================] - 1s 940us/step - loss: 3318.4621 - val_loss: 3305.5519\n",
      "Epoch 471/500\n",
      "1061/1061 [==============================] - 1s 901us/step - loss: 3318.5376 - val_loss: 3305.1514\n",
      "Epoch 472/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3318.8498 - val_loss: 3304.5930\n",
      "Epoch 473/500\n",
      "1061/1061 [==============================] - 1s 903us/step - loss: 3317.9672 - val_loss: 3305.1983\n",
      "Epoch 474/500\n",
      "1061/1061 [==============================] - 1s 923us/step - loss: 3318.6985 - val_loss: 3305.3286\n",
      "Epoch 475/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3318.2918 - val_loss: 3306.2201\n",
      "Epoch 476/500\n",
      "1061/1061 [==============================] - 1s 877us/step - loss: 3316.7543 - val_loss: 3305.6086\n",
      "Epoch 477/500\n",
      "1061/1061 [==============================] - 1s 892us/step - loss: 3317.0863 - val_loss: 3303.2568\n",
      "Epoch 478/500\n",
      "1061/1061 [==============================] - 1s 898us/step - loss: 3317.5473 - val_loss: 3305.3163\n",
      "Epoch 479/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3316.9131 - val_loss: 3305.0911\n",
      "Epoch 480/500\n",
      "1061/1061 [==============================] - 1s 891us/step - loss: 3316.4885 - val_loss: 3304.5237\n",
      "Epoch 481/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3317.7327 - val_loss: 3305.7097\n",
      "Epoch 482/500\n",
      "1061/1061 [==============================] - 1s 882us/step - loss: 3317.3921 - val_loss: 3302.8807\n",
      "Epoch 483/500\n",
      "1061/1061 [==============================] - 1s 899us/step - loss: 3318.5681 - val_loss: 3307.2481\n",
      "Epoch 484/500\n",
      "1061/1061 [==============================] - 1s 925us/step - loss: 3317.8939 - val_loss: 3304.0170\n",
      "Epoch 485/500\n",
      "1061/1061 [==============================] - 1s 944us/step - loss: 3316.7801 - val_loss: 3304.3381\n",
      "Epoch 486/500\n",
      "1061/1061 [==============================] - 1s 936us/step - loss: 3318.0121 - val_loss: 3304.5374\n",
      "Epoch 487/500\n",
      "1061/1061 [==============================] - 1s 914us/step - loss: 3317.9266 - val_loss: 3303.9705\n",
      "Epoch 488/500\n",
      "1061/1061 [==============================] - 1s 843us/step - loss: 3317.6949 - val_loss: 3306.5069\n",
      "Epoch 489/500\n",
      "1061/1061 [==============================] - 1s 885us/step - loss: 3318.0132 - val_loss: 3305.8726\n",
      "Epoch 490/500\n",
      "1061/1061 [==============================] - 1s 900us/step - loss: 3317.2298 - val_loss: 3306.2170\n",
      "Epoch 491/500\n",
      "1061/1061 [==============================] - 1s 925us/step - loss: 3317.4589 - val_loss: 3303.7758\n",
      "Epoch 492/500\n",
      "1061/1061 [==============================] - 1s 930us/step - loss: 3316.7206 - val_loss: 3304.4827\n",
      "Epoch 493/500\n",
      "1061/1061 [==============================] - 1s 913us/step - loss: 3316.4820 - val_loss: 3303.7012\n",
      "Epoch 494/500\n",
      "1061/1061 [==============================] - 1s 907us/step - loss: 3318.0680 - val_loss: 3305.9763\n",
      "Epoch 495/500\n",
      "1061/1061 [==============================] - 1s 915us/step - loss: 3318.7138 - val_loss: 3303.6185\n",
      "Epoch 496/500\n",
      "1061/1061 [==============================] - 1s 917us/step - loss: 3316.8920 - val_loss: 3303.3227\n",
      "Epoch 497/500\n",
      "1061/1061 [==============================] - 1s 890us/step - loss: 3317.1048 - val_loss: 3304.6286\n",
      "Epoch 498/500\n",
      "1061/1061 [==============================] - 1s 872us/step - loss: 3316.4787 - val_loss: 3307.9337\n",
      "Epoch 499/500\n",
      "1061/1061 [==============================] - 1s 915us/step - loss: 3316.4147 - val_loss: 3306.0888\n",
      "Epoch 500/500\n",
      "1061/1061 [==============================] - 1s 905us/step - loss: 3317.2243 - val_loss: 3304.3186\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3975a9da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_model(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "decoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
