{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with 2-hidden layers\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# To ensure reproducibility using Keras during development\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "randomState = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"PA1673_full_old\", \"train_model_input.txt.xz\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0, header=0, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "epochs = 200\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "intermediate_dim = 100\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"PA1673_full_old\", \"tybalt_2layer_{}latent_stats.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"PA1673_full_old\", \"tybalt_2layer_{}latent_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"PA1673_full_old\", \"train_input_2layer_{}latent_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"PA1673_full_old\", \"tybalt_2layer_{}latent_encoder_model.h5\".format(latent_dim))\n",
    "weights_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"PA1673_full_old\", \"tybalt_2layer_{}latent_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"PA1673_full_old\", \"tybalt_2layer_{}latent_decoder_model.h5\".format(latent_dim))\n",
    "weights_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"PA1673_full_old\", \"tybalt_2layer_{}latent_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent, random_state = randomState)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:74: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "hidden_dense_linear = Dense(intermediate_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "hidden_dense_batchnorm = BatchNormalization()(hidden_dense_linear)\n",
    "hidden_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "# Note:\n",
    "# Normalize and relu filter at each layer adds non-linear component (relu is non-linear function)\n",
    "# If architecture is layer-layer-normalization-relu then the computation is still linear\n",
    "# Add additional layers in triplicate\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_model = Sequential()\n",
    "decoder_model.add(Dense(intermediate_dim, activation='relu', input_dim=latent_dim))\n",
    "decoder_model.add(Dense(original_dim, activation='sigmoid'))\n",
    "rnaseq_reconstruct = decoder_model(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 804 samples, validate on 89 samples\n",
      "Epoch 1/200\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 3780.2000 - val_loss: 3607.4841\n",
      "Epoch 2/200\n",
      "804/804 [==============================] - 1s 945us/step - loss: 3630.1086 - val_loss: 3536.0649\n",
      "Epoch 3/200\n",
      "804/804 [==============================] - 1s 883us/step - loss: 3555.2498 - val_loss: 3616.7388\n",
      "Epoch 4/200\n",
      "804/804 [==============================] - 1s 976us/step - loss: 3522.6267 - val_loss: 3560.8015\n",
      "Epoch 5/200\n",
      "804/804 [==============================] - 1s 907us/step - loss: 3506.6472 - val_loss: 3517.0291\n",
      "Epoch 6/200\n",
      "804/804 [==============================] - 1s 900us/step - loss: 3496.5498 - val_loss: 3595.1636\n",
      "Epoch 7/200\n",
      "804/804 [==============================] - 1s 882us/step - loss: 3490.8094 - val_loss: 3546.0073\n",
      "Epoch 8/200\n",
      "804/804 [==============================] - 1s 865us/step - loss: 3487.0279 - val_loss: 3513.5359\n",
      "Epoch 9/200\n",
      "804/804 [==============================] - 1s 873us/step - loss: 3483.8404 - val_loss: 3538.0271\n",
      "Epoch 10/200\n",
      "804/804 [==============================] - 1s 879us/step - loss: 3477.9856 - val_loss: 3506.9592\n",
      "Epoch 11/200\n",
      "804/804 [==============================] - 1s 876us/step - loss: 3481.2363 - val_loss: 3476.8508\n",
      "Epoch 12/200\n",
      "804/804 [==============================] - 1s 910us/step - loss: 3479.5903 - val_loss: 3459.5879\n",
      "Epoch 13/200\n",
      "804/804 [==============================] - 1s 865us/step - loss: 3476.3977 - val_loss: 3456.7380\n",
      "Epoch 14/200\n",
      "804/804 [==============================] - 1s 889us/step - loss: 3476.6147 - val_loss: 3441.0085\n",
      "Epoch 15/200\n",
      "804/804 [==============================] - 1s 881us/step - loss: 3471.6968 - val_loss: 3464.0530\n",
      "Epoch 16/200\n",
      "804/804 [==============================] - 1s 881us/step - loss: 3470.9522 - val_loss: 3462.6797\n",
      "Epoch 17/200\n",
      "804/804 [==============================] - 1s 884us/step - loss: 3471.4980 - val_loss: 3452.3040\n",
      "Epoch 18/200\n",
      "804/804 [==============================] - 1s 899us/step - loss: 3467.7454 - val_loss: 3438.9536\n",
      "Epoch 19/200\n",
      "804/804 [==============================] - 1s 923us/step - loss: 3467.9307 - val_loss: 3435.8582\n",
      "Epoch 20/200\n",
      "804/804 [==============================] - 1s 924us/step - loss: 3466.5807 - val_loss: 3445.6067\n",
      "Epoch 21/200\n",
      "804/804 [==============================] - 1s 873us/step - loss: 3468.3970 - val_loss: 3452.4038\n",
      "Epoch 22/200\n",
      "804/804 [==============================] - 1s 881us/step - loss: 3465.7438 - val_loss: 3450.3269\n",
      "Epoch 23/200\n",
      "804/804 [==============================] - 1s 882us/step - loss: 3463.3648 - val_loss: 3459.7212\n",
      "Epoch 24/200\n",
      "804/804 [==============================] - 1s 877us/step - loss: 3462.9366 - val_loss: 3452.9727\n",
      "Epoch 25/200\n",
      "804/804 [==============================] - 1s 879us/step - loss: 3461.3151 - val_loss: 3470.4722\n",
      "Epoch 26/200\n",
      "804/804 [==============================] - 1s 866us/step - loss: 3461.9655 - val_loss: 3445.7927\n",
      "Epoch 27/200\n",
      "804/804 [==============================] - 1s 906us/step - loss: 3460.8550 - val_loss: 3437.6716\n",
      "Epoch 28/200\n",
      "804/804 [==============================] - 1s 897us/step - loss: 3459.3703 - val_loss: 3436.8267\n",
      "Epoch 29/200\n",
      "804/804 [==============================] - 1s 877us/step - loss: 3456.3042 - val_loss: 3438.3494\n",
      "Epoch 30/200\n",
      "804/804 [==============================] - 1s 881us/step - loss: 3457.4783 - val_loss: 3430.2109\n",
      "Epoch 31/200\n",
      "804/804 [==============================] - 1s 865us/step - loss: 3457.5291 - val_loss: 3422.3853\n",
      "Epoch 32/200\n",
      "804/804 [==============================] - 1s 871us/step - loss: 3457.0487 - val_loss: 3418.0017\n",
      "Epoch 33/200\n",
      "804/804 [==============================] - 1s 880us/step - loss: 3456.1817 - val_loss: 3419.3037\n",
      "Epoch 34/200\n",
      "804/804 [==============================] - 1s 898us/step - loss: 3453.9859 - val_loss: 3409.7068\n",
      "Epoch 35/200\n",
      "804/804 [==============================] - 1s 907us/step - loss: 3455.1128 - val_loss: 3415.3252\n",
      "Epoch 36/200\n",
      "804/804 [==============================] - 1s 880us/step - loss: 3454.4167 - val_loss: 3417.3918\n",
      "Epoch 37/200\n",
      "804/804 [==============================] - 1s 875us/step - loss: 3452.3459 - val_loss: 3438.2522\n",
      "Epoch 38/200\n",
      "804/804 [==============================] - 1s 873us/step - loss: 3452.6021 - val_loss: 3458.6194\n",
      "Epoch 39/200\n",
      "804/804 [==============================] - 1s 873us/step - loss: 3451.2048 - val_loss: 3451.5078\n",
      "Epoch 40/200\n",
      "804/804 [==============================] - 1s 867us/step - loss: 3447.8700 - val_loss: 3439.7615\n",
      "Epoch 41/200\n",
      "804/804 [==============================] - 1s 870us/step - loss: 3447.1576 - val_loss: 3428.3767\n",
      "Epoch 42/200\n",
      "804/804 [==============================] - 1s 869us/step - loss: 3447.1230 - val_loss: 3423.1685\n",
      "Epoch 43/200\n",
      "804/804 [==============================] - 1s 910us/step - loss: 3446.5485 - val_loss: 3410.7188\n",
      "Epoch 44/200\n",
      "804/804 [==============================] - 1s 885us/step - loss: 3448.2061 - val_loss: 3420.0200\n",
      "Epoch 45/200\n",
      "804/804 [==============================] - 1s 940us/step - loss: 3445.0315 - val_loss: 3419.0664\n",
      "Epoch 46/200\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 3445.2587 - val_loss: 3407.6228\n",
      "Epoch 47/200\n",
      "804/804 [==============================] - 1s 909us/step - loss: 3444.2459 - val_loss: 3412.3635\n",
      "Epoch 48/200\n",
      "804/804 [==============================] - 1s 840us/step - loss: 3443.3153 - val_loss: 3404.5852\n",
      "Epoch 49/200\n",
      "804/804 [==============================] - 1s 884us/step - loss: 3440.3960 - val_loss: 3421.8379\n",
      "Epoch 50/200\n",
      "804/804 [==============================] - 1s 944us/step - loss: 3442.6957 - val_loss: 3422.7427\n",
      "Epoch 51/200\n",
      "804/804 [==============================] - 1s 898us/step - loss: 3439.4742 - val_loss: 3421.5515\n",
      "Epoch 52/200\n",
      "804/804 [==============================] - 1s 856us/step - loss: 3439.2934 - val_loss: 3420.5154\n",
      "Epoch 53/200\n",
      "804/804 [==============================] - 1s 971us/step - loss: 3441.4406 - val_loss: 3414.2458\n",
      "Epoch 54/200\n",
      "804/804 [==============================] - 1s 953us/step - loss: 3439.2136 - val_loss: 3405.8108\n",
      "Epoch 55/200\n",
      "804/804 [==============================] - 1s 992us/step - loss: 3436.6669 - val_loss: 3399.2043\n",
      "Epoch 56/200\n",
      "804/804 [==============================] - 1s 887us/step - loss: 3437.4346 - val_loss: 3406.2886\n",
      "Epoch 57/200\n",
      "804/804 [==============================] - 1s 868us/step - loss: 3436.0119 - val_loss: 3414.5039\n",
      "Epoch 58/200\n",
      "804/804 [==============================] - 1s 957us/step - loss: 3436.6695 - val_loss: 3420.1328\n",
      "Epoch 59/200\n",
      "804/804 [==============================] - 1s 936us/step - loss: 3434.4268 - val_loss: 3420.7581\n",
      "Epoch 60/200\n",
      "804/804 [==============================] - 1s 892us/step - loss: 3434.6065 - val_loss: 3422.7190\n",
      "Epoch 61/200\n",
      "804/804 [==============================] - 1s 986us/step - loss: 3435.9998 - val_loss: 3425.6248\n",
      "Epoch 62/200\n",
      "804/804 [==============================] - 1s 997us/step - loss: 3435.4468 - val_loss: 3412.7148\n",
      "Epoch 63/200\n",
      "804/804 [==============================] - 1s 881us/step - loss: 3430.1133 - val_loss: 3400.8960\n",
      "Epoch 64/200\n",
      "804/804 [==============================] - 1s 859us/step - loss: 3432.7868 - val_loss: 3400.1221\n",
      "Epoch 65/200\n",
      "804/804 [==============================] - 1s 826us/step - loss: 3428.5138 - val_loss: 3396.9526\n",
      "Epoch 66/200\n",
      "804/804 [==============================] - 1s 849us/step - loss: 3431.1780 - val_loss: 3400.1616\n",
      "Epoch 67/200\n",
      "804/804 [==============================] - 1s 851us/step - loss: 3429.2685 - val_loss: 3395.8735\n",
      "Epoch 68/200\n",
      "804/804 [==============================] - 1s 821us/step - loss: 3426.9766 - val_loss: 3394.7402\n",
      "Epoch 69/200\n",
      "804/804 [==============================] - 1s 831us/step - loss: 3425.3895 - val_loss: 3396.8933\n",
      "Epoch 70/200\n",
      "804/804 [==============================] - 1s 862us/step - loss: 3425.5758 - val_loss: 3393.9814\n",
      "Epoch 71/200\n",
      "804/804 [==============================] - 1s 827us/step - loss: 3424.3974 - val_loss: 3399.6506\n",
      "Epoch 72/200\n",
      "804/804 [==============================] - 1s 849us/step - loss: 3425.0217 - val_loss: 3394.5474\n",
      "Epoch 73/200\n",
      "804/804 [==============================] - 1s 845us/step - loss: 3425.9710 - val_loss: 3392.3501\n",
      "Epoch 74/200\n",
      "804/804 [==============================] - 1s 826us/step - loss: 3424.2936 - val_loss: 3391.5552\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804/804 [==============================] - 1s 844us/step - loss: 3423.2603 - val_loss: 3398.2134\n",
      "Epoch 76/200\n",
      "804/804 [==============================] - 1s 831us/step - loss: 3423.4240 - val_loss: 3395.9744\n",
      "Epoch 77/200\n",
      "804/804 [==============================] - 1s 880us/step - loss: 3424.8064 - val_loss: 3391.1250\n",
      "Epoch 78/200\n",
      "804/804 [==============================] - 1s 860us/step - loss: 3425.3696 - val_loss: 3390.9885\n",
      "Epoch 79/200\n",
      "804/804 [==============================] - 1s 839us/step - loss: 3423.7247 - val_loss: 3398.6260\n",
      "Epoch 80/200\n",
      "804/804 [==============================] - 1s 837us/step - loss: 3421.4068 - val_loss: 3397.8757\n",
      "Epoch 81/200\n",
      "804/804 [==============================] - 1s 847us/step - loss: 3419.7284 - val_loss: 3394.4104\n",
      "Epoch 82/200\n",
      "804/804 [==============================] - 1s 846us/step - loss: 3421.6990 - val_loss: 3402.5491\n",
      "Epoch 83/200\n",
      "804/804 [==============================] - 1s 839us/step - loss: 3418.9391 - val_loss: 3391.1050\n",
      "Epoch 84/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3417.6984 - val_loss: 3393.4902\n",
      "Epoch 85/200\n",
      "804/804 [==============================] - 1s 865us/step - loss: 3418.4212 - val_loss: 3386.4390\n",
      "Epoch 86/200\n",
      "804/804 [==============================] - 1s 835us/step - loss: 3418.8181 - val_loss: 3393.4863\n",
      "Epoch 87/200\n",
      "804/804 [==============================] - 1s 837us/step - loss: 3421.8643 - val_loss: 3387.7566\n",
      "Epoch 88/200\n",
      "804/804 [==============================] - 1s 837us/step - loss: 3415.9078 - val_loss: 3385.3813\n",
      "Epoch 89/200\n",
      "804/804 [==============================] - 1s 841us/step - loss: 3416.0531 - val_loss: 3384.6897\n",
      "Epoch 90/200\n",
      "804/804 [==============================] - 1s 841us/step - loss: 3413.7072 - val_loss: 3386.1499\n",
      "Epoch 91/200\n",
      "804/804 [==============================] - 1s 854us/step - loss: 3415.6915 - val_loss: 3383.3655\n",
      "Epoch 92/200\n",
      "804/804 [==============================] - 1s 885us/step - loss: 3414.0213 - val_loss: 3384.7805\n",
      "Epoch 93/200\n",
      "804/804 [==============================] - 1s 855us/step - loss: 3415.0784 - val_loss: 3387.8977\n",
      "Epoch 94/200\n",
      "804/804 [==============================] - 1s 853us/step - loss: 3411.5315 - val_loss: 3380.5547\n",
      "Epoch 95/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3415.3257 - val_loss: 3384.9360\n",
      "Epoch 96/200\n",
      "804/804 [==============================] - 1s 830us/step - loss: 3412.8730 - val_loss: 3389.0940\n",
      "Epoch 97/200\n",
      "804/804 [==============================] - 1s 829us/step - loss: 3412.1489 - val_loss: 3391.9121\n",
      "Epoch 98/200\n",
      "804/804 [==============================] - 1s 836us/step - loss: 3411.5826 - val_loss: 3393.4519\n",
      "Epoch 99/200\n",
      "804/804 [==============================] - 1s 837us/step - loss: 3410.6523 - val_loss: 3382.3892\n",
      "Epoch 100/200\n",
      "804/804 [==============================] - 1s 855us/step - loss: 3409.1560 - val_loss: 3388.4177\n",
      "Epoch 101/200\n",
      "804/804 [==============================] - 1s 873us/step - loss: 3408.4143 - val_loss: 3381.6450\n",
      "Epoch 102/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3407.4570 - val_loss: 3382.6714\n",
      "Epoch 103/200\n",
      "804/804 [==============================] - 1s 828us/step - loss: 3406.9599 - val_loss: 3386.7854\n",
      "Epoch 104/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3406.2991 - val_loss: 3383.1907\n",
      "Epoch 105/200\n",
      "804/804 [==============================] - 1s 837us/step - loss: 3406.6653 - val_loss: 3373.5161\n",
      "Epoch 106/200\n",
      "804/804 [==============================] - 1s 827us/step - loss: 3405.8572 - val_loss: 3379.1843\n",
      "Epoch 107/200\n",
      "804/804 [==============================] - 1s 844us/step - loss: 3407.5250 - val_loss: 3375.5391\n",
      "Epoch 108/200\n",
      "804/804 [==============================] - 1s 865us/step - loss: 3406.0647 - val_loss: 3378.9976\n",
      "Epoch 109/200\n",
      "804/804 [==============================] - 1s 877us/step - loss: 3406.7716 - val_loss: 3379.6260\n",
      "Epoch 110/200\n",
      "804/804 [==============================] - 1s 868us/step - loss: 3405.6379 - val_loss: 3376.4309\n",
      "Epoch 111/200\n",
      "804/804 [==============================] - 1s 833us/step - loss: 3404.7164 - val_loss: 3376.2437\n",
      "Epoch 112/200\n",
      "804/804 [==============================] - 1s 850us/step - loss: 3405.5364 - val_loss: 3386.9810\n",
      "Epoch 113/200\n",
      "804/804 [==============================] - 1s 842us/step - loss: 3405.6399 - val_loss: 3378.0159\n",
      "Epoch 114/200\n",
      "804/804 [==============================] - 1s 842us/step - loss: 3404.8834 - val_loss: 3376.5566\n",
      "Epoch 115/200\n",
      "804/804 [==============================] - 1s 836us/step - loss: 3401.1150 - val_loss: 3379.8118\n",
      "Epoch 116/200\n",
      "804/804 [==============================] - 1s 863us/step - loss: 3401.5304 - val_loss: 3381.9983\n",
      "Epoch 117/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3399.6805 - val_loss: 3373.3352\n",
      "Epoch 118/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3400.0823 - val_loss: 3375.6572\n",
      "Epoch 119/200\n",
      "804/804 [==============================] - 1s 834us/step - loss: 3399.5139 - val_loss: 3374.2021\n",
      "Epoch 120/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3400.4594 - val_loss: 3373.2908\n",
      "Epoch 121/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3397.9995 - val_loss: 3372.5986\n",
      "Epoch 122/200\n",
      "804/804 [==============================] - 1s 854us/step - loss: 3396.8834 - val_loss: 3371.7324\n",
      "Epoch 123/200\n",
      "804/804 [==============================] - 1s 876us/step - loss: 3396.2166 - val_loss: 3372.8765\n",
      "Epoch 124/200\n",
      "804/804 [==============================] - 1s 844us/step - loss: 3396.0539 - val_loss: 3374.7891\n",
      "Epoch 125/200\n",
      "804/804 [==============================] - 1s 857us/step - loss: 3395.9922 - val_loss: 3379.9438\n",
      "Epoch 126/200\n",
      "804/804 [==============================] - 1s 851us/step - loss: 3397.3972 - val_loss: 3377.0815\n",
      "Epoch 127/200\n",
      "804/804 [==============================] - 1s 846us/step - loss: 3397.9263 - val_loss: 3375.8535\n",
      "Epoch 128/200\n",
      "804/804 [==============================] - 1s 850us/step - loss: 3396.3451 - val_loss: 3381.2927\n",
      "Epoch 129/200\n",
      "804/804 [==============================] - 1s 848us/step - loss: 3397.8366 - val_loss: 3377.3030\n",
      "Epoch 130/200\n",
      "804/804 [==============================] - 1s 847us/step - loss: 3395.5647 - val_loss: 3372.9573\n",
      "Epoch 131/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3395.1299 - val_loss: 3372.2947\n",
      "Epoch 132/200\n",
      "804/804 [==============================] - 1s 851us/step - loss: 3391.8980 - val_loss: 3375.5217\n",
      "Epoch 133/200\n",
      "804/804 [==============================] - 1s 817us/step - loss: 3395.1410 - val_loss: 3377.5330\n",
      "Epoch 134/200\n",
      "804/804 [==============================] - 1s 842us/step - loss: 3393.8457 - val_loss: 3374.0312\n",
      "Epoch 135/200\n",
      "804/804 [==============================] - 1s 836us/step - loss: 3393.9706 - val_loss: 3367.8765\n",
      "Epoch 136/200\n",
      "804/804 [==============================] - 1s 846us/step - loss: 3393.1260 - val_loss: 3371.5144\n",
      "Epoch 137/200\n",
      "804/804 [==============================] - 1s 867us/step - loss: 3392.7069 - val_loss: 3366.7708\n",
      "Epoch 138/200\n",
      "804/804 [==============================] - 1s 882us/step - loss: 3391.8058 - val_loss: 3368.6763\n",
      "Epoch 139/200\n",
      "804/804 [==============================] - 1s 848us/step - loss: 3391.8330 - val_loss: 3363.6116\n",
      "Epoch 140/200\n",
      "804/804 [==============================] - 1s 829us/step - loss: 3392.4908 - val_loss: 3365.2341\n",
      "Epoch 141/200\n",
      "804/804 [==============================] - 1s 839us/step - loss: 3391.6489 - val_loss: 3364.7822\n",
      "Epoch 142/200\n",
      "804/804 [==============================] - 1s 840us/step - loss: 3392.1926 - val_loss: 3365.8645\n",
      "Epoch 143/200\n",
      "804/804 [==============================] - 1s 830us/step - loss: 3391.0109 - val_loss: 3367.2437\n",
      "Epoch 144/200\n",
      "804/804 [==============================] - 1s 834us/step - loss: 3389.9004 - val_loss: 3367.1597\n",
      "Epoch 145/200\n",
      "804/804 [==============================] - 1s 853us/step - loss: 3387.4018 - val_loss: 3368.3303\n",
      "Epoch 146/200\n",
      "804/804 [==============================] - 1s 849us/step - loss: 3390.1093 - val_loss: 3367.4663\n",
      "Epoch 147/200\n",
      "804/804 [==============================] - 1s 833us/step - loss: 3388.7344 - val_loss: 3365.0842\n",
      "Epoch 148/200\n",
      "804/804 [==============================] - 1s 856us/step - loss: 3389.7052 - val_loss: 3373.7869\n",
      "Epoch 149/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804/804 [==============================] - 1s 857us/step - loss: 3389.8313 - val_loss: 3373.3215\n",
      "Epoch 150/200\n",
      "804/804 [==============================] - 1s 837us/step - loss: 3389.9119 - val_loss: 3365.3691\n",
      "Epoch 151/200\n",
      "804/804 [==============================] - 1s 853us/step - loss: 3386.2370 - val_loss: 3364.3159\n",
      "Epoch 152/200\n",
      "804/804 [==============================] - 1s 882us/step - loss: 3387.7392 - val_loss: 3369.0176\n",
      "Epoch 153/200\n",
      "804/804 [==============================] - 1s 861us/step - loss: 3389.3171 - val_loss: 3365.2278\n",
      "Epoch 154/200\n",
      "804/804 [==============================] - 1s 830us/step - loss: 3386.9156 - val_loss: 3364.8782\n",
      "Epoch 155/200\n",
      "804/804 [==============================] - 1s 832us/step - loss: 3387.0376 - val_loss: 3373.4854\n",
      "Epoch 156/200\n",
      "804/804 [==============================] - 1s 835us/step - loss: 3385.6808 - val_loss: 3365.1873\n",
      "Epoch 157/200\n",
      "804/804 [==============================] - 1s 849us/step - loss: 3384.4260 - val_loss: 3364.8652\n",
      "Epoch 158/200\n",
      "804/804 [==============================] - 1s 829us/step - loss: 3384.9292 - val_loss: 3366.3147\n",
      "Epoch 159/200\n",
      "804/804 [==============================] - 1s 828us/step - loss: 3387.3516 - val_loss: 3371.2847\n",
      "Epoch 160/200\n",
      "804/804 [==============================] - 1s 861us/step - loss: 3387.6140 - val_loss: 3372.3140\n",
      "Epoch 161/200\n",
      "804/804 [==============================] - 1s 845us/step - loss: 3385.2983 - val_loss: 3370.0283\n",
      "Epoch 162/200\n",
      "804/804 [==============================] - 1s 835us/step - loss: 3385.0718 - val_loss: 3377.7644\n",
      "Epoch 163/200\n",
      "804/804 [==============================] - 1s 844us/step - loss: 3385.8214 - val_loss: 3380.1882\n",
      "Epoch 164/200\n",
      "804/804 [==============================] - 1s 852us/step - loss: 3384.4565 - val_loss: 3373.3599\n",
      "Epoch 165/200\n",
      "804/804 [==============================] - 1s 837us/step - loss: 3383.6492 - val_loss: 3374.7732\n",
      "Epoch 166/200\n",
      "804/804 [==============================] - 1s 867us/step - loss: 3385.0782 - val_loss: 3375.8240\n",
      "Epoch 167/200\n",
      "804/804 [==============================] - 1s 869us/step - loss: 3383.3231 - val_loss: 3366.2158\n",
      "Epoch 168/200\n",
      "804/804 [==============================] - 1s 863us/step - loss: 3380.7199 - val_loss: 3364.7502\n",
      "Epoch 169/200\n",
      "804/804 [==============================] - 1s 829us/step - loss: 3384.1750 - val_loss: 3362.9709\n",
      "Epoch 170/200\n",
      "804/804 [==============================] - 1s 837us/step - loss: 3384.0487 - val_loss: 3359.9871\n",
      "Epoch 171/200\n",
      "804/804 [==============================] - 1s 838us/step - loss: 3385.0534 - val_loss: 3357.3352\n",
      "Epoch 172/200\n",
      "804/804 [==============================] - 1s 836us/step - loss: 3382.7320 - val_loss: 3355.9824\n",
      "Epoch 173/200\n",
      "804/804 [==============================] - 1s 831us/step - loss: 3381.4848 - val_loss: 3358.9785\n",
      "Epoch 174/200\n",
      "804/804 [==============================] - 1s 840us/step - loss: 3381.1492 - val_loss: 3358.4653\n",
      "Epoch 175/200\n",
      "804/804 [==============================] - 1s 853us/step - loss: 3378.5052 - val_loss: 3356.9153\n",
      "Epoch 176/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3380.8103 - val_loss: 3357.7756\n",
      "Epoch 177/200\n",
      "804/804 [==============================] - 1s 839us/step - loss: 3379.2196 - val_loss: 3358.8496\n",
      "Epoch 178/200\n",
      "804/804 [==============================] - 1s 845us/step - loss: 3378.6409 - val_loss: 3356.4712\n",
      "Epoch 179/200\n",
      "804/804 [==============================] - 1s 849us/step - loss: 3378.9020 - val_loss: 3362.6047\n",
      "Epoch 180/200\n",
      "804/804 [==============================] - 1s 851us/step - loss: 3376.7705 - val_loss: 3356.7332\n",
      "Epoch 181/200\n",
      "804/804 [==============================] - 1s 840us/step - loss: 3379.3066 - val_loss: 3363.1733\n",
      "Epoch 182/200\n",
      "804/804 [==============================] - 1s 873us/step - loss: 3379.5788 - val_loss: 3358.7415\n",
      "Epoch 183/200\n",
      "804/804 [==============================] - 1s 862us/step - loss: 3378.6198 - val_loss: 3359.6846\n",
      "Epoch 184/200\n",
      "804/804 [==============================] - 1s 846us/step - loss: 3376.8331 - val_loss: 3359.8726\n",
      "Epoch 185/200\n",
      "804/804 [==============================] - 1s 839us/step - loss: 3376.8187 - val_loss: 3356.0232\n",
      "Epoch 186/200\n",
      "804/804 [==============================] - 1s 841us/step - loss: 3376.1424 - val_loss: 3360.5291\n",
      "Epoch 187/200\n",
      "804/804 [==============================] - 1s 842us/step - loss: 3378.7161 - val_loss: 3358.6584\n",
      "Epoch 188/200\n",
      "804/804 [==============================] - 1s 842us/step - loss: 3377.4366 - val_loss: 3361.0400\n",
      "Epoch 189/200\n",
      "804/804 [==============================] - 1s 833us/step - loss: 3377.7034 - val_loss: 3357.3438\n",
      "Epoch 190/200\n",
      "804/804 [==============================] - 1s 834us/step - loss: 3377.3532 - val_loss: 3358.0183\n",
      "Epoch 191/200\n",
      "804/804 [==============================] - 1s 862us/step - loss: 3377.3433 - val_loss: 3357.2844\n",
      "Epoch 192/200\n",
      "804/804 [==============================] - 1s 850us/step - loss: 3375.5931 - val_loss: 3360.0310\n",
      "Epoch 193/200\n",
      "804/804 [==============================] - 1s 852us/step - loss: 3374.7999 - val_loss: 3352.6985\n",
      "Epoch 194/200\n",
      "804/804 [==============================] - 1s 850us/step - loss: 3375.3928 - val_loss: 3356.0737\n",
      "Epoch 195/200\n",
      "804/804 [==============================] - 1s 855us/step - loss: 3374.6969 - val_loss: 3353.8606\n",
      "Epoch 196/200\n",
      "804/804 [==============================] - 1s 843us/step - loss: 3375.7928 - val_loss: 3353.6331\n",
      "Epoch 197/200\n",
      "804/804 [==============================] - 1s 859us/step - loss: 3373.4582 - val_loss: 3354.7380\n",
      "Epoch 198/200\n",
      "804/804 [==============================] - 1s 870us/step - loss: 3375.1689 - val_loss: 3353.1289\n",
      "Epoch 199/200\n",
      "804/804 [==============================] - 1s 845us/step - loss: 3374.5209 - val_loss: 3355.9878\n",
      "Epoch 200/200\n",
      "804/804 [==============================] - 1s 846us/step - loss: 3374.8810 - val_loss: 3358.0713\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f6d0e8e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_model(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "decoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
