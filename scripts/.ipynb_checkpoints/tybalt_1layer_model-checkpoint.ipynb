{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with single hidden layer\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"all-pseudomonas-gene-normalized.pcl\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0)\n",
    "rnaseq = rnaseq.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "intermediate_dim = 100\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_1layer_{}_stats.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_1layer_{}_hist.png\".format(latent_dim))\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_1layer_encoded_{}.tsv\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexj\\AppData\\Local\\conda\\conda\\envs\\Pa\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_to_reconstruct = Dense(original_dim,\n",
    "                               kernel_initializer='glorot_uniform',\n",
    "                               activation='sigmoid')\n",
    "rnaseq_reconstruct = decoder_to_reconstruct(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1072 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "1072/1072 [==============================] - 1s 1ms/step - loss: 3812.7565 - val_loss: 3760.8546\n",
      "Epoch 2/100\n",
      "1072/1072 [==============================] - 1s 567us/step - loss: 3729.0367 - val_loss: 3691.6638\n",
      "Epoch 3/100\n",
      "1072/1072 [==============================] - 1s 552us/step - loss: 3670.6788 - val_loss: 3677.3672\n",
      "Epoch 4/100\n",
      "1072/1072 [==============================] - 1s 540us/step - loss: 3622.2727 - val_loss: 3628.7030\n",
      "Epoch 5/100\n",
      "1072/1072 [==============================] - 1s 484us/step - loss: 3588.8914 - val_loss: 3586.8926\n",
      "Epoch 6/100\n",
      "1072/1072 [==============================] - 1s 709us/step - loss: 3569.3799 - val_loss: 3591.0278\n",
      "Epoch 7/100\n",
      "1072/1072 [==============================] - 1s 724us/step - loss: 3542.5957 - val_loss: 3617.7612\n",
      "Epoch 8/100\n",
      "1072/1072 [==============================] - 1s 501us/step - loss: 3533.8719 - val_loss: 3537.8909\n",
      "Epoch 9/100\n",
      "1072/1072 [==============================] - 1s 484us/step - loss: 3520.4430 - val_loss: 3521.5466\n",
      "Epoch 10/100\n",
      "1072/1072 [==============================] - 1s 495us/step - loss: 3511.1290 - val_loss: 3514.9793\n",
      "Epoch 11/100\n",
      "1072/1072 [==============================] - 1s 493us/step - loss: 3504.9300 - val_loss: 3509.6650\n",
      "Epoch 12/100\n",
      "1072/1072 [==============================] - 0s 449us/step - loss: 3493.5913 - val_loss: 3498.3732\n",
      "Epoch 13/100\n",
      "1072/1072 [==============================] - 1s 487us/step - loss: 3488.6615 - val_loss: 3499.8076\n",
      "Epoch 14/100\n",
      "1072/1072 [==============================] - 1s 510us/step - loss: 3480.3005 - val_loss: 3504.3308\n",
      "Epoch 15/100\n",
      "1072/1072 [==============================] - 1s 573us/step - loss: 3473.8908 - val_loss: 3490.0654\n",
      "Epoch 16/100\n",
      "1072/1072 [==============================] - 1s 808us/step - loss: 3473.5263 - val_loss: 3505.2286\n",
      "Epoch 17/100\n",
      "1072/1072 [==============================] - 1s 687us/step - loss: 3469.0436 - val_loss: 3482.1415\n",
      "Epoch 18/100\n",
      "1072/1072 [==============================] - 1s 781us/step - loss: 3462.3551 - val_loss: 3483.0661\n",
      "Epoch 19/100\n",
      "1072/1072 [==============================] - 1s 635us/step - loss: 3458.0808 - val_loss: 3483.8261\n",
      "Epoch 20/100\n",
      "1072/1072 [==============================] - 1s 503us/step - loss: 3459.3134 - val_loss: 3471.8605\n",
      "Epoch 21/100\n",
      "1072/1072 [==============================] - 1s 497us/step - loss: 3457.4890 - val_loss: 3477.3135\n",
      "Epoch 22/100\n",
      "1072/1072 [==============================] - 1s 490us/step - loss: 3453.5048 - val_loss: 3477.1591\n",
      "Epoch 23/100\n",
      "1072/1072 [==============================] - 1s 653us/step - loss: 3451.5076 - val_loss: 3480.7226\n",
      "Epoch 24/100\n",
      "1072/1072 [==============================] - 1s 596us/step - loss: 3449.5942 - val_loss: 3491.5384\n",
      "Epoch 25/100\n",
      "1072/1072 [==============================] - 1s 579us/step - loss: 3447.4164 - val_loss: 3465.1837\n",
      "Epoch 26/100\n",
      "1072/1072 [==============================] - 1s 532us/step - loss: 3448.7889 - val_loss: 3493.2881\n",
      "Epoch 27/100\n",
      "1072/1072 [==============================] - 1s 550us/step - loss: 3442.9581 - val_loss: 3469.8190\n",
      "Epoch 28/100\n",
      "1072/1072 [==============================] - 1s 485us/step - loss: 3441.6295 - val_loss: 3469.9372\n",
      "Epoch 29/100\n",
      "1072/1072 [==============================] - 1s 505us/step - loss: 3443.9184 - val_loss: 3461.9761\n",
      "Epoch 30/100\n",
      "1072/1072 [==============================] - 1s 472us/step - loss: 3438.7733 - val_loss: 3458.1228\n",
      "Epoch 31/100\n",
      "1072/1072 [==============================] - 1s 485us/step - loss: 3438.0254 - val_loss: 3458.2044\n",
      "Epoch 32/100\n",
      "1072/1072 [==============================] - 1s 513us/step - loss: 3438.4769 - val_loss: 3450.4624\n",
      "Epoch 33/100\n",
      "1072/1072 [==============================] - 1s 519us/step - loss: 3434.9259 - val_loss: 3461.3170\n",
      "Epoch 34/100\n",
      "1072/1072 [==============================] - 1s 548us/step - loss: 3434.2749 - val_loss: 3467.5704\n",
      "Epoch 35/100\n",
      "1072/1072 [==============================] - 1s 558us/step - loss: 3433.9552 - val_loss: 3454.3579\n",
      "Epoch 36/100\n",
      "1072/1072 [==============================] - 1s 538us/step - loss: 3431.5525 - val_loss: 3451.3573\n",
      "Epoch 37/100\n",
      "1072/1072 [==============================] - 1s 628us/step - loss: 3430.1098 - val_loss: 3443.5554\n",
      "Epoch 38/100\n",
      "1072/1072 [==============================] - 0s 457us/step - loss: 3426.9062 - val_loss: 3447.5641\n",
      "Epoch 39/100\n",
      "1072/1072 [==============================] - 1s 629us/step - loss: 3427.5601 - val_loss: 3449.1092\n",
      "Epoch 40/100\n",
      "1072/1072 [==============================] - 0s 453us/step - loss: 3425.4581 - val_loss: 3448.5413\n",
      "Epoch 41/100\n",
      "1072/1072 [==============================] - 1s 524us/step - loss: 3422.4468 - val_loss: 3448.6063\n",
      "Epoch 42/100\n",
      "1072/1072 [==============================] - 1s 652us/step - loss: 3423.7772 - val_loss: 3445.7818\n",
      "Epoch 43/100\n",
      "1072/1072 [==============================] - 1s 502us/step - loss: 3423.6156 - val_loss: 3435.5863\n",
      "Epoch 44/100\n",
      "1072/1072 [==============================] - 1s 555us/step - loss: 3421.7262 - val_loss: 3440.0599\n",
      "Epoch 45/100\n",
      "1072/1072 [==============================] - 1s 548us/step - loss: 3419.9444 - val_loss: 3435.1576\n",
      "Epoch 46/100\n",
      "1072/1072 [==============================] - 1s 547us/step - loss: 3419.6334 - val_loss: 3435.9767\n",
      "Epoch 47/100\n",
      "1072/1072 [==============================] - 0s 446us/step - loss: 3418.3451 - val_loss: 3439.2997\n",
      "Epoch 48/100\n",
      "1072/1072 [==============================] - 1s 476us/step - loss: 3419.6630 - val_loss: 3435.0918\n",
      "Epoch 49/100\n",
      "1072/1072 [==============================] - 0s 448us/step - loss: 3415.9859 - val_loss: 3436.2431\n",
      "Epoch 50/100\n",
      "1072/1072 [==============================] - 1s 467us/step - loss: 3415.6254 - val_loss: 3438.6580\n",
      "Epoch 51/100\n",
      "1072/1072 [==============================] - 0s 455us/step - loss: 3417.2754 - val_loss: 3434.9795\n",
      "Epoch 52/100\n",
      "1072/1072 [==============================] - 0s 465us/step - loss: 3415.1964 - val_loss: 3437.0532\n",
      "Epoch 53/100\n",
      "1072/1072 [==============================] - 0s 449us/step - loss: 3413.2686 - val_loss: 3438.0109\n",
      "Epoch 54/100\n",
      "1072/1072 [==============================] - 0s 453us/step - loss: 3413.3445 - val_loss: 3436.1636\n",
      "Epoch 55/100\n",
      "1072/1072 [==============================] - 0s 451us/step - loss: 3411.9479 - val_loss: 3433.9772\n",
      "Epoch 56/100\n",
      "1072/1072 [==============================] - 0s 460us/step - loss: 3409.7720 - val_loss: 3435.0947\n",
      "Epoch 57/100\n",
      "1072/1072 [==============================] - 0s 443us/step - loss: 3412.1085 - val_loss: 3436.3869\n",
      "Epoch 58/100\n",
      "1072/1072 [==============================] - 0s 466us/step - loss: 3409.4587 - val_loss: 3431.7331\n",
      "Epoch 59/100\n",
      "1072/1072 [==============================] - 0s 460us/step - loss: 3407.9862 - val_loss: 3429.9217\n",
      "Epoch 60/100\n",
      "1072/1072 [==============================] - 1s 469us/step - loss: 3408.2120 - val_loss: 3426.2858\n",
      "Epoch 61/100\n",
      "1072/1072 [==============================] - 0s 459us/step - loss: 3409.8155 - val_loss: 3430.2198\n",
      "Epoch 62/100\n",
      "1072/1072 [==============================] - 1s 514us/step - loss: 3407.4975 - val_loss: 3420.6752\n",
      "Epoch 63/100\n",
      "1072/1072 [==============================] - 1s 562us/step - loss: 3407.5797 - val_loss: 3421.1376\n",
      "Epoch 64/100\n",
      "1072/1072 [==============================] - 1s 691us/step - loss: 3408.1866 - val_loss: 3423.7305\n",
      "Epoch 65/100\n",
      "1072/1072 [==============================] - 0s 457us/step - loss: 3407.2564 - val_loss: 3431.9296\n",
      "Epoch 66/100\n",
      "1072/1072 [==============================] - 1s 675us/step - loss: 3406.7647 - val_loss: 3421.9345\n",
      "Epoch 67/100\n",
      "1072/1072 [==============================] - 1s 479us/step - loss: 3404.3797 - val_loss: 3430.3340\n",
      "Epoch 68/100\n",
      "1072/1072 [==============================] - 0s 455us/step - loss: 3403.2657 - val_loss: 3419.4725\n",
      "Epoch 69/100\n",
      "1072/1072 [==============================] - 1s 472us/step - loss: 3402.9906 - val_loss: 3416.7382\n",
      "Epoch 70/100\n",
      "1072/1072 [==============================] - 0s 451us/step - loss: 3403.9671 - val_loss: 3417.9264\n",
      "Epoch 71/100\n",
      "1072/1072 [==============================] - 0s 466us/step - loss: 3403.5906 - val_loss: 3413.9428\n",
      "Epoch 72/100\n",
      "1072/1072 [==============================] - 0s 458us/step - loss: 3402.5058 - val_loss: 3417.8001\n",
      "Epoch 73/100\n",
      "1072/1072 [==============================] - 0s 457us/step - loss: 3402.3097 - val_loss: 3420.3724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "1072/1072 [==============================] - 1s 580us/step - loss: 3399.8633 - val_loss: 3418.6447\n",
      "Epoch 75/100\n",
      "1072/1072 [==============================] - 1s 518us/step - loss: 3399.4645 - val_loss: 3417.8789\n",
      "Epoch 76/100\n",
      "1072/1072 [==============================] - 1s 543us/step - loss: 3400.9705 - val_loss: 3424.9216\n",
      "Epoch 77/100\n",
      "1072/1072 [==============================] - 0s 464us/step - loss: 3398.3183 - val_loss: 3422.5690\n",
      "Epoch 78/100\n",
      "1072/1072 [==============================] - 1s 504us/step - loss: 3399.0416 - val_loss: 3418.6666\n",
      "Epoch 79/100\n",
      "1072/1072 [==============================] - 1s 533us/step - loss: 3398.2116 - val_loss: 3413.9653\n",
      "Epoch 80/100\n",
      "1072/1072 [==============================] - 1s 530us/step - loss: 3397.4418 - val_loss: 3412.8165\n",
      "Epoch 81/100\n",
      "1072/1072 [==============================] - 1s 603us/step - loss: 3398.7511 - val_loss: 3411.9232\n",
      "Epoch 82/100\n",
      "1072/1072 [==============================] - 1s 498us/step - loss: 3398.4869 - val_loss: 3412.6349\n",
      "Epoch 83/100\n",
      "1072/1072 [==============================] - 0s 455us/step - loss: 3396.8181 - val_loss: 3415.4136\n",
      "Epoch 84/100\n",
      "1072/1072 [==============================] - 1s 483us/step - loss: 3396.2354 - val_loss: 3413.4685\n",
      "Epoch 85/100\n",
      "1072/1072 [==============================] - 0s 451us/step - loss: 3396.6926 - val_loss: 3413.3666\n",
      "Epoch 86/100\n",
      "1072/1072 [==============================] - 1s 566us/step - loss: 3394.9980 - val_loss: 3411.2339\n",
      "Epoch 87/100\n",
      "1072/1072 [==============================] - 1s 583us/step - loss: 3396.8268 - val_loss: 3412.7609\n",
      "Epoch 88/100\n",
      "1072/1072 [==============================] - 0s 457us/step - loss: 3395.1968 - val_loss: 3409.6114\n",
      "Epoch 89/100\n",
      "1072/1072 [==============================] - 1s 538us/step - loss: 3395.1927 - val_loss: 3410.2864\n",
      "Epoch 90/100\n",
      "1072/1072 [==============================] - 1s 530us/step - loss: 3394.7487 - val_loss: 3407.7949\n",
      "Epoch 91/100\n",
      "1072/1072 [==============================] - 0s 449us/step - loss: 3393.1207 - val_loss: 3409.1622\n",
      "Epoch 92/100\n",
      "1072/1072 [==============================] - 0s 450us/step - loss: 3394.7325 - val_loss: 3408.8498\n",
      "Epoch 93/100\n",
      "1072/1072 [==============================] - 1s 553us/step - loss: 3394.5923 - val_loss: 3413.3663\n",
      "Epoch 94/100\n",
      "1072/1072 [==============================] - 1s 582us/step - loss: 3392.6987 - val_loss: 3414.0007\n",
      "Epoch 95/100\n",
      "1072/1072 [==============================] - 1s 810us/step - loss: 3392.8938 - val_loss: 3409.4942\n",
      "Epoch 96/100\n",
      "1072/1072 [==============================] - 1s 550us/step - loss: 3392.3387 - val_loss: 3409.7554\n",
      "Epoch 97/100\n",
      "1072/1072 [==============================] - 1s 530us/step - loss: 3391.9350 - val_loss: 3410.1964\n",
      "Epoch 98/100\n",
      "1072/1072 [==============================] - 1s 557us/step - loss: 3392.1605 - val_loss: 3408.8619\n",
      "Epoch 99/100\n",
      "1072/1072 [==============================] - 1s 495us/step - loss: 3391.2415 - val_loss: 3408.5336\n",
      "Epoch 100/100\n",
      "1072/1072 [==============================] - 1s 466us/step - loss: 3391.5315 - val_loss: 3409.5137\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xd4XNW18OHfmtGMeu+WXOResQ3GphpwHEyNQwk21fRLubQkhJCEQEK4CZd8kJAQAjehXcrFoTr0jinu4IJxxVWWbPXeZ/b3xz62ZFmSJVujkUbrfZ55NLPnzDnreEBLu4sxBqWUUqqzXMEOQCmlVN+iiUMppVSXaOJQSinVJZo4lFJKdYkmDqWUUl2iiUMppVSXaOJQSinVJZo4lFJKdYkmDqWUUl0SFuwAAiElJcUMGTIk2GEopVSfsmLFiiJjTOrBjgvJxDFkyBCWL18e7DCUUqpPEZHtnTlOm6qUUkp1iSYOpZRSXaKJQymlVJeEZB+HUqp/amxsJDc3l7q6umCH0qtFRESQnZ2Nx+M5pM9r4lBKhYzc3FxiY2MZMmQIIhLscHolYwzFxcXk5uaSk5NzSOfQpiqlVMioq6sjOTlZk0YHRITk5OTDqpVp4lBKhRRNGgd3uP9GmjhayCur5cH3NrCtqDrYoSilVK+liaOFkuoGHv5oMxv2VAY7FKVUHxUTExPsEAJOE0cLSdFeAEqrG4IciVJK9V6aOFrYmzhKajRxKKUOjzGG22+/nfHjxzNhwgRefPFFAPLz85k+fTqTJk1i/PjxfPbZZ/h8Pi6//PJ9xz700ENBjr5jARuOKyIRwEIg3LnOS8aYu0Xke8AD2KRVBVxujNksIuHAM8BRQDEwxxizzTnXncBVgA+42RjzbiBijvC4ifK6tcahVAj4zb/X8m1eRbeec+yAOO4+e1ynjn3llVdYuXIlq1atoqioiKOPPprp06fz/PPPM2vWLH75y1/i8/moqalh5cqV7Nq1i2+++QaAsrKybo27uwWyxlEPzDDGTAQmAaeJyDHAo8DFxphJwPPAr5zjrwJKjTHDgYeA+wFEZCwwFxgHnAb8TUTcgQo6McpLSXVjoE6vlOonPv/8cy688ELcbjfp6emcdNJJLFu2jKOPPponn3ySe+65hzVr1hAbG8vQoUPZsmULN910E++88w5xcXHBDr9DAatxGGMMtkYB4HEexnns/VeJB/Kc57OBe5znLwF/FTtmbDbwf8aYemCriGwGpgKLAhF3UrSXUm2qUqrP62zNIFDsr8ADTZ8+nYULF/Lmm29y6aWXcvvtt3PZZZexatUq3n33XR555BHmz5/PE0880cMRd15A+zhExC0iK4EC4H1jzBLgauAtEckFLgX+4ByeBewEMMY0AeVAcstyR65T1vpa14rIchFZXlhYeMgxJ0Z7KdGmKqXUYZo+fTovvvgiPp+PwsJCFi5cyNSpU9m+fTtpaWlcc801XHXVVXz11VcUFRXh9/s577zzuPfee/nqq6+CHX6HArrkiDHGB0wSkQTgVREZD9wGnGGMWSIitwMPYpNJWzNSTAflra/1OPA4wJQpU9pO9Z2QFOVhe7HO41BKHZ5zzjmHRYsWMXHiRESE//7v/yYjI4Onn36aBx54AI/HQ0xMDM888wy7du3iiiuuwO/3A/D73/8+yNF3rEfWqjLGlInIJ8DpwESn5gHwIvCO8zwXGAjkikgYthmrpEX5Xtk0N291O61xKKUOR1WVbaEXER544AEeeOCB/d6fN28e8+bNO+Bzvb2W0VLAmqpEJNWpaSAikcBMYB0QLyIjncO+75QBLAD2/mueD3zk9JMsAOaKSLiI5AAjgKWBijspyktlXRONPn+gLqGUUn1aIGscmcDTzggoFzDfGPOGiFwDvCwifqAUuNI5/p/A/zqd3yXYkVQYY9aKyHzgW6AJuNFpAguIxL2TAGsaSIuNCNRllFKqzwrkqKrVwOQ2yl8FXm2jvA74UTvnug+4r7tjbEvz7PFGTRxKKdUGnTneSmKUM3tc+zmUUqpNmjha2bfsiCYOpZRqkyaOVhKj7VaKul6VUkq1TRNHK3ubqnS9KqWUapsmjlY8bhexEWHaVKWUCriO9u7Ytm0b48eP78FoOk8TRxt0vSqllGpfj8wc72vsCrmaOJTq097+Oexe073nzJgAp/+h3bfvuOMOBg8ezA033ADAPffcg4iwcOFCSktLaWxs5He/+x2zZ8/u0mXr6uq4/vrrWb58OWFhYTz44IOccsoprF27liuuuIKGhgb8fj8vv/wyAwYM4IILLiA3Nxefz8ddd93FnDlzDuu2W9PE0YakaC8FlXXBDkMp1cfMnTuXW2+9dV/imD9/Pu+88w633XYbcXFxFBUVccwxx/CDH/wAu/h35zzyyCMArFmzhvXr13PqqaeyceNG/v73v3PLLbdw8cUX09DQgM/n46233mLAgAG8+eabAJSXl3f7fWriaENilJcNu3XfcaX6tA5qBoEyefJkCgoKyMvLo7CwkMTERDIzM7nttttYuHAhLpeLXbt2sWfPHjIyMjp93s8//5ybbroJgNGjRzN48GA2btzIsccey3333Udubi7nnnsuI0aMYMKECfz0pz/ljjvu4KyzzuLEE0/s9vvUPo42JEV7tKlKKXVIzj//fF566SVefPFF5s6dy3PPPUdhYSErVqxg5cqVpKenU1fXtRaN9vb2uOiii1iwYAGRkZHMmjWLjz76iJEjR7JixQomTJjAnXfeyW9/+9vuuK39aI2jDYnRXmobfdQ2+Ij0BmyzQaVUCJo7dy7XXHMNRUVFfPrpp8yfP5+0tDQ8Hg8ff/wx27dv7/I5p0+fznPPPceMGTPYuHEjO3bsYNSoUWzZsoWhQ4dy8803s2XLFlavXs3o0aNJSkrikksuISYmhqeeeqrb71ETR0uNdVC6lTSPze6lNQ1EeiODHJRSqi8ZN24clZWVZGVlkZmZycUXX8zZZ5/NlClTmDRpEqNHj+7yOW+44Qauu+46JkyYQFhYGE899RTh4eG8+OKLPPvss3g8HjIyMvj1r3/NsmXLuP3223G5XHg8Hh599NFuv0dprwrUl02ZMsUsX7686x/cuQz+OZMVJzzOeR/E8MZNJzA+K777A1RKBcS6desYM2ZMsMPoE9r6txKRFcaYKQf7rPZxtBSTBkCSKQXQuRxKKdUGbapqKSYdgLimEmCgdpArpQJuzZo1XHrppfuVhYeHs2TJknY+EXyaOFryREBEPNGNxYCuV6VUX2SM6dIciWCbMGECK1eu7NFrHm4XhTZVtRaTjreuCJdASU1jsKNRSnVBREQExcXFh/2LMZQZYyguLiYi4tA3qtMaR2sx6biq9pAQ5dUah1J9THZ2Nrm5uRQWFgY7lF4tIiKC7OzsQ/68Jo7WYtIg72sSozy6J4dSfYzH4yEnJyfYYYQ8bapqLSYDqgrsCrla41BKqQNo4mgtJg0aqsiI8OmoKqWUaoMmjtacIbmDvJWaOJRSqg2aOFpzJgFmhlVQWtOgozOUUqoVTRytxdqljjNc5TT6DFX1TUEOSCmlehdNHK05TVXJlAFQWq1zOZRSqiVNHK1FJoG4SfSXAOiQXKWUakUTR2suF8SkEdtoE4cOyVVKqf1p4mhLTDpRDXa9qqKq+iAHo5RSvYsmjrbEpBNeZ5cs2F3etS0elVIq1GniaEtMGq7qApKjveRXaOJQSqmWNHG0JSYdqgsZEOchv6w22NEopVSvoomjLbEZYHwMj20kX5uqlFJqP5o42uLMHh8eWUWe1jiUUmo/mjja4kwCHBxeRUVdE9U6e1wppfbRxNGWvetVuSsAtLlKKaVa0MTRFqfGkSp22ZH8cm2uUkqpvTRxtMUbDd7YfcuO5JdpjUMppfYKWOIQkQgRWSoiq0RkrYj8xin/TERWOo88EXnNKRcReVhENovIahE5ssW55onIJucxL1Ax7ycmbd/s8TytcSil1D6B3HO8HphhjKkSEQ/wuYi8bYw5ce8BIvIy8Lrz8nRghPOYBjwKTBORJOBuYApggBUissAYUxrA2CEmHXd1ISkx4Tp7XCmlWghYjcNYVc5Lj/PYtyuSiMQCM4DXnKLZwDPO5xYDCSKSCcwC3jfGlDjJ4n3gtEDFvU9sOlTtYUBCBHmaOJRSap+A9nGIiFtEVgIF2F/+S1q8fQ7woTGmwnmdBexs8X6uU9ZeeWDF2MSRGR+hs8eVUqqFgCYOY4zPGDMJyAamisj4Fm9fCLzQ4rW0dYoOyvcjIteKyHIRWV5YWHg4YVsxaVBfwaBY0eG4SinVQo+MqjLGlAGf4DQxiUgyMBV4s8VhucDAFq+zgbwOyltf43FjzBRjzJTU1NTDD9oZkjs0spqq+iYq63QnQKWUgsCOqkoVkQTneSQwE1jvvP0j4A1jTMs/5RcAlzmjq44Byo0x+cC7wKkikigiicCpTllgxdi9x7M9lYBOAlRKqb0COaoqE3haRNzYBDXfGPOG895c4A+tjn8LOAPYDNQAVwAYY0pE5F5gmXPcb40xJQGM29o7e9xVBiSQV1bLyPTYgF9WKaV6u4AlDmPMamByO++d3EaZAW5s5/gngCe6M76DSh4G4XEMyHsXmKM1DqWUcujM8fZ4o2HKFURu+jeDZY+OrFJKKYcmjo5Mux4RNzdGvqc1DqWUcmji6EhcJhwxh9n+j6gs2RPsaJRSqlfQxHEwx91EOPVMK34l2JEopVSvoInjYNJGszH+eGbXv4FpqAl2NEopFXSaODph47ArSJJKale+HOxQlFIq6DRxdIIMPp5a46U2d1WwQ1FKqaDTxNEJmYmR7DBp+Iu3BjsUpZQKOk0cnTAg3iaOsPLtwQ5FKaWCThNHJ6TGhpNLOtE1uWAOWJhXKaX6FU0cneB2CVWR2Xj9tVDdDUu2K6VUH6aJo5MaEwbbJ6XbghqHUkoFmyaOTvIkDwXAlGwJciRKKRVcmjg6KSZ9KH4j1BVo4lBK9W+aODopOy2J3SRSu2dzsENRSqmg0sTRSYOTo9hh0jGlOpdDKdW/aeLopIGJUezwpxFRuSPYoSilVFBp4uikSK+bEu8AohuKQBc7VEr1Y5o4uqAudpB9UqYzyJVS/Zcmjq5IHGJ/6lwOpVQ/dtDEISLHi0i08/wSEXlQRAYHPrTeJzJ9BACNRd8FORKllAqeztQ4HgVqRGQi8DNgO/BMQKPqpdLTM6k0kVTv1iG5Sqn+qzOJo8kYY4DZwJ+NMX8GYgMbVu80MDmaHSaNpiKdBKiU6r86kzgqReRO4BLgTRFxA57AhtU72bkcaYRV6JBcpVT/1ZnEMQeoB64yxuwGsoAHAhpVL5Uc7SXflUFMzS7w+4MdjlJKBUVYJ46pxDZR+URkJDAaeCGwYfVOIkJV1EDCahugMh/is4IdklJK9bjO1DgWAuEikgV8CFwBPBXIoHoz/77l1XXpEaVU/9SZxCHGmBrgXOAvxphzgHGBDav38qYOA8BfoolDKdU/dSpxiMixwMXAm06ZO3Ah9W5xGTk0GRc1uzcFOxSllAqKziSOW4E7gVeNMWtFZCjwcWDD6r0GpcSz3aTTsHtDsENRSqmgOGjnuDHmU+BTEYkVkRhjzBbg5sCH1jsNTo5ioxnA1FKdBKiU6p86s+TIBBH5GvgG+FZEVohIv+3jGJAQyRYGEF21HXxNwQ5HKaV6XGeaqh4DfmyMGWyMGQT8BPifwIbVe3ncLqpihhJmGjteJdeYngtKKaV6UGcSR7QxZl+fhjHmEyA6YBH1AZ6M0QCYoo3tH/TSlfD6jT0UkVJK9ZzOJI4tInKXiAxxHr8C+vVY1JTBtqWuMvfbtg9oaoANb0P+qh6MSimlekZnEseVQCrwivNIAS4PYEy93qicgRSa+PYTR95X0FQL1cU9G5hSSvWAzoyqKqXVKCoR+SPw00AF1duNyYxjtRnAoPaaqrZ9bn/WFNm+DpGeC04ppQLsUHcAvKBbo+hjorxhFIYPJr56a9ud4Nu/sD99DVBf2bPBKaVUgB1q4jjon9AiEiEiS0VklYisFZHfOOUiIveJyEYRWSciN7cof1hENovIahE5ssW55onIJucx7xBj7laNicOJ9ldCTavmKF8T7FgCEfH2dU1RzwenlFIB1G5TlYgktfcWnUgc2KXYZxhjqkTEA3wuIm8DY4CBwGhjjF9E0pzjTwdGOI9p2J0Hpzlx3A1MAQywQkQWOE1oQRM5YAwUQNmOtSSMOan5jfxV0FgNYy+CVc/bfo6kocELVCmlullHNY4VwHLnZ8vHcqDhYCc2VpXz0uM8DHA98FtjjN85rsA5ZjbwjPO5xUCCiGQCs4D3jTElTrJ4Hzita7fZ/dKHHgHAni2tRk5td/o3xv7A/tQah1IqxLRb4zDG5BzuyZ3dAlcAw4FHjDFLRGQYMEdEzgEKgZuNMZuwG0TtbPHxXKesvfKgGj5iFLXGS23euv3f2PYFJA+HtLH2dbUmDqVUaDnUPo5OMcb4jDGTgGxgqoiMB8KBOmPMFOwM9Cecw9tq/jIdlO9HRK4VkeUisrywsLB7bqADcZHh5LqzCGu5ZpXfBzsWweDjITrFlmmNQykVYgKaOPYyxpQBn2CbmHKBl523XgWOcJ7nYvs+9soG8joob32Nx40xU4wxU1JTU7s1/vaUR+eQVNti2ZE930B9BQw5AbzREBapNQ6lVMgJWOIQkVQRSXCeRwIzgfXAa8AM57CTgL2TIRYAlzmjq44Byo0x+cC7wKkikigiicCpTlnQmeSRZPgLKCsvtwXbnGG4g4+zP6NTDhx1pZRSfVy7iUNEZrR4ntPqvXM7ce5M4GMRWQ0sw3ZwvwH8AThPRNYAvweudo5/C9gCbMY2Yd0AYIwpAe51zrEM27Fe0qm7C7DY7LG4xLB14xrw+2HzB5AwGOKz7QFRyVrjUEqFnI5mjv8R2DuX4uUWzwF+hV1+pF3GmNXA5DbKy4Az2yg3QJurAhpjnqC5L6TXGDD8CPgcmtYugNX3ws7FcMJtzQdEp2gfh1Iq5HSUOKSd52297pfiskbjRzh622MQmQQ/fBQmXth8QFQKFHawgq5SSvVBHSUO087ztl73T55I1qXM4ts9tcy88u8kpg7Y/32tcSilQlBHiWOoiCzA1i72Psd5fdhzPELGuY9z+8Of8/utTVzYejBXVDI01kBDDXijghKeUkp1t44Sx+wWz//Y6r3Wr/utsZlx5KRE8+bqfC6cOmj/N1vO5fAOOvDDSinVB3U0c/zTtspFZCAwF2jz/f5GRDhzQiZ/+2QzxVX1JMeEN78Z5SSO6iJI0MShlAoNnZrHISIpInK9iCzETuRLD2hUfcyZR2TiN/DO2t37v7GvxqFzOZRSoaOjeRyxInKZiLwDLMWuNzXUGDPMGNNvN3Fqy+iMWIam2uaq/UQl2586l0MpFUI6qnEUAFcB9wHDjDE/oROr4vZHIsJZEzJZvKWYwsr65jd0vSqlVAjqKHH8AojA7otxp7OqrWrHmUcMOLC5KjwOXB6tcSilQkq7icMY85AxZhrwA+wQ3NeAASJyh4iM7KkA+4qR6TEMTY3m/W/3NBeK6FwOpVTIOWjnuDFmizHmPmPMBOBoIB54O+CR9TEiwnHDklmxrYQmn7/5jagUuwugUkqFiI46x/8qIse3LDPGrDHG/MIYo81WbZiWk0x1g4+1eRXNhdHJWuNQSoWUjmocm4A/isg2EblfRCb1VFB91bQcu0370q0tFu+NStE+DqVUSOmoj+PPxphjsXtmlABPisg6Efm19nG0LS0ugpyUaJZsbdE0pXtyKKVCTGf6OLYbY+43xkwGLgLOAdYd5GP91rScJJZuLcHnd9aBjEqxuwI21Xf8QaWU6iMOmjhExCMiZ4vIc9hO8Y3AeQGPrI+aNjSJiromNuyutAXRziRArXUopUJER53j3xeRJ7B7fl+L3aFvmDFmjjHmtZ4KsK+ZmmMTxb7mqpbrVSmlVAg42ATARcAYY8zZxpjnjDHVPRRXn5WVEEl2YiRLtjgd5Dp7XCkVYjpaHfeUngwklEzLSebjDQUYY5B9NQ5tqlJKhYZOrY6rumZaThIl1Q1sLqjSGodSKuRo4giAaUPtfI4lW0sgIgHErX0cSqmQoYkjAAYlRZERF8EnGwrB5YKoJK1xKKVChiaOABARLjh6IB+s28PrK3fp7HGlVEjRxBEgN88YztFDEvnFK2uo9SboPA6lVMjQxBEgYW4XD184mXCPmw8KEzD5q6CuPNhhKaXUYdPEEUCZ8ZE8eMFEHq88HmmsgdXzgx2SUkodNk0cAXbyqDSOOuYUvvEPoX7JE2BMsENSSqnDoomjB9wwYzjzzUzCi7+F3OXBDkcppQ6LJo4ekBYbQcSRc6g24VQt+keww1FKqcOiiaOHXDnjCN4wx+Nd9yrUlgU7HKWUOmSaOHpIRnwERaMuwmvqKV/6bLDDUUqpQ6aJowfNPuNM1vhz8H35N130UCnVZ2ni6EHZiVEsGnYbUXUF1P7zLKgpOfiHlFKql9HE0cMu+NGF/MxzJ66SzfifOluTh1Kqz9HE0cMSorzMmTuPqxt+jK9wIzx1FpTtCHZYSinVaZo4guD44SmMPn42l9f/hMbSHfD4ybDt82CHpZRSnaKJI0h+OmsUxWnH8aOme6n3JMAzs2HZP4MdllJKHZQmjiAJD3Pz90uOosA7iJPL76J8wInw5o9hx+Jgh6aUUh0KWOIQkQgRWSoiq0RkrYj8xil/SkS2ishK5zHJKRcReVhENovIahE5ssW55onIJucxL1Ax97QhKdH86/rjCI9OYMaOy6mPyoC37wC/v+sn2/Mt+Bq7P0illGolkDWOemCGMWYiMAk4TUSOcd673RgzyXmsdMpOB0Y4j2uBRwFEJAm4G5gGTAXuFpHEAMbdo7ISIpl/3bGkJCbxi8rzIX8lrHyu7YPXvgpPnAbF3+1fvvsbePQ4WKbLmSilAi9gicNYVc5Lj/PoaGnY2cAzzucWAwkikgnMAt43xpQYY0qB94HTAhV3MKTFRvDUlUfzFifwXcQ4+PA3++/dUVsKL18N/7ocdiyCzx7c/wRLHgUMbHynJ8NWSvVTAe3jEBG3iKwECrC//Jc4b93nNEc9JCLhTlkWsLPFx3OdsvbKQ0pmfCQ3nDycW8ovxFQXwQf3wJqX4O2fw9+OtbWNk38BU66E1S9CRZ79YHUxrP4XuMNh+5dQX9XhdZRS6nAFNHEYY3zGmElANjBVRMYDdwKjgaOBJOAO53Bp6xQdlO9HRK4VkeUisrywsLBb4u9p10wfSmn8ON7zzoTlT8DLV8GKpyBlBFz1Ppx8Bxx/CxgfLPm7/dCKJ8FXD6feC74G2PZZUO9BKRX6emRUlTGmDPgEOM0Yk+80R9UDT2L7LcDWJAa2+Fg2kNdBeetrPG6MmWKMmZKamhqAuwi8CI+bX545hp9UzOHLCffCtZ/CnTth3r8hyxkrkDgExv4Qlj9pZ50v+ycMPRmOuhw80bDp/eDdgFKqXwjkqKpUEUlwnkcCM4H1Tr8FIiLAD4FvnI8sAC5zRlcdA5QbY/KBd4FTRSTR6RQ/1SkLSaePz2BcTjY3rB3NNyYH3J4DDzr+ZqivgBcuhMo8mHY9hIVDznTY/H7P7jJYkQdv/Qwa63rumkqpoApkjSMT+FhEVgPLsH0cbwDPicgaYA2QAvzOOf4tYAuwGfgf4AYAY0wJcK9zjmXAb52ykCQi3HfOBCLC3Jz7ty/530XbMK0TwYDJNknsXAyJOTDiVFs+YqZdvqR4c/cHtvZVeO6CA4cKr3oBlj4G333U/ddUSvVKYYE6sTFmNTC5jfIZ7RxvgBvbee8J4IluDbAXG54Ww1u3nMiP56/krtfXsmRrCX/80UQiPO7mg46/FbYuhGnXgcvJ/8O/b39uet/2i3SX6mJ44zY7uqvgW8gY3/ze3gmL330Io8/ovmsqpXotnTneSyVFe3li3tH87LRRvLE6n9/8e+3+Bwz/Hlz1AUy9prkscTCkjLTNVd3pw3uahwdv/6K53O+Hnc5Auc0fdO81lVK9liaOXszlEm44eTg3nDyMF5bu5MVlrVbRHXg0uNz7lw2fCdu+gIaa5jK/HyryYdeK/eeHdMbOpfDVM3DsjZAwaP9RW4Xr7fmyjoLSbQdOTFRKhaSANVWp7vOTU0exOrecu15fy5jMOI7ITmj/4OEzYfHf4JVroLEGSrZA+S7wO8uR5JwEl70O0tYo51Z8TXb9rNgBcNLP7SiuDW/bRORy2cmIAKf8Ep49FzZ/CMnDDv+GlVK9mtY4+gC3S3j4wsmkxoRz/bNfUVDZwQimwcdDXBZs/cz2SQw4Eo77Tzjz/8ExN8LWTzvXrFRbBm/9FHavgdP+C8JjYMgJUFtiaxpg+zdi0mHYDNtJ/92H3XPDSqleTWscfURStJdHLzmSOY8t5sLHF/P8NceQHhdx4IGeCLj1G1ujaF2raGqAjW/D+7+2v+z3NnPVV0HJd+D22seGt2HhA7YZ6uhr7LwRsEkJ7N4h6WPtqK5Bx9jrDP8erHwemurt0GClVMjSGkcfckR2Ak9fOZXd5XXMfXwx+eW1bR/ocrXdFBXmhe/dbUdGrXrBluWthL8eDY9Nh78dA385Et77pe23uO4zOPOPzedKHAzxTj9H+S479Hegs27l8Jm2aUyXhVcq5GmNo4+ZmpPEM1dN5fInlnHBY4s4fXwmMeFhxEd6OGNCJqmxB/lrf+xsyJoCH/0OXGHw71shKhnO/YetgfgabSf44GPb/vyQE2DTu839G4OcxDHkRHB5bDPY0JO674aVUr2OHDC5LARMmTLFLF++PNhhBNTKnWX8+MWV5JfXUdvoA2xz1n0/HM/pEzI7/vD2L+HJ0+3z7Kkw9zmISevchb9+Fl6/0U5AzF0BP98Bbufvj6fOsv0q13/R8TmUUr2SiKwwxkw52HFa4+ijJg1M4KOfngxAo8/Ppj1V3PHyaq5/7ivOmZzFb2aPIy6ijeVKAAYfZzvKffVw6n22X6Szhpxgf25daEdouVv8JzT8e3ZV32fPtwsxhkXAzN9A6shDukelVO+kfRwhwON2MXZAHK/ccBy3zhzBv1flcck/llBe28F05fpTAAAamklEQVSOgKf9lx1p1ZWkAZAwGOKdNSf3NlPtNf48yD4aqguhrsJOFnz+AjuM93Csng/r3ji8cyiluo0mjhDicbu4deZIHrv0KNblVzDviaVU1nXzdrIizbWO1okjYRBc/QH8x6dwzYdw0b+gYpfdgOpQtrU1Bj65385JefU/Dj8BKaW6hSaOEPS9Men89aIj+WZXOVc8uYzq+qbuvcC4cyF5hO0f6cigaXDWn+zckXd/0bVrGAMf3A2f/BeMPA0aqmHRI4ces1Kq22jiCFGzxmXw8IWT+XpnGWf/5XOWbevGv9ZHngo3LbeTAg9m8sVw7H/C0sfhtRugqqBz13j/Lvjiz3D01TD3BTsabMljWutQqhfQzvEQdsaETBIiPfzs5dVc8NgiLjtmMMcNT+HbvArW764g2hvGiSNTOHFEKikxAZy09/3f2qG/ix6Bdf+Gk39u538Yn61ZZB25/74jOxbDl3+x2+Se4cwjOeln8O1rdjmVGb8KXKxKqYPS4bj9QHV9Ew+8u4GnF23DGPt7OCclmrKaRkqqGwA4cUQKd589luFpsYELpGgTvH3HgUuTDD0ZLn7JJg9fEzx+kl3y5D+Xgje6+bgXL4Utn8CtqyEyMXBxKtVPdXY4riaOfmTTnkoq65sYnRFLlDcMv9+wNq+Cj9YX8MQXW6mub+LqE4dy8/eGE+UNUGXUGMhdbte8EjfsWWOH8B51BZz1kN1L/Z2fwwX/C2N/sP9nd38Dfz/eNn19/97mfUiUUt1CE4cmji4pqqrnD2+v56UVucRFhHHK6DRmjknnpFGp7c8H6S7v3w1f/AlO+DEs/R8YOBUuebntZVNevhrW/AuSh9sEMvHCrg8pVkq1SROHJo5DsmJ7Cf+3dCcfrS+guLqBMJcwZUgiM0an8b0x6QxL7USHeFf5/fCvy2z/h9sLNyxuf3l2XxOsW2A7zvNX2gRy5XsQndz9cSnVz2ji0MRxWHx+w9c7SvlwfQEfry9g/e5KAEamx3DGhEyOGpzIht2VrMotp6ymgbvOGsvI9MPoH2mosbWJoSfDtGsPfrwxsOk92++RPQUufc0u4gjQWGd3Qdy+yK6pVbTR9olEpzh9IwLGb/tUxvwAJvxIay1KoYlDE0c3yyur5f1v9/DmmnyWbSth7382mfER1DX68PkN/3PZFKYN7eG//Ne8BC9fBZMugdl/tYssvvVTuyOhO9wmlfTxUF9hZ7TXlDhLzruhptguJx+VbPtYTvwJeKN6Nn6lehFNHJo4Aqagoo71uysZnRFLWlwEO0tquPzJpewsqeWhOZM484jmRRaNsR3wi7cUM2tcBgOTAvCL+eP/gk/vh8xJTvPVCJh1n629dLQ3iDF2ifglj8H6N+3w37Me7P74lOojNHFo4uhRZTUNXP30cpZvLyU9LpwJWQkMSori040FfFdYDUB8pIeHL5zMSSNTu/fixthmrvVvwPSfwnE3d30zqXd+AYsfgSvetotAKtUPaeLQxNHj6hp9zF++k693lLE6t4xtxTVMGZzI7ElZTMiK5/aXVrFhTyU/njmSq08cSqTX3X0X9/uhsRrCD7GfpaHabmTl9sJ1X9g+j/JddsmTYTPsAo7tyV8FnmhIGd7+MXv/P+vMXu9KBYkmDk0cQef3G1yu5l+UtQ0+fvHqGl79ehcACVEeMuIi8BtDWU0jFXWNTBmcxB2njWZCdnzPB7z5Q3j2XNvXkTIK3rod6svte0fOg9PvB09k8/E7l9nE8t1Hts/kuJvsrPiWx5RsgVUv2h0X68ph+u0w9drmjvzK3XYr3uLN9lFTbGfVD58JAybrXBXVozRxaOLolYwxfLyhgHX5leSX17K7vA63S0iI9BLpdbNgVR4l1Q2cPXEAPzoqm4z4CNJiw4mP9CA98df6q9c1b6s78Bj4wV9g1fPw+UOQNg5GzrJb5pZ8B3lf2471426G4k12k6ukoTD+fCjaAHu+teWIsyuiwJaPITHHjuTa8gnkLnUuLHa5+vBYu7Uvxp579Jkw7hwYMt1uzVu4AQrWQv5q259TtMlOlDztD4de21LKoYlDE0efVFnXyOMLt/CPz7bu29kQbAtPtDeM6HA3Q5Kj+eHkLM6YkEl8ZDdPTqwpgfmXwYjv2wmGLqc5bdMH8Np1dofDuCy7hPzw78HR1zQv9rjlE/j3LVC6HZJyIG2s3Z9kwo8gPsses/kDePdXULgOMifCmLNhxCxIGdk8JLi6CL77GDa+Yx8NVeCJsoljr/A4+/mYdPjmZUgcAuf9w44iU+oQaeLQxNGnlVY3sKmgij0VdeypqKO8tpHqeh/V9U0s317Cd4XVeMNcTB+RypQhiUwemMAR2Qnd22/Sms9Znt7dwXIsfh/4GvZvrmrrmLpyiEo6+DUba20T2tZPITYDUsdA2mhIGNLcjLXtC7tfSUWerX0Mn2n7ZeIGtH0Pxt/cVHYwq16EFU/C3OcPjLeuAiLiOnce1Sdo4tDEEbKMMazOLeeVr3L5ZGMh24vtX+Ix4WFcduxgrjohh+RArvbbG9WWwUf32tn3VXts2bAZdmXijAm2c37tK7a2E5cJV7xz8OSxcxk8dYZNhEfMhXMfa35v8aPw3q/govm25qVCgiYOTRz9RnFVPV/vKOPVlbt4a00+EWFuZo1Lp7rBR0FlPfWNPqaPTGXWuAwmD0zYr8M+5BgDe9bChrdh0V9tzWbSRVCea2stSUNth/30n8GMX7Z/nsrd8NhJtvls5Gl28cmL5ts+nu1fwlNn2WXxk4bC9Yt05n2I0MShiaNf2lxQySMff8dnm4pIjvaSFheOMbBkazGNPkNGXASXHjuYS6YNJj6quX+kvLYRYwwetwtvmAuPOwRGM9WWwsI/2k20PJEw4y47yfH1/4TVL8JV77XdJ9LUAE+fBbvX2K2Ak0c0L3V/2ev2PW8MzLzb9ged8is46fbA3svGd+0+LePPtSsB6LDmgNDEoYlDtVBR18jH6wt4aUUun20qItrr5uyJAyipbmDNrnLyy+v2O35sZhynjE7l5FFpTB6YQFhfTiTVxXZdrr39EXXl8LfjbC3hPz5rXmalutiOKFvxlB0Ndv6T9hc1wK4V8I+ZdhkXEbj6Q0gfC/Pn2Q78G5fYDvpA2PQBvDAH/E4fU9pYmHyp3R2ys301qlM0cWjiUO34Nq+Cxxd+x1trdpOdFMmErHjGZsbhDXPR6PNTVdfE4q0lrNheis9viI0I47hhyZwwIpWc5Giiwt1Ee8PISowkJnz/jvKymgY8bhfR4b18c80tn8Azs2HAkXazrNoyO4TY12D3kj/2BjsMuKX3f21XJT7vnzDhfFtWvgv+erQdbnzhC12Po77S9suIC3JOsv0vLe1YDM/80E6uvOAZO1Bg9Xw7jDl5uJ1bM3zmIf0TqANp4tDEoQ7CGNPh3JDy2kY+31TEZ5sK+WxTEbvKavd7XwRykqMZlxWPz+9ndW45uaW1xEaEcdOM4cw7bgjhYW2P8mo9OTIoPv+TrWFEJNgRU0lDYdLFtibRFr8fSrceuOT953+CD+62v/inXmv7RESgZKud7zJw6oE7NuZ9Dcv+Ad+8amf875Uyym4lHJ8NUSl2HbKYVNuZH9NiqZpNH8DbP7PnH30WnPWn/d9Xh0QThyYO1Y2MMewoqaGgsp6q+iaq65vYWljNN3nlfLOrArdLmJAdz/gB8SzdWszHGwoZmBTJvGOHkBTtJSY8jPLaRhZvKWHxlmKKquo5cUQqs8alM3NMOonRfbjJxdcEXz5sE0HFLvsLv7Gmed5JVAqc+juYONeO+PrgHpuwPNEw4TyYfJldW2zLJ7YDv3CDHVpsfBCXDVe+befNtNZUbwcAfHI/RMTDOY92vfZRucdOtOxoiHU/oolDE4cKos82FXLfm+v27WOyV2KUh2OGJpMU7eWj9QXkl9fhEjhqcCIzRqdz3LBkvGEufH6D2yUMTY1ut9bS6/iabH/H2lchJg3Sx9kJip/eD7nL7BIqRZvBV2+XZzn+1vbngfiaoGq3/aXe0ZwYsFsKv3wVFK63nf8jTrX9IAmD2u5E9zXa1ZCXPg7bv7BJZ/hMOxEzPgtcYfaROipws/H9Plvrqthlk5evHiZfcmDNrIdp4tDEoYLMGENxdQPV9U1U1jURHuZiWGrMviYqYwxrdpXz/rd7+Gh9AWvzKg44h8ctjM2MY9LABGaMsYnF43bh9xuWby/lw3V7GJURyxkTMonw9NIE4/fDymfh0wdsMpl1X/s7PB6qxlp47y5b68H5neaNscu7JA62kyFrSmxNpmgj1BTZxDLpEijfARvfg+qC/c/pDrdzYcacbSdN7lxs57bEZcIJt9mmuUMZ3VW6DV69HnZ8uX957AD44SP2mkGiiUMTh+pjdpfXsXJnKcaAyyU0NPlZm1fByp2lrM4tp6bBR0KUh+OHp7ByRxm7ymoRsVM3EqI8nH9kNrPGZzAhK/6AJNLk8/NtfgVLt5YQ4XFz+viM0JwkWVdhax571tqfJVttv0xFvt1eOC7L9p+MO8fWTPYuKeP3w55voK7Mjt5qrLN7tXy7ACpy7TGRiXbgQP4qWxvKmgLDTrFzXip325rLxAtt2d7zVuTZOTSeKDsIYetCePcXgMD377Hni82A8p12nbSijXZBzbQxdsXmxho7Cq62zG5GljjE1twyJ9kE5o21zWx1Fc5Cmd/Z+zzE5KOJQxOHCiF1jT4WbizkrTX5fL65iHED4vnh5AF8f2wGq3eW8dySHby7djdNfrOvlpIY7aWu0Udto5/vCqqoqm/adz63S5g+IoUpQ+wyIk0+Q01jE0WVDRRW1dPQ5GNwUjSDU6IYmBhFcrSXhCgv6XHhoZlw2mMM7F5tax8pI+0yL411sPI5+OJPULbTNsvFZtjntSW25pB5hE0wlfkHnnPIifDDvx3Yb9NYCx/ea/eF2UtcNiFFJNg10Uq22rXLWnKH26auvUadCRc+f0i3G/TEISIRwEIgHAgDXjLG3N3i/b8AVxhjYpzX4cAzwFFAMTDHGLPNee9O4CrAB9xsjHm3o2tr4lD9UWl1A8u3l/LVjlK+3lFKbYOP8DA34R4XA5OimJaTxLScZMpqG3h9ZR4LVubtN1LM63aREuMlNTYct0vYUVJLUVX9AdeZmpPEuZOzOD0Qi0z2JcbYvoq9HetN9baP5+vnbHNU5kTIOsqOVmuqszUIb7QdBdbRcvm1ZYCxtRS3d//mML/P1izyVtrmtoZqO6Q5KskOT04eYRfY7OpGZo7ekDgEiDbGVImIB/gcuMUYs1hEpgC3AOe0SBw3AEcYY64TkbnOe3NEZCzwAjAVGAB8AIw0xvjaui5o4lCqM4wx1DX6cbsEt0twCQcMT66qb2JXaS2lNQ2U1TSwaU8Vr67cxZbCasKczvsR6bEMT40hPtJDpNeNx+0ir6yWrUXVbC+uJszlIiYijLiIMAYmRTE8LYYRabGkxHjxhrkID3MT4XH1zLL5qkNBTxytgonCJo7rgeXYX/4XAZtaJI53gXuMMYtEJAzYDaQCPwcwxvy+9XHtXU8Th1KBs3eRyfe+3c2G3ZVs3FPFztIaWv8qGRAfwaBkOyu9sq6JirpG8srq8PkP/J2TkxLN+Udlc87kLAYk7D+KqqKuka93lAFw/LDkvj2Lv5frbOII6OBlEXEDK4DhwCPGmCUicguwwBiT3+ovjCxgJ4AxpklEyoFkp3xxi+NynTKlVBCICBMHJjBxYMK+soYmPzUNTdQ2+qhv9JMeF9HmEvf1TT62FlWzaU8VFXWN1Df6qXX6bx54dwN/fG8DQ5KjiYsIIy7SQ2FlPRv2VO5LSpnxEcw5eiAnjUylqKqB3eW1VDf4GJAQSXaifaREh+8budbk87O5sIrNBVWEuYRIbxgx4W7GZMYR5dW5G4cqoP9yTnPSJBFJAF4VkenAj4CT2zi8rXqq6aB8/w+LXAtcCzBoUBuThZRSAeMNc+EN85JwkOPCw9yMzohjdMb+8zduPGU4O0tqeOWrXWwqqNxXQ0mNDee08RlMGZxEVX0Tzy/dwZ8+2MSfPtjUYSwD4iOIiQhjc0EVdY3+A47xuIWJ2QkcMzSZQclRpMR4SY4Op8Hnt9sY1zYyKDmKidkJeMNsDaewsp4vvyvC7RKm5iSRFtt/VwTukZRrjCkTkU+AU7C1j81ObSNKRDYbY4ZjaxIDgVynqSoeKGlRvlc2kNfGNR4HHgfbVBW4u1FKBcLApChumTmiw2NOG5/BjuIa1u2uICMugsz4CKLCw8grqyW3tIadJbXkldeSV1ZHWU0DF00dzITsOEal20RV29hEaXUjy7eXsmhLMX/7ZDNttJztE+V1c9TgREqqGw6YZzMsNZqclJh9fddet4u4SA8JUR4GxEcwNSeZEWl23k55bSPLt5WwqaAKtwhhbiE2wsOxw5LJSjjIBMdeKGCJQ0RSgUYnaUQCM4H7jTEZLY6pcpIGwAJgHrAIOB/4yBhjRGQB8LyIPIjtHB8BLEUp1S8NSo7a13ey18j0WEamd36W98yx6YAd5lxYWU9hVT0lVQ2Ee1wkRHqJiQhjw+5KFn1XxJKtJcRFeLh91iimj0jFZwxLthSzeEvxfqPS6pt8VNQ2Ul7bSKPPZqPEKA/pcRH7Nbe1Nio9lmOHJRPmEuqb/DT5/cRHekmO9pIU7SUhykNshIfYiDB8fkN9k20OTI4JZ1BS1AFNggdbg607BHJU1RHA04AbcAHzjTG/bXVMVYvO8Qjgf4HJ2JrGXGPMFue9XwJXAk3ArcaYtzu6tnaOK6WCxRhDbmkti7cUs3hLCQWVdRw1OJFpOcmMz7I1nyafobCqnoUbC/l4QwErtpfiEiE8zIXb5aKitpEG34FNbG1JjwvHG+aitsFHdb2P08Zn8NCcSYcUe68aVdXTNHEopfoyYwyV9U2UVDVQXtu4r8/H7RIiPG48bqGwsp7txTVsL67BbwxRXjdRXjfjs+KZPenQxg/1ilFVSimluk5EiIvwEBfROydY6oBopZRSXaKJQymlVJdo4lBKKdUlmjiUUkp1iSYOpZRSXaKJQymlVJdo4lBKKdUlmjiUUkp1SUjOHBeRQmD7YZwiBSjqpnD6iv54z9A/71vvuf/o6n0PNsakHuygkEwch0tElndm2n0o6Y/3DP3zvvWe+49A3bc2VSmllOoSTRxKKaW6RBNH2x4PdgBB0B/vGfrnfes99x8BuW/t41BKKdUlWuNQSinVJZo4WhCR00Rkg4hsFpGfBzueQBCRgSLysYisE5G1InKLU54kIu+LyCbnZ2KwYw0EEXGLyNci8obzOkdEljj3/aKIeIMdY3cSkQQReUlE1jvf+bH94bsWkduc/76/EZEXRCQiFL9rEXlCRApE5JsWZW1+v2I97Px+Wy0iRx7qdTVxOETEDTwCnA6MBS4UkbHBjSogmoCfGGPGAMcANzr3+XPgQ2PMCOBD53UougVY1+L1/cBDzn2XAlcFJarA+TPwjjFmNDARe+8h/V2LSBZwMzDFGDMeu331XELzu34KOK1VWXvf7+nACOdxLfDooV5UE0ezqcBmY8wWY0wD8H/A7CDH1O2MMfnGmK+c55XYXyRZ2Ht92jnsaeCHwYkwcEQkGzgT+IfzWoAZwEvOISF13yISB0wH/glgjGkwxpTRD75r7O6mkSISBkQB+YTgd22MWQiUtCpu7/udDTxjrMVAgohkHsp1NXE0ywJ2tnid65SFLBEZAkwGlgDpxph8sMkFSAteZAHzJ+BngN95nQyUGWOanNeh9p0PBQqBJ53muX+ISDQh/l0bY3YBfwR2YBNGObCC0P6uW2rv++2233GaOJpJG2UhO+RMRGKAl4FbjTEVwY4n0ETkLKDAGLOiZXEbh4bSdx4GHAk8aoyZDFQTYs1SbXHa9GcDOcAAIBrbTNNaKH3XndFt/71r4miWCwxs8TobyAtSLAElIh5s0njOGPOKU7xnb7XV+VkQrPgC5HjgByKyDdsMOQNbA0lwmjMg9L7zXCDXGLPEef0SNpGE+nc9E9hqjCk0xjQCrwDHEdrfdUvtfb/d9jtOE0ezZcAIZ+SFF9uZtiDIMXU7p13/n8A6Y8yDLd5aAMxzns8DXu/p2ALJGHOnMSbbGDME+91+ZIy5GPgYON85LKTu2xizG9gpIqOcou8B3xLi3zW2ieoYEYly/nvfe98h+1230t73uwC4zBlddQxQvrdJq6t0AmALInIG9q9QN/CEMea+IIfU7UTkBOAzYA3Nbf2/wPZzzAcGYf/H+5ExpnWnW0gQkZOBnxpjzhKRodgaSBLwNXCJMaY+mPF1JxGZhB0M4AW2AFdg/2AM6e9aRH4DzMGOIvwauBrbnh9S37WIvACcjF0Fdw9wN/AabXy/ThL9K3YUVg1whTFm+SFdVxOHUkqprtCmKqWUUl2iiUMppVSXaOJQSinVJZo4lFJKdYkmDqWUUl2iiUOpLhARn4isbPHotpnYIjKk5SqnSvVWYQc/RCnVQq0xZlKwg1AqmLTGoVQ3EJFtInK/iCx1HsOd8sEi8qGz/8GHIjLIKU8XkVdFZJXzOM45lVtE/sfZS+I9EYl0jr9ZRL51zvN/QbpNpQBNHEp1VWSrpqo5Ld6rMMZMxc7O/ZNT9lfsUtZHAM8BDzvlDwOfGmMmYtePWuuUjwAeMcaMA8qA85zynwOTnfNcF6ibU6ozdOa4Ul0gIlXGmJg2yrcBM4wxW5xFJHcbY5JFpAjINMY0OuX5xpgUESkEslsueeEsc/++swEPInIH4DHG/E5E3gGqsMtJvGaMqQrwrSrVLq1xKNV9TDvP2zumLS3XTvLR3A95JnaHyqOAFS1WeVWqx2niUKr7zGnxc5Hz/EvsarwAFwOfO88/BK6Hffugx7V3UhFxAQONMR9jN6JKAA6o9SjVU/SvFqW6JlJEVrZ4/Y4xZu+Q3HARWYL9g+xCp+xm4AkRuR27G98VTvktwOMichW2ZnE9dre6triBZ0UkHrsZz0POFrBKBYX2cSjVDZw+jinGmKJgx6JUoGlTlVJKqS7RGodSSqku0RqHUkqpLtHEoZRSqks0cSillOoSTRxKKaW6RBOHUkqpLtHEoZRSqkv+PxXwTSxK/lApAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25c9facf7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
