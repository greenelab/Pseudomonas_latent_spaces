{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee\n",
    "# (updated October 2018)\n",
    "# \n",
    "# Main\n",
    "#\n",
    "# Dataset: Pseudomonas aeruginosa gene expression from compendium \n",
    "# referenced in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5069748/\n",
    "# \n",
    "# Condition: expression of PA1673 gene\n",
    "#\n",
    "# Task: To predict the expression of other (non-PA1673) genes by:\n",
    "#        1. Define offset vector = avg(expression of genes corresponding to high levels of PA1673) \n",
    "#           - avg(expression of genes corresponding to low levels of PA1673)\n",
    "#        2. scale factor = how far along the gradient of low-high PA1673 expression\n",
    "#        3. prediction = baseline expression + scale factor * offset vector \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functions import generate_input, vae, def_offset, interpolate, plot\n",
    "\n",
    "randomState = 123\n",
    "from numpy.random import seed\n",
    "seed(randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists: /home/alexandra/Documents/Repos/Pseudomonas_latent_spaces/data/PA1673_gradient_test\n",
      "directory already exists: /home/alexandra/Documents/Repos/Pseudomonas_latent_spaces/encoded/PA1673_gradient_test\n",
      "directory already exists: /home/alexandra/Documents/Repos/Pseudomonas_latent_spaces/models/PA1673_gradient_test\n",
      "directory already exists: /home/alexandra/Documents/Repos/Pseudomonas_latent_spaces/output/PA1673_gradient_test\n",
      "directory already exists: /home/alexandra/Documents/Repos/Pseudomonas_latent_spaces/stats/PA1673_gradient_test\n",
      "directory already exists: /home/alexandra/Documents/Repos/Pseudomonas_latent_spaces/viz/PA1673_gradient_test\n"
     ]
    }
   ],
   "source": [
    "# Name of analysis\n",
    "analysis_name = 'PA1673_gradient_test'\n",
    "\n",
    "# Create list of base directories\n",
    "base_dirs = [os.path.join(os.path.dirname(os.getcwd()), 'data'),\n",
    "             os.path.join(os.path.dirname(os.getcwd()), 'encoded'),\n",
    "             os.path.join(os.path.dirname(os.getcwd()), 'models'),\n",
    "             os.path.join(os.path.dirname(os.getcwd()), 'output'),\n",
    "             os.path.join(os.path.dirname(os.getcwd()), 'stats'),\n",
    "             os.path.join(os.path.dirname(os.getcwd()), 'viz')    \n",
    "]\n",
    "\n",
    "# Check if directory exist otherwise create\n",
    "for each_dir in base_dirs:\n",
    "    analysis_dir = os.path.join(each_dir, analysis_name)\n",
    "    if os.path.exists(analysis_dir):\n",
    "        print('directory already exists: {}'.format(analysis_dir))\n",
    "    else:\n",
    "        os.mkdir(analysis_dir)\n",
    "        print('creating new directory: {}'.format(analysis_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/Documents/Repos/Pseudomonas_latent_spaces/scripts/functions/vae.py:259: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n",
      "  vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1072 samples, validate on 119 samples\n",
      "Epoch 1/200\n",
      "1072/1072 [==============================] - 1s 1ms/step - loss: 3755.4063 - val_loss: 3626.1111\n",
      "Epoch 2/200\n",
      "1072/1072 [==============================] - 1s 855us/step - loss: 3590.7111 - val_loss: 3595.5127\n",
      "Epoch 3/200\n",
      "1072/1072 [==============================] - 1s 836us/step - loss: 3525.7637 - val_loss: 3616.1362\n",
      "Epoch 4/200\n",
      "1072/1072 [==============================] - 1s 837us/step - loss: 3498.3181 - val_loss: 3639.6006\n",
      "Epoch 5/200\n",
      "1072/1072 [==============================] - 1s 847us/step - loss: 3486.8382 - val_loss: 3590.9883\n",
      "Epoch 6/200\n",
      "1072/1072 [==============================] - 1s 828us/step - loss: 3476.3841 - val_loss: 3565.4029\n",
      "Epoch 7/200\n",
      "1072/1072 [==============================] - 1s 956us/step - loss: 3470.9305 - val_loss: 3535.8960\n",
      "Epoch 8/200\n",
      "1072/1072 [==============================] - 1s 829us/step - loss: 3468.5891 - val_loss: 3506.7823\n",
      "Epoch 9/200\n",
      "1072/1072 [==============================] - 1s 833us/step - loss: 3466.2634 - val_loss: 3503.3706\n",
      "Epoch 10/200\n",
      "1072/1072 [==============================] - 1s 821us/step - loss: 3462.7193 - val_loss: 3488.5911\n",
      "Epoch 11/200\n",
      "1072/1072 [==============================] - 1s 839us/step - loss: 3461.1343 - val_loss: 3499.1896\n",
      "Epoch 12/200\n",
      "1072/1072 [==============================] - 1s 833us/step - loss: 3457.0647 - val_loss: 3491.8416\n",
      "Epoch 13/200\n",
      "1072/1072 [==============================] - 1s 820us/step - loss: 3451.8256 - val_loss: 3501.1078\n",
      "Epoch 14/200\n",
      "1072/1072 [==============================] - 1s 826us/step - loss: 3454.4251 - val_loss: 3484.3983\n",
      "Epoch 15/200\n",
      "1072/1072 [==============================] - 1s 829us/step - loss: 3450.0458 - val_loss: 3484.1401\n",
      "Epoch 16/200\n",
      "1072/1072 [==============================] - 1s 862us/step - loss: 3448.6765 - val_loss: 3485.5857\n",
      "Epoch 17/200\n",
      "1072/1072 [==============================] - 1s 865us/step - loss: 3447.5358 - val_loss: 3485.6474\n",
      "Epoch 18/200\n",
      "1072/1072 [==============================] - 1s 847us/step - loss: 3444.7536 - val_loss: 3494.4406\n",
      "Epoch 19/200\n",
      "1072/1072 [==============================] - 1s 844us/step - loss: 3442.7461 - val_loss: 3473.9055\n",
      "Epoch 20/200\n",
      "1072/1072 [==============================] - 1s 835us/step - loss: 3441.6342 - val_loss: 3461.7969\n",
      "Epoch 21/200\n",
      "1072/1072 [==============================] - 1s 841us/step - loss: 3440.3161 - val_loss: 3459.6949\n",
      "Epoch 22/200\n",
      "1072/1072 [==============================] - 1s 814us/step - loss: 3439.2970 - val_loss: 3458.7326\n",
      "Epoch 23/200\n",
      "1072/1072 [==============================] - 1s 800us/step - loss: 3436.5441 - val_loss: 3458.1415\n",
      "Epoch 24/200\n",
      "1072/1072 [==============================] - 1s 837us/step - loss: 3434.9605 - val_loss: 3454.2909\n",
      "Epoch 25/200\n",
      "1072/1072 [==============================] - 1s 839us/step - loss: 3433.9181 - val_loss: 3454.1952\n",
      "Epoch 26/200\n",
      "1072/1072 [==============================] - 1s 837us/step - loss: 3430.3715 - val_loss: 3455.0300\n",
      "Epoch 27/200\n",
      "1072/1072 [==============================] - 1s 815us/step - loss: 3428.9055 - val_loss: 3451.6681\n",
      "Epoch 28/200\n",
      "1072/1072 [==============================] - 1s 833us/step - loss: 3428.2011 - val_loss: 3452.2426\n",
      "Epoch 29/200\n",
      "1072/1072 [==============================] - 1s 849us/step - loss: 3426.4495 - val_loss: 3448.8890\n",
      "Epoch 30/200\n",
      "1072/1072 [==============================] - 1s 826us/step - loss: 3424.7899 - val_loss: 3451.0207\n",
      "Epoch 31/200\n",
      "1072/1072 [==============================] - 1s 817us/step - loss: 3423.4962 - val_loss: 3449.8068\n",
      "Epoch 32/200\n",
      "1072/1072 [==============================] - 1s 819us/step - loss: 3420.6679 - val_loss: 3444.9147\n",
      "Epoch 33/200\n",
      "1072/1072 [==============================] - 1s 824us/step - loss: 3420.9806 - val_loss: 3438.1932\n",
      "Epoch 34/200\n",
      "1072/1072 [==============================] - 1s 832us/step - loss: 3420.4748 - val_loss: 3439.0204\n",
      "Epoch 35/200\n",
      "1072/1072 [==============================] - 1s 813us/step - loss: 3417.7733 - val_loss: 3434.7779\n",
      "Epoch 36/200\n",
      "1072/1072 [==============================] - 1s 836us/step - loss: 3416.5510 - val_loss: 3432.5033\n",
      "Epoch 37/200\n",
      "1072/1072 [==============================] - 1s 842us/step - loss: 3416.9445 - val_loss: 3438.4401\n",
      "Epoch 38/200\n",
      "1072/1072 [==============================] - 1s 850us/step - loss: 3412.2185 - val_loss: 3437.9845\n",
      "Epoch 39/200\n",
      "1072/1072 [==============================] - 1s 844us/step - loss: 3413.8122 - val_loss: 3437.0722\n",
      "Epoch 40/200\n",
      "1072/1072 [==============================] - 1s 842us/step - loss: 3411.0052 - val_loss: 3432.2147\n",
      "Epoch 41/200\n",
      "1072/1072 [==============================] - 1s 862us/step - loss: 3411.4127 - val_loss: 3430.1807\n",
      "Epoch 42/200\n",
      "1072/1072 [==============================] - 1s 857us/step - loss: 3408.1952 - val_loss: 3426.4693\n",
      "Epoch 43/200\n",
      "1072/1072 [==============================] - 1s 832us/step - loss: 3409.2662 - val_loss: 3426.9325\n",
      "Epoch 44/200\n",
      "1072/1072 [==============================] - 1s 819us/step - loss: 3406.5381 - val_loss: 3422.6215\n",
      "Epoch 45/200\n",
      "1072/1072 [==============================] - 1s 824us/step - loss: 3406.2416 - val_loss: 3421.8721\n",
      "Epoch 46/200\n",
      "1072/1072 [==============================] - 1s 814us/step - loss: 3406.0040 - val_loss: 3423.2707\n",
      "Epoch 47/200\n",
      "1072/1072 [==============================] - 1s 834us/step - loss: 3404.6193 - val_loss: 3421.6877\n",
      "Epoch 48/200\n",
      "1072/1072 [==============================] - 1s 834us/step - loss: 3402.2862 - val_loss: 3421.6263\n",
      "Epoch 49/200\n",
      "1072/1072 [==============================] - 1s 833us/step - loss: 3401.9176 - val_loss: 3427.0152\n",
      "Epoch 50/200\n",
      "1072/1072 [==============================] - 1s 834us/step - loss: 3401.4323 - val_loss: 3416.1301\n",
      "Epoch 51/200\n",
      "1072/1072 [==============================] - 1s 877us/step - loss: 3398.7238 - val_loss: 3415.2195\n",
      "Epoch 52/200\n",
      "1072/1072 [==============================] - 1s 927us/step - loss: 3398.8461 - val_loss: 3417.7367\n",
      "Epoch 53/200\n",
      "1072/1072 [==============================] - 1s 947us/step - loss: 3398.4872 - val_loss: 3417.5871\n",
      "Epoch 54/200\n",
      "1072/1072 [==============================] - 1s 893us/step - loss: 3397.2272 - val_loss: 3419.4239\n",
      "Epoch 55/200\n",
      "1072/1072 [==============================] - 1s 797us/step - loss: 3398.9820 - val_loss: 3421.1255\n",
      "Epoch 56/200\n",
      "1072/1072 [==============================] - 1s 809us/step - loss: 3396.1690 - val_loss: 3415.6100\n",
      "Epoch 57/200\n",
      "1072/1072 [==============================] - 1s 943us/step - loss: 3396.4744 - val_loss: 3410.0819\n",
      "Epoch 58/200\n",
      "1072/1072 [==============================] - 1s 1ms/step - loss: 3395.1195 - val_loss: 3412.4501\n",
      "Epoch 59/200\n",
      "1072/1072 [==============================] - 1s 942us/step - loss: 3395.1925 - val_loss: 3410.1150\n",
      "Epoch 60/200\n",
      "1072/1072 [==============================] - 1s 792us/step - loss: 3393.2422 - val_loss: 3412.9762\n",
      "Epoch 61/200\n",
      "1072/1072 [==============================] - 1s 884us/step - loss: 3393.0358 - val_loss: 3408.8418\n",
      "Epoch 62/200\n",
      "1072/1072 [==============================] - 1s 846us/step - loss: 3391.4525 - val_loss: 3409.4242\n",
      "Epoch 63/200\n",
      "1072/1072 [==============================] - 1s 880us/step - loss: 3392.7443 - val_loss: 3406.6022\n",
      "Epoch 64/200\n",
      "1072/1072 [==============================] - 1s 786us/step - loss: 3390.8894 - val_loss: 3405.4096\n",
      "Epoch 65/200\n",
      "1072/1072 [==============================] - 1s 797us/step - loss: 3390.1553 - val_loss: 3412.8571\n",
      "Epoch 66/200\n",
      "1072/1072 [==============================] - 1s 796us/step - loss: 3388.6244 - val_loss: 3410.0780\n",
      "Epoch 67/200\n",
      "1072/1072 [==============================] - 1s 789us/step - loss: 3389.7376 - val_loss: 3404.4072\n",
      "Epoch 68/200\n",
      "1072/1072 [==============================] - 1s 786us/step - loss: 3387.8373 - val_loss: 3403.8871\n",
      "Epoch 69/200\n",
      "1072/1072 [==============================] - 1s 803us/step - loss: 3385.8272 - val_loss: 3405.4295\n",
      "Epoch 70/200\n",
      "1072/1072 [==============================] - 1s 784us/step - loss: 3387.2111 - val_loss: 3399.6366\n",
      "Epoch 71/200\n",
      "1072/1072 [==============================] - 1s 790us/step - loss: 3386.4002 - val_loss: 3402.9237\n",
      "Epoch 72/200\n",
      "1072/1072 [==============================] - 1s 806us/step - loss: 3383.7242 - val_loss: 3402.6934\n",
      "Epoch 73/200\n",
      "1072/1072 [==============================] - 1s 839us/step - loss: 3384.4719 - val_loss: 3403.8637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200\n",
      "1072/1072 [==============================] - 1s 942us/step - loss: 3382.6542 - val_loss: 3402.8215\n",
      "Epoch 75/200\n",
      "1072/1072 [==============================] - 1s 831us/step - loss: 3384.4238 - val_loss: 3407.7270\n",
      "Epoch 76/200\n",
      "1072/1072 [==============================] - 1s 804us/step - loss: 3381.9606 - val_loss: 3401.2578\n",
      "Epoch 77/200\n",
      "1072/1072 [==============================] - 1s 795us/step - loss: 3381.2134 - val_loss: 3404.5175\n",
      "Epoch 78/200\n",
      "1072/1072 [==============================] - 1s 793us/step - loss: 3381.9351 - val_loss: 3401.4558\n",
      "Epoch 79/200\n",
      "1072/1072 [==============================] - 1s 807us/step - loss: 3381.6267 - val_loss: 3400.0814\n",
      "Epoch 80/200\n",
      "1072/1072 [==============================] - 1s 793us/step - loss: 3381.1797 - val_loss: 3397.2899\n",
      "Epoch 81/200\n",
      "1072/1072 [==============================] - 1s 797us/step - loss: 3378.8547 - val_loss: 3396.8396\n",
      "Epoch 82/200\n",
      "1072/1072 [==============================] - 1s 782us/step - loss: 3379.4999 - val_loss: 3396.3839\n",
      "Epoch 83/200\n",
      "1072/1072 [==============================] - 1s 790us/step - loss: 3379.3964 - val_loss: 3396.4444\n",
      "Epoch 84/200\n",
      "1072/1072 [==============================] - 1s 811us/step - loss: 3379.8747 - val_loss: 3395.0447\n",
      "Epoch 85/200\n",
      "1072/1072 [==============================] - 1s 816us/step - loss: 3378.8776 - val_loss: 3395.0238\n",
      "Epoch 86/200\n",
      "1072/1072 [==============================] - 1s 793us/step - loss: 3376.2809 - val_loss: 3394.7272\n",
      "Epoch 87/200\n",
      "1072/1072 [==============================] - 1s 803us/step - loss: 3378.5289 - val_loss: 3393.5445\n",
      "Epoch 88/200\n",
      "1072/1072 [==============================] - 1s 958us/step - loss: 3376.1746 - val_loss: 3395.2583\n",
      "Epoch 89/200\n",
      "1072/1072 [==============================] - 1s 786us/step - loss: 3376.0331 - val_loss: 3392.8728\n",
      "Epoch 90/200\n",
      "1072/1072 [==============================] - 1s 789us/step - loss: 3374.6227 - val_loss: 3391.0841\n",
      "Epoch 91/200\n",
      "1072/1072 [==============================] - 1s 813us/step - loss: 3373.2110 - val_loss: 3393.6070\n",
      "Epoch 92/200\n",
      "1072/1072 [==============================] - 1s 869us/step - loss: 3375.0865 - val_loss: 3390.5997\n",
      "Epoch 93/200\n",
      "1072/1072 [==============================] - 1s 797us/step - loss: 3373.6235 - val_loss: 3390.1682\n",
      "Epoch 94/200\n",
      "1072/1072 [==============================] - 1s 800us/step - loss: 3371.8903 - val_loss: 3392.0559\n",
      "Epoch 95/200\n",
      "1072/1072 [==============================] - 1s 831us/step - loss: 3372.4981 - val_loss: 3390.5456\n",
      "Epoch 96/200\n",
      "1072/1072 [==============================] - 1s 799us/step - loss: 3371.0545 - val_loss: 3393.8516\n",
      "Epoch 97/200\n",
      "1072/1072 [==============================] - 1s 802us/step - loss: 3370.2222 - val_loss: 3388.0561\n",
      "Epoch 98/200\n",
      "1072/1072 [==============================] - 1s 843us/step - loss: 3371.8956 - val_loss: 3391.2378\n",
      "Epoch 99/200\n",
      "1072/1072 [==============================] - 1s 830us/step - loss: 3372.2173 - val_loss: 3392.3017\n",
      "Epoch 100/200\n",
      "1072/1072 [==============================] - 1s 796us/step - loss: 3371.3414 - val_loss: 3393.4958\n",
      "Epoch 101/200\n",
      "1072/1072 [==============================] - 1s 806us/step - loss: 3370.5755 - val_loss: 3393.8551\n",
      "Epoch 102/200\n",
      "1072/1072 [==============================] - 1s 808us/step - loss: 3370.8588 - val_loss: 3389.0280\n",
      "Epoch 103/200\n",
      "1072/1072 [==============================] - 1s 792us/step - loss: 3369.6557 - val_loss: 3388.9111\n",
      "Epoch 104/200\n",
      "1072/1072 [==============================] - 1s 808us/step - loss: 3370.3639 - val_loss: 3388.1343\n",
      "Epoch 105/200\n",
      "1072/1072 [==============================] - 1s 798us/step - loss: 3368.6776 - val_loss: 3389.3057\n",
      "Epoch 106/200\n",
      "1072/1072 [==============================] - 1s 796us/step - loss: 3369.5865 - val_loss: 3388.4077\n",
      "Epoch 107/200\n",
      "1072/1072 [==============================] - 1s 843us/step - loss: 3369.7176 - val_loss: 3386.7828\n",
      "Epoch 108/200\n",
      "1072/1072 [==============================] - 1s 805us/step - loss: 3367.1465 - val_loss: 3385.8481\n",
      "Epoch 109/200\n",
      "1072/1072 [==============================] - 1s 802us/step - loss: 3366.0920 - val_loss: 3386.0986\n",
      "Epoch 110/200\n",
      "1072/1072 [==============================] - 1s 806us/step - loss: 3367.2705 - val_loss: 3388.3105\n",
      "Epoch 111/200\n",
      "1072/1072 [==============================] - 1s 811us/step - loss: 3366.6544 - val_loss: 3385.4171\n",
      "Epoch 112/200\n",
      "1072/1072 [==============================] - 1s 791us/step - loss: 3363.9326 - val_loss: 3383.4651\n",
      "Epoch 113/200\n",
      "1072/1072 [==============================] - 1s 790us/step - loss: 3366.9810 - val_loss: 3385.5182\n",
      "Epoch 114/200\n",
      "1072/1072 [==============================] - 1s 790us/step - loss: 3364.3864 - val_loss: 3382.1484\n",
      "Epoch 115/200\n",
      "1072/1072 [==============================] - 1s 785us/step - loss: 3364.2775 - val_loss: 3382.4793\n",
      "Epoch 116/200\n",
      "1072/1072 [==============================] - 1s 795us/step - loss: 3364.1538 - val_loss: 3384.8251\n",
      "Epoch 117/200\n",
      "1072/1072 [==============================] - 1s 784us/step - loss: 3363.5385 - val_loss: 3382.9244\n",
      "Epoch 118/200\n",
      "1072/1072 [==============================] - 1s 804us/step - loss: 3364.2638 - val_loss: 3383.9184\n",
      "Epoch 119/200\n",
      "1072/1072 [==============================] - 1s 884us/step - loss: 3364.3946 - val_loss: 3381.9201\n",
      "Epoch 120/200\n",
      "1072/1072 [==============================] - 1s 868us/step - loss: 3363.5964 - val_loss: 3386.7890\n",
      "Epoch 121/200\n",
      "1072/1072 [==============================] - 1s 832us/step - loss: 3362.1546 - val_loss: 3380.2185\n",
      "Epoch 122/200\n",
      "1072/1072 [==============================] - 1s 838us/step - loss: 3362.9044 - val_loss: 3382.6835\n",
      "Epoch 123/200\n",
      "1072/1072 [==============================] - 1s 821us/step - loss: 3361.8109 - val_loss: 3382.3054\n",
      "Epoch 124/200\n",
      "1072/1072 [==============================] - 1s 813us/step - loss: 3359.9486 - val_loss: 3380.0260\n",
      "Epoch 125/200\n",
      "1072/1072 [==============================] - 1s 810us/step - loss: 3361.2292 - val_loss: 3377.5099\n",
      "Epoch 126/200\n",
      "1072/1072 [==============================] - 1s 802us/step - loss: 3361.3346 - val_loss: 3377.7624\n",
      "Epoch 127/200\n",
      "1072/1072 [==============================] - 1s 808us/step - loss: 3360.5953 - val_loss: 3379.3581\n",
      "Epoch 128/200\n",
      "1072/1072 [==============================] - 1s 811us/step - loss: 3359.5020 - val_loss: 3379.7217\n",
      "Epoch 129/200\n",
      "1072/1072 [==============================] - 1s 870us/step - loss: 3359.2584 - val_loss: 3384.4680\n",
      "Epoch 130/200\n",
      "1072/1072 [==============================] - 1s 836us/step - loss: 3358.9180 - val_loss: 3382.1812\n",
      "Epoch 131/200\n",
      "1072/1072 [==============================] - 1s 825us/step - loss: 3358.9703 - val_loss: 3379.8375\n",
      "Epoch 132/200\n",
      "1072/1072 [==============================] - 1s 831us/step - loss: 3359.2822 - val_loss: 3379.2051\n",
      "Epoch 133/200\n",
      "1072/1072 [==============================] - 1s 795us/step - loss: 3358.7139 - val_loss: 3380.8107\n",
      "Epoch 134/200\n",
      "1072/1072 [==============================] - 1s 826us/step - loss: 3357.1018 - val_loss: 3385.4112\n",
      "Epoch 135/200\n",
      "1072/1072 [==============================] - 1s 788us/step - loss: 3358.0131 - val_loss: 3382.2809\n",
      "Epoch 136/200\n",
      "1072/1072 [==============================] - 1s 787us/step - loss: 3358.6047 - val_loss: 3381.2242\n",
      "Epoch 137/200\n",
      "1072/1072 [==============================] - 1s 785us/step - loss: 3357.2671 - val_loss: 3379.8715\n",
      "Epoch 138/200\n",
      "1072/1072 [==============================] - 1s 785us/step - loss: 3358.1811 - val_loss: 3378.7906\n",
      "Epoch 139/200\n",
      "1072/1072 [==============================] - 1s 781us/step - loss: 3355.3783 - val_loss: 3379.1992\n",
      "Epoch 140/200\n",
      "1072/1072 [==============================] - 1s 785us/step - loss: 3358.8055 - val_loss: 3379.8308\n",
      "Epoch 141/200\n",
      "1072/1072 [==============================] - 1s 805us/step - loss: 3356.5748 - val_loss: 3379.4728\n",
      "Epoch 142/200\n",
      "1072/1072 [==============================] - 1s 833us/step - loss: 3356.2944 - val_loss: 3377.2825\n",
      "Epoch 143/200\n",
      "1072/1072 [==============================] - 1s 795us/step - loss: 3356.0803 - val_loss: 3381.5940\n",
      "Epoch 144/200\n",
      "1072/1072 [==============================] - 1s 785us/step - loss: 3355.5795 - val_loss: 3378.4728\n",
      "Epoch 145/200\n",
      "1072/1072 [==============================] - 1s 790us/step - loss: 3354.3820 - val_loss: 3377.1064\n",
      "Epoch 146/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072/1072 [==============================] - 1s 784us/step - loss: 3354.7546 - val_loss: 3378.2789\n",
      "Epoch 147/200\n",
      "1072/1072 [==============================] - 1s 791us/step - loss: 3355.0741 - val_loss: 3379.2193\n",
      "Epoch 148/200\n",
      "1072/1072 [==============================] - 1s 806us/step - loss: 3353.2771 - val_loss: 3377.5331\n",
      "Epoch 149/200\n",
      "1072/1072 [==============================] - 1s 794us/step - loss: 3353.5687 - val_loss: 3381.5664\n",
      "Epoch 150/200\n",
      "1072/1072 [==============================] - 1s 891us/step - loss: 3354.4635 - val_loss: 3378.3059\n",
      "Epoch 151/200\n",
      "1072/1072 [==============================] - 1s 813us/step - loss: 3354.0929 - val_loss: 3378.6409\n",
      "Epoch 152/200\n",
      "1072/1072 [==============================] - 1s 804us/step - loss: 3353.0237 - val_loss: 3376.0940\n",
      "Epoch 153/200\n",
      "1072/1072 [==============================] - 1s 851us/step - loss: 3353.1666 - val_loss: 3377.5581\n",
      "Epoch 154/200\n",
      "1072/1072 [==============================] - 1s 842us/step - loss: 3353.4580 - val_loss: 3377.8744\n",
      "Epoch 155/200\n",
      "1072/1072 [==============================] - 1s 873us/step - loss: 3352.6165 - val_loss: 3378.0561\n",
      "Epoch 156/200\n",
      "1072/1072 [==============================] - 1s 810us/step - loss: 3351.1868 - val_loss: 3376.5723\n",
      "Epoch 157/200\n",
      "1072/1072 [==============================] - 1s 811us/step - loss: 3351.5244 - val_loss: 3377.2749\n",
      "Epoch 158/200\n",
      "1072/1072 [==============================] - 1s 802us/step - loss: 3351.8240 - val_loss: 3377.4800\n",
      "Epoch 159/200\n",
      "1072/1072 [==============================] - 1s 799us/step - loss: 3350.2865 - val_loss: 3376.3120\n",
      "Epoch 160/200\n",
      "1072/1072 [==============================] - 1s 792us/step - loss: 3351.8802 - val_loss: 3375.4575\n",
      "Epoch 161/200\n",
      "1072/1072 [==============================] - 1s 790us/step - loss: 3350.7940 - val_loss: 3377.4500\n",
      "Epoch 162/200\n",
      "1072/1072 [==============================] - 1s 804us/step - loss: 3349.7531 - val_loss: 3376.6621\n",
      "Epoch 163/200\n",
      "1072/1072 [==============================] - 1s 795us/step - loss: 3349.5245 - val_loss: 3376.4519\n",
      "Epoch 164/200\n",
      "1072/1072 [==============================] - 1s 815us/step - loss: 3350.1056 - val_loss: 3374.4396\n",
      "Epoch 165/200\n",
      "1072/1072 [==============================] - 1s 827us/step - loss: 3349.8853 - val_loss: 3377.5615\n",
      "Epoch 166/200\n",
      "1072/1072 [==============================] - 1s 803us/step - loss: 3348.9135 - val_loss: 3375.2722\n",
      "Epoch 167/200\n",
      "1072/1072 [==============================] - 1s 794us/step - loss: 3348.5900 - val_loss: 3377.5062\n",
      "Epoch 168/200\n",
      "1072/1072 [==============================] - 1s 790us/step - loss: 3348.3008 - val_loss: 3376.6180\n",
      "Epoch 169/200\n",
      "1072/1072 [==============================] - 1s 796us/step - loss: 3349.5896 - val_loss: 3369.1050\n",
      "Epoch 170/200\n",
      "1072/1072 [==============================] - 1s 810us/step - loss: 3347.3581 - val_loss: 3372.6394\n",
      "Epoch 171/200\n",
      "1072/1072 [==============================] - 1s 802us/step - loss: 3349.1872 - val_loss: 3375.0898\n",
      "Epoch 172/200\n",
      "1072/1072 [==============================] - 1s 791us/step - loss: 3347.6388 - val_loss: 3371.3059\n",
      "Epoch 173/200\n",
      "1072/1072 [==============================] - 1s 784us/step - loss: 3347.0383 - val_loss: 3373.8055\n",
      "Epoch 174/200\n",
      "1072/1072 [==============================] - 1s 785us/step - loss: 3348.8504 - val_loss: 3372.3585\n",
      "Epoch 175/200\n",
      "1072/1072 [==============================] - 1s 824us/step - loss: 3346.9420 - val_loss: 3371.1081\n",
      "Epoch 176/200\n",
      "1072/1072 [==============================] - 1s 808us/step - loss: 3347.2831 - val_loss: 3371.4266\n",
      "Epoch 177/200\n",
      "1072/1072 [==============================] - 1s 787us/step - loss: 3346.2226 - val_loss: 3371.4122\n",
      "Epoch 178/200\n",
      "1072/1072 [==============================] - 1s 806us/step - loss: 3348.2064 - val_loss: 3372.9756\n",
      "Epoch 179/200\n",
      "1072/1072 [==============================] - 1s 796us/step - loss: 3345.7891 - val_loss: 3372.0893\n",
      "Epoch 180/200\n",
      "1072/1072 [==============================] - 1s 803us/step - loss: 3346.7195 - val_loss: 3371.0005\n",
      "Epoch 181/200\n",
      "1072/1072 [==============================] - 1s 792us/step - loss: 3345.5865 - val_loss: 3373.7164\n",
      "Epoch 182/200\n",
      "1072/1072 [==============================] - 1s 799us/step - loss: 3346.2964 - val_loss: 3372.6981\n",
      "Epoch 183/200\n",
      "1072/1072 [==============================] - 1s 859us/step - loss: 3344.9180 - val_loss: 3373.1550\n",
      "Epoch 184/200\n",
      "1072/1072 [==============================] - 1s 827us/step - loss: 3344.6114 - val_loss: 3370.1864\n",
      "Epoch 185/200\n",
      "1072/1072 [==============================] - 1s 839us/step - loss: 3346.2692 - val_loss: 3371.4773\n",
      "Epoch 186/200\n",
      "1072/1072 [==============================] - 1s 802us/step - loss: 3345.5296 - val_loss: 3369.3338\n",
      "Epoch 187/200\n",
      "1072/1072 [==============================] - 1s 793us/step - loss: 3344.5950 - val_loss: 3372.4552\n",
      "Epoch 188/200\n",
      "1072/1072 [==============================] - 1s 796us/step - loss: 3344.7138 - val_loss: 3371.0697\n",
      "Epoch 189/200\n",
      "1072/1072 [==============================] - 1s 796us/step - loss: 3344.9033 - val_loss: 3368.4029\n",
      "Epoch 190/200\n",
      "1072/1072 [==============================] - 1s 791us/step - loss: 3345.0011 - val_loss: 3369.0536\n",
      "Epoch 191/200\n",
      "1072/1072 [==============================] - 1s 795us/step - loss: 3343.4790 - val_loss: 3369.2664\n",
      "Epoch 192/200\n",
      "1072/1072 [==============================] - 1s 789us/step - loss: 3343.4057 - val_loss: 3371.8507\n",
      "Epoch 193/200\n",
      "1072/1072 [==============================] - 1s 801us/step - loss: 3344.1691 - val_loss: 3367.9924\n",
      "Epoch 194/200\n",
      "1072/1072 [==============================] - 1s 783us/step - loss: 3344.6534 - val_loss: 3372.0119\n",
      "Epoch 195/200\n",
      "1072/1072 [==============================] - 1s 794us/step - loss: 3342.6670 - val_loss: 3370.7307\n",
      "Epoch 196/200\n",
      "1072/1072 [==============================] - 1s 831us/step - loss: 3342.5589 - val_loss: 3368.6412\n",
      "Epoch 197/200\n",
      "1072/1072 [==============================] - 1s 796us/step - loss: 3343.8455 - val_loss: 3371.1440\n",
      "Epoch 198/200\n",
      "1072/1072 [==============================] - 1s 785us/step - loss: 3344.1660 - val_loss: 3370.9734\n",
      "Epoch 199/200\n",
      "1072/1072 [==============================] - 1s 792us/step - loss: 3342.5678 - val_loss: 3367.0695\n",
      "Epoch 200/200\n",
      "1072/1072 [==============================] - 1s 785us/step - loss: 3343.2902 - val_loss: 3369.6564\n",
      "Number of genes in low expression group is (60, 5548)\n",
      "Number of gene in high expression group is (60, 5548)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes in low expression group is (60, 5548)\n",
      "Number of gene in high expression group is (60, 5548)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/Documents/Repos/Pseudomonas_latent_spaces/scripts/functions/plot.py:46: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  X_sorted = X.loc[sorted_id.index].dropna()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_label() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7f56f3600964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Plot prediction per sample along gradient of PA1673 expression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mviz_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_corr_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Repos/Pseudomonas_latent_spaces/scripts/functions/plot.py\u001b[0m in \u001b[0;36mplot_corr_gradient\u001b[0;34m(out_dir, viz_dir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pearson correlation score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Sorted samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gene space correlation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Latent space correlation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: set_label() takes 2 positional arguments but 3 were given"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f422b1a27b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre-process input\n",
    "data_dir = os.path.join(base_dirs[0], analysis_name)\n",
    "generate_input.generate_input_PA1673_gradient(data_dir)\n",
    "\n",
    "# Run Tybalt\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "epochs = 200\n",
    "kappa = 0.01\n",
    "intermediate_dim = 100\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "vae.tybalt_2layer_model(learning_rate, batch_size, epochs, kappa, intermediate_dim, latent_dim, epsilon_std, base_dir, analysis_name)\n",
    "\n",
    "\n",
    "# Define offset vectors in gene space and latent space\n",
    "data_dir = os.path.join(base_dirs[0], analysis_name)\n",
    "target_gene = \"PA1673\"\n",
    "percent_low = 5\n",
    "percent_high = 95\n",
    "\n",
    "def_offset.gene_space_offset(data_dir, target_gene, percent_low, percent_high)\n",
    "\n",
    "model_dir = os.path.join(base_dirs[2], analysis_name)\n",
    "encoded_dir = os.path.join(base_dirs[1], analysis_name)\n",
    "\n",
    "def_offset.latent_space_offset(data_dir, model_dir, encoded_dir, target_gene, percent_low, percent_high)\n",
    "\n",
    "\n",
    "# Predict gene expression using offset in gene space and latent space\n",
    "out_dir = os.path.join(base_dirs[3], analysis_name)\n",
    "\n",
    "interpolate.interpolate_in_gene_space(data_dir, target_gene, out_dir, percent_low, percent_high)\n",
    "interpolate.interpolate_in_latent_space(data_dir, model_dir, encoded_dir, target_gene, out_dir, percent_low, percent_high)\n",
    "\n",
    "\n",
    "# Plot prediction per sample along gradient of PA1673 expression\n",
    "viz_dir = os.path.join(base_dirs[5], analysis_name)\n",
    "plot.plot_corr_gradient(out_dir, viz_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pa]",
   "language": "python",
   "name": "conda-env-Pa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
