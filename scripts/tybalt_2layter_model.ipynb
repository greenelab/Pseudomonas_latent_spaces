{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with multiple hidden layers\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train_model_input.txt.xz\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0, header=0, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "intermediate_dim = 100\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}_train_stats.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}_train_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"tybalt_2layer_{}_train_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}_train_encoder_model.h5\".format(latent_dim))\n",
    "weights_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}_train_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}_train_decoder_model.h5\".format(latent_dim))\n",
    "weights_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}_train_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:74: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "hidden_dense_linear = Dense(intermediate_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "hidden_dense_batchnorm = BatchNormalization()(hidden_dense_linear)\n",
    "hidden_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "# Note:\n",
    "# Normalize and relu filter at each layer adds non-linear component (relu is non-linear function)\n",
    "# If architecture is layer-layer-normalization-relu then the computation is still linear\n",
    "# Add additional layers in triplicate\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_model = Sequential()\n",
    "decoder_model.add(Dense(intermediate_dim, activation='relu', input_dim=latent_dim))\n",
    "decoder_model.add(Dense(original_dim, activation='sigmoid'))\n",
    "rnaseq_reconstruct = decoder_model(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1065 samples, validate on 118 samples\n",
      "Epoch 1/100\n",
      "1065/1065 [==============================] - 1s 774us/step - loss: 3683.9162 - val_loss: 3567.3376\n",
      "Epoch 2/100\n",
      "1065/1065 [==============================] - 0s 443us/step - loss: 3521.4820 - val_loss: 3628.6087\n",
      "Epoch 3/100\n",
      "1065/1065 [==============================] - 0s 462us/step - loss: 3491.2961 - val_loss: 3585.6456\n",
      "Epoch 4/100\n",
      "1065/1065 [==============================] - 0s 431us/step - loss: 3478.6882 - val_loss: 3522.5681\n",
      "Epoch 5/100\n",
      "1065/1065 [==============================] - 1s 516us/step - loss: 3471.1970 - val_loss: 3484.6561\n",
      "Epoch 6/100\n",
      "1065/1065 [==============================] - 0s 434us/step - loss: 3467.0834 - val_loss: 3472.3272\n",
      "Epoch 7/100\n",
      "1065/1065 [==============================] - 0s 412us/step - loss: 3458.0686 - val_loss: 3460.7770\n",
      "Epoch 8/100\n",
      "1065/1065 [==============================] - 0s 403us/step - loss: 3454.0912 - val_loss: 3467.6468\n",
      "Epoch 9/100\n",
      "1065/1065 [==============================] - 0s 413us/step - loss: 3452.0206 - val_loss: 3462.9363\n",
      "Epoch 10/100\n",
      "1065/1065 [==============================] - 0s 404us/step - loss: 3449.3535 - val_loss: 3450.8427\n",
      "Epoch 11/100\n",
      "1065/1065 [==============================] - 0s 411us/step - loss: 3446.0053 - val_loss: 3451.4104\n",
      "Epoch 12/100\n",
      "1065/1065 [==============================] - 0s 395us/step - loss: 3444.1638 - val_loss: 3452.6434\n",
      "Epoch 13/100\n",
      "1065/1065 [==============================] - 0s 392us/step - loss: 3438.9491 - val_loss: 3443.9289\n",
      "Epoch 14/100\n",
      "1065/1065 [==============================] - 0s 434us/step - loss: 3433.7169 - val_loss: 3441.5748\n",
      "Epoch 15/100\n",
      "1065/1065 [==============================] - 0s 432us/step - loss: 3430.5147 - val_loss: 3446.2103\n",
      "Epoch 16/100\n",
      "1065/1065 [==============================] - 0s 455us/step - loss: 3431.3612 - val_loss: 3433.0930\n",
      "Epoch 17/100\n",
      "1065/1065 [==============================] - 0s 463us/step - loss: 3426.6937 - val_loss: 3433.8508\n",
      "Epoch 18/100\n",
      "1065/1065 [==============================] - 0s 431us/step - loss: 3424.2228 - val_loss: 3438.8182\n",
      "Epoch 19/100\n",
      "1065/1065 [==============================] - 0s 397us/step - loss: 3424.6078 - val_loss: 3427.3496\n",
      "Epoch 20/100\n",
      "1065/1065 [==============================] - 0s 398us/step - loss: 3418.5806 - val_loss: 3432.0595\n",
      "Epoch 21/100\n",
      "1065/1065 [==============================] - 0s 397us/step - loss: 3418.6170 - val_loss: 3425.3668\n",
      "Epoch 22/100\n",
      "1065/1065 [==============================] - 0s 396us/step - loss: 3419.5805 - val_loss: 3421.6999\n",
      "Epoch 23/100\n",
      "1065/1065 [==============================] - 0s 421us/step - loss: 3414.8957 - val_loss: 3421.2334\n",
      "Epoch 24/100\n",
      "1065/1065 [==============================] - 0s 423us/step - loss: 3412.1748 - val_loss: 3421.2986\n",
      "Epoch 25/100\n",
      "1065/1065 [==============================] - 0s 428us/step - loss: 3409.6554 - val_loss: 3422.5921\n",
      "Epoch 26/100\n",
      "1065/1065 [==============================] - 0s 438us/step - loss: 3410.6627 - val_loss: 3416.2345\n",
      "Epoch 27/100\n",
      "1065/1065 [==============================] - 0s 462us/step - loss: 3406.6922 - val_loss: 3413.9933\n",
      "Epoch 28/100\n",
      "1065/1065 [==============================] - 1s 472us/step - loss: 3407.1947 - val_loss: 3414.7143\n",
      "Epoch 29/100\n",
      "1065/1065 [==============================] - 0s 431us/step - loss: 3402.7795 - val_loss: 3411.7223\n",
      "Epoch 30/100\n",
      "1065/1065 [==============================] - 0s 416us/step - loss: 3402.7758 - val_loss: 3410.0226\n",
      "Epoch 31/100\n",
      "1065/1065 [==============================] - 0s 414us/step - loss: 3401.5435 - val_loss: 3409.5198\n",
      "Epoch 32/100\n",
      "1065/1065 [==============================] - 0s 411us/step - loss: 3400.5676 - val_loss: 3406.4132\n",
      "Epoch 33/100\n",
      "1065/1065 [==============================] - 0s 435us/step - loss: 3400.7146 - val_loss: 3409.7408\n",
      "Epoch 34/100\n",
      "1065/1065 [==============================] - 0s 404us/step - loss: 3400.3197 - val_loss: 3408.4852\n",
      "Epoch 35/100\n",
      "1065/1065 [==============================] - 0s 401us/step - loss: 3397.5459 - val_loss: 3402.8762\n",
      "Epoch 36/100\n",
      "1065/1065 [==============================] - 0s 409us/step - loss: 3395.9553 - val_loss: 3398.7155\n",
      "Epoch 37/100\n",
      "1065/1065 [==============================] - 0s 398us/step - loss: 3395.1322 - val_loss: 3403.3269\n",
      "Epoch 38/100\n",
      "1065/1065 [==============================] - 0s 413us/step - loss: 3392.9595 - val_loss: 3401.7874\n",
      "Epoch 39/100\n",
      "1065/1065 [==============================] - 0s 457us/step - loss: 3391.9107 - val_loss: 3399.0284\n",
      "Epoch 40/100\n",
      "1065/1065 [==============================] - 0s 426us/step - loss: 3391.0265 - val_loss: 3401.0193\n",
      "Epoch 41/100\n",
      "1065/1065 [==============================] - 0s 434us/step - loss: 3391.2243 - val_loss: 3401.5625\n",
      "Epoch 42/100\n",
      "1065/1065 [==============================] - 0s 456us/step - loss: 3388.7210 - val_loss: 3396.8556\n",
      "Epoch 43/100\n",
      "1065/1065 [==============================] - 0s 403us/step - loss: 3387.8856 - val_loss: 3402.9606\n",
      "Epoch 44/100\n",
      "1065/1065 [==============================] - 0s 453us/step - loss: 3388.1416 - val_loss: 3400.4970\n",
      "Epoch 45/100\n",
      "1065/1065 [==============================] - 0s 408us/step - loss: 3388.0545 - val_loss: 3399.3357\n",
      "Epoch 46/100\n",
      "1065/1065 [==============================] - 0s 397us/step - loss: 3386.3539 - val_loss: 3395.6919\n",
      "Epoch 47/100\n",
      "1065/1065 [==============================] - 0s 395us/step - loss: 3385.1615 - val_loss: 3392.9618\n",
      "Epoch 48/100\n",
      "1065/1065 [==============================] - 0s 393us/step - loss: 3384.8785 - val_loss: 3391.0949\n",
      "Epoch 49/100\n",
      "1065/1065 [==============================] - 0s 398us/step - loss: 3383.9524 - val_loss: 3393.0222\n",
      "Epoch 50/100\n",
      "1065/1065 [==============================] - 0s 398us/step - loss: 3382.3067 - val_loss: 3396.2219\n",
      "Epoch 51/100\n",
      "1065/1065 [==============================] - 0s 407us/step - loss: 3384.2848 - val_loss: 3394.3807\n",
      "Epoch 52/100\n",
      "1065/1065 [==============================] - 0s 436us/step - loss: 3382.0195 - val_loss: 3388.4002\n",
      "Epoch 53/100\n",
      "1065/1065 [==============================] - 0s 416us/step - loss: 3379.1379 - val_loss: 3385.8911\n",
      "Epoch 54/100\n",
      "1065/1065 [==============================] - 0s 423us/step - loss: 3378.3996 - val_loss: 3390.8481\n",
      "Epoch 55/100\n",
      "1065/1065 [==============================] - 0s 396us/step - loss: 3379.7697 - val_loss: 3389.9890\n",
      "Epoch 56/100\n",
      "1065/1065 [==============================] - 0s 392us/step - loss: 3378.5482 - val_loss: 3386.7007\n",
      "Epoch 57/100\n",
      "1065/1065 [==============================] - 0s 404us/step - loss: 3376.3014 - val_loss: 3386.4374\n",
      "Epoch 58/100\n",
      "1065/1065 [==============================] - 0s 398us/step - loss: 3377.1205 - val_loss: 3386.9902\n",
      "Epoch 59/100\n",
      "1065/1065 [==============================] - 0s 393us/step - loss: 3375.6725 - val_loss: 3389.3434\n",
      "Epoch 60/100\n",
      "1065/1065 [==============================] - 0s 393us/step - loss: 3374.6807 - val_loss: 3388.0418\n",
      "Epoch 61/100\n",
      "1065/1065 [==============================] - 0s 401us/step - loss: 3374.6271 - val_loss: 3385.6365\n",
      "Epoch 62/100\n",
      "1065/1065 [==============================] - 0s 409us/step - loss: 3374.9069 - val_loss: 3382.2018\n",
      "Epoch 63/100\n",
      "1065/1065 [==============================] - 0s 416us/step - loss: 3372.9369 - val_loss: 3384.5084\n",
      "Epoch 64/100\n",
      "1065/1065 [==============================] - 0s 441us/step - loss: 3371.1655 - val_loss: 3382.1829\n",
      "Epoch 65/100\n",
      "1065/1065 [==============================] - 0s 410us/step - loss: 3370.7016 - val_loss: 3382.3370\n",
      "Epoch 66/100\n",
      "1065/1065 [==============================] - 0s 432us/step - loss: 3371.5246 - val_loss: 3378.4708\n",
      "Epoch 67/100\n",
      "1065/1065 [==============================] - 0s 457us/step - loss: 3370.2317 - val_loss: 3382.0105\n",
      "Epoch 68/100\n",
      "1065/1065 [==============================] - 0s 436us/step - loss: 3370.4371 - val_loss: 3380.1712\n",
      "Epoch 69/100\n",
      "1065/1065 [==============================] - 0s 420us/step - loss: 3369.4089 - val_loss: 3377.4075\n",
      "Epoch 70/100\n",
      "1065/1065 [==============================] - 0s 402us/step - loss: 3368.8340 - val_loss: 3375.4327\n",
      "Epoch 71/100\n",
      "1065/1065 [==============================] - 0s 403us/step - loss: 3368.5050 - val_loss: 3378.3653\n",
      "Epoch 72/100\n",
      "1065/1065 [==============================] - 0s 394us/step - loss: 3367.1012 - val_loss: 3375.7918\n",
      "Epoch 73/100\n",
      "1065/1065 [==============================] - 0s 394us/step - loss: 3365.5839 - val_loss: 3374.2562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "1065/1065 [==============================] - 0s 394us/step - loss: 3365.5413 - val_loss: 3377.2995\n",
      "Epoch 75/100\n",
      "1065/1065 [==============================] - 0s 393us/step - loss: 3365.3432 - val_loss: 3375.9489\n",
      "Epoch 76/100\n",
      "1065/1065 [==============================] - 0s 399us/step - loss: 3366.6964 - val_loss: 3375.8798\n",
      "Epoch 77/100\n",
      "1065/1065 [==============================] - 0s 433us/step - loss: 3363.6296 - val_loss: 3371.3105\n",
      "Epoch 78/100\n",
      "1065/1065 [==============================] - 0s 436us/step - loss: 3364.8508 - val_loss: 3374.9116\n",
      "Epoch 79/100\n",
      "1065/1065 [==============================] - 0s 400us/step - loss: 3365.8261 - val_loss: 3375.8945\n",
      "Epoch 80/100\n",
      "1065/1065 [==============================] - 0s 410us/step - loss: 3363.7371 - val_loss: 3377.7522\n",
      "Epoch 81/100\n",
      "1065/1065 [==============================] - 0s 413us/step - loss: 3362.1235 - val_loss: 3371.4330\n",
      "Epoch 82/100\n",
      "1065/1065 [==============================] - 0s 395us/step - loss: 3362.2362 - val_loss: 3372.4925\n",
      "Epoch 83/100\n",
      "1065/1065 [==============================] - 0s 396us/step - loss: 3361.1914 - val_loss: 3372.8371\n",
      "Epoch 84/100\n",
      "1065/1065 [==============================] - 0s 400us/step - loss: 3361.3888 - val_loss: 3372.1608\n",
      "Epoch 85/100\n",
      "1065/1065 [==============================] - 0s 394us/step - loss: 3361.9658 - val_loss: 3374.3672\n",
      "Epoch 86/100\n",
      "1065/1065 [==============================] - 0s 398us/step - loss: 3362.1938 - val_loss: 3370.4881\n",
      "Epoch 87/100\n",
      "1065/1065 [==============================] - 0s 411us/step - loss: 3359.5848 - val_loss: 3370.1750\n",
      "Epoch 88/100\n",
      "1065/1065 [==============================] - 0s 449us/step - loss: 3360.3642 - val_loss: 3373.8864\n",
      "Epoch 89/100\n",
      "1065/1065 [==============================] - 0s 432us/step - loss: 3359.0104 - val_loss: 3368.5945\n",
      "Epoch 90/100\n",
      "1065/1065 [==============================] - 0s 419us/step - loss: 3359.3627 - val_loss: 3369.0492\n",
      "Epoch 91/100\n",
      "1065/1065 [==============================] - 0s 453us/step - loss: 3359.2141 - val_loss: 3372.2732\n",
      "Epoch 92/100\n",
      "1065/1065 [==============================] - 0s 440us/step - loss: 3358.0972 - val_loss: 3370.9899\n",
      "Epoch 93/100\n",
      "1065/1065 [==============================] - 0s 418us/step - loss: 3357.4373 - val_loss: 3369.1642\n",
      "Epoch 94/100\n",
      "1065/1065 [==============================] - 0s 399us/step - loss: 3358.2568 - val_loss: 3369.6417\n",
      "Epoch 95/100\n",
      "1065/1065 [==============================] - 0s 401us/step - loss: 3356.8609 - val_loss: 3366.5685\n",
      "Epoch 96/100\n",
      "1065/1065 [==============================] - 0s 400us/step - loss: 3357.9604 - val_loss: 3365.7063\n",
      "Epoch 97/100\n",
      "1065/1065 [==============================] - 0s 391us/step - loss: 3356.8349 - val_loss: 3366.4060\n",
      "Epoch 98/100\n",
      "1065/1065 [==============================] - 0s 397us/step - loss: 3355.3569 - val_loss: 3365.1381\n",
      "Epoch 99/100\n",
      "1065/1065 [==============================] - ETA: 0s - loss: 3355.60 - 0s 419us/step - loss: 3355.2480 - val_loss: 3366.5263\n",
      "Epoch 100/100\n",
      "1065/1065 [==============================] - 0s 397us/step - loss: 3353.5161 - val_loss: 3367.1658\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4499643908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_model(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "decoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
