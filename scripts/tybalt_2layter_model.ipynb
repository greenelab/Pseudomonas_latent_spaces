{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with 2-hidden layers\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# To ensure reproducibility using Keras during development\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "randomState = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"train_model_input_anr.txt.xz\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0, header=0, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "intermediate_dim = 100\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}latent_stats.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}latent_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"train_input_2layer_{}latent_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_encoder_model.h5\".format(latent_dim))\n",
    "weights_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_decoder_model.h5\".format(latent_dim))\n",
    "weights_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent, random_state = randomState)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:74: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "hidden_dense_linear = Dense(intermediate_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "hidden_dense_batchnorm = BatchNormalization()(hidden_dense_linear)\n",
    "hidden_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "# Note:\n",
    "# Normalize and relu filter at each layer adds non-linear component (relu is non-linear function)\n",
    "# If architecture is layer-layer-normalization-relu then the computation is still linear\n",
    "# Add additional layers in triplicate\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_model = Sequential()\n",
    "decoder_model.add(Dense(intermediate_dim, activation='relu', input_dim=latent_dim))\n",
    "decoder_model.add(Dense(original_dim, activation='sigmoid'))\n",
    "rnaseq_reconstruct = decoder_model(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1061 samples, validate on 118 samples\n",
      "Epoch 1/100\n",
      "1061/1061 [==============================] - 1s 789us/step - loss: 3687.8007 - val_loss: 3570.3414\n",
      "Epoch 2/100\n",
      "1061/1061 [==============================] - 0s 456us/step - loss: 3525.4637 - val_loss: 3563.1208\n",
      "Epoch 3/100\n",
      "1061/1061 [==============================] - 0s 429us/step - loss: 3498.5362 - val_loss: 3582.2366\n",
      "Epoch 4/100\n",
      "1061/1061 [==============================] - 0s 430us/step - loss: 3488.5820 - val_loss: 3476.6426\n",
      "Epoch 5/100\n",
      "1061/1061 [==============================] - 0s 424us/step - loss: 3478.4819 - val_loss: 3490.4569\n",
      "Epoch 6/100\n",
      "1061/1061 [==============================] - 0s 430us/step - loss: 3470.2805 - val_loss: 3477.5594\n",
      "Epoch 7/100\n",
      "1061/1061 [==============================] - 0s 466us/step - loss: 3466.0273 - val_loss: 3470.7361\n",
      "Epoch 8/100\n",
      "1061/1061 [==============================] - 0s 458us/step - loss: 3462.9224 - val_loss: 3451.9954\n",
      "Epoch 9/100\n",
      "1061/1061 [==============================] - 0s 453us/step - loss: 3455.4269 - val_loss: 3450.0622\n",
      "Epoch 10/100\n",
      "1061/1061 [==============================] - 0s 445us/step - loss: 3453.4267 - val_loss: 3437.5804\n",
      "Epoch 11/100\n",
      "1061/1061 [==============================] - 0s 440us/step - loss: 3450.3461 - val_loss: 3434.0983\n",
      "Epoch 12/100\n",
      "1061/1061 [==============================] - 0s 437us/step - loss: 3446.4410 - val_loss: 3420.1984\n",
      "Epoch 13/100\n",
      "1061/1061 [==============================] - 0s 439us/step - loss: 3442.0231 - val_loss: 3431.0597\n",
      "Epoch 14/100\n",
      "1061/1061 [==============================] - 0s 429us/step - loss: 3440.2506 - val_loss: 3421.1272\n",
      "Epoch 15/100\n",
      "1061/1061 [==============================] - 0s 436us/step - loss: 3436.8862 - val_loss: 3418.2698\n",
      "Epoch 16/100\n",
      "1061/1061 [==============================] - 0s 439us/step - loss: 3433.6824 - val_loss: 3413.6008\n",
      "Epoch 17/100\n",
      "1061/1061 [==============================] - 0s 435us/step - loss: 3429.3559 - val_loss: 3407.9603\n",
      "Epoch 18/100\n",
      "1061/1061 [==============================] - 0s 433us/step - loss: 3428.2062 - val_loss: 3408.0276\n",
      "Epoch 19/100\n",
      "1061/1061 [==============================] - 0s 461us/step - loss: 3427.0425 - val_loss: 3396.5096\n",
      "Epoch 20/100\n",
      "1061/1061 [==============================] - 1s 488us/step - loss: 3423.9468 - val_loss: 3403.2807\n",
      "Epoch 21/100\n",
      "1061/1061 [==============================] - 0s 471us/step - loss: 3423.2124 - val_loss: 3399.1584\n",
      "Epoch 22/100\n",
      "1061/1061 [==============================] - 1s 481us/step - loss: 3419.5928 - val_loss: 3399.9611\n",
      "Epoch 23/100\n",
      "1061/1061 [==============================] - 0s 435us/step - loss: 3418.6435 - val_loss: 3394.1477\n",
      "Epoch 24/100\n",
      "1061/1061 [==============================] - 0s 438us/step - loss: 3416.1637 - val_loss: 3393.6375\n",
      "Epoch 25/100\n",
      "1061/1061 [==============================] - 0s 447us/step - loss: 3413.5178 - val_loss: 3388.4807\n",
      "Epoch 26/100\n",
      "1061/1061 [==============================] - 0s 444us/step - loss: 3414.2506 - val_loss: 3395.4215\n",
      "Epoch 27/100\n",
      "1061/1061 [==============================] - 0s 440us/step - loss: 3412.2935 - val_loss: 3387.4086\n",
      "Epoch 28/100\n",
      "1061/1061 [==============================] - 0s 440us/step - loss: 3409.1488 - val_loss: 3380.6375\n",
      "Epoch 29/100\n",
      "1061/1061 [==============================] - 0s 453us/step - loss: 3408.0829 - val_loss: 3383.3123\n",
      "Epoch 30/100\n",
      "1061/1061 [==============================] - 0s 455us/step - loss: 3406.2016 - val_loss: 3381.9672\n",
      "Epoch 31/100\n",
      "1061/1061 [==============================] - 0s 449us/step - loss: 3405.7429 - val_loss: 3385.6064\n",
      "Epoch 32/100\n",
      "1061/1061 [==============================] - 0s 452us/step - loss: 3404.9503 - val_loss: 3374.6390\n",
      "Epoch 33/100\n",
      "1061/1061 [==============================] - 0s 444us/step - loss: 3403.2227 - val_loss: 3378.4516\n",
      "Epoch 34/100\n",
      "1061/1061 [==============================] - 0s 447us/step - loss: 3398.9818 - val_loss: 3377.8032\n",
      "Epoch 35/100\n",
      "1061/1061 [==============================] - 0s 448us/step - loss: 3398.9302 - val_loss: 3375.1075\n",
      "Epoch 36/100\n",
      "1061/1061 [==============================] - 0s 447us/step - loss: 3400.0854 - val_loss: 3370.3508\n",
      "Epoch 37/100\n",
      "1061/1061 [==============================] - 0s 447us/step - loss: 3399.1749 - val_loss: 3370.0886\n",
      "Epoch 38/100\n",
      "1061/1061 [==============================] - 0s 446us/step - loss: 3397.8437 - val_loss: 3371.9023\n",
      "Epoch 39/100\n",
      "1061/1061 [==============================] - 0s 444us/step - loss: 3397.6965 - val_loss: 3370.1140\n",
      "Epoch 40/100\n",
      "1061/1061 [==============================] - 0s 462us/step - loss: 3398.9206 - val_loss: 3371.9279\n",
      "Epoch 41/100\n",
      "1061/1061 [==============================] - 0s 447us/step - loss: 3394.5253 - val_loss: 3368.8026\n",
      "Epoch 42/100\n",
      "1061/1061 [==============================] - 0s 440us/step - loss: 3395.3265 - val_loss: 3369.9677\n",
      "Epoch 43/100\n",
      "1061/1061 [==============================] - 0s 452us/step - loss: 3393.4145 - val_loss: 3365.2090\n",
      "Epoch 44/100\n",
      "1061/1061 [==============================] - 1s 474us/step - loss: 3392.4509 - val_loss: 3366.7874\n",
      "Epoch 45/100\n",
      "1061/1061 [==============================] - 1s 489us/step - loss: 3390.6617 - val_loss: 3367.5062\n",
      "Epoch 46/100\n",
      "1061/1061 [==============================] - 0s 458us/step - loss: 3392.0466 - val_loss: 3368.2384\n",
      "Epoch 47/100\n",
      "1061/1061 [==============================] - 0s 455us/step - loss: 3388.1137 - val_loss: 3361.8872\n",
      "Epoch 48/100\n",
      "1061/1061 [==============================] - 0s 458us/step - loss: 3389.1489 - val_loss: 3368.6040\n",
      "Epoch 49/100\n",
      "1061/1061 [==============================] - 0s 449us/step - loss: 3386.4287 - val_loss: 3363.5044\n",
      "Epoch 50/100\n",
      "1061/1061 [==============================] - 0s 444us/step - loss: 3388.8370 - val_loss: 3361.0008\n",
      "Epoch 51/100\n",
      "1061/1061 [==============================] - 0s 440us/step - loss: 3386.3749 - val_loss: 3361.9463\n",
      "Epoch 52/100\n",
      "1061/1061 [==============================] - 0s 442us/step - loss: 3386.8886 - val_loss: 3363.3790\n",
      "Epoch 53/100\n",
      "1061/1061 [==============================] - 0s 450us/step - loss: 3383.9680 - val_loss: 3360.6203\n",
      "Epoch 54/100\n",
      "1061/1061 [==============================] - 1s 498us/step - loss: 3385.5101 - val_loss: 3360.6191\n",
      "Epoch 55/100\n",
      "1061/1061 [==============================] - 0s 462us/step - loss: 3383.4066 - val_loss: 3363.8203\n",
      "Epoch 56/100\n",
      "1061/1061 [==============================] - 0s 459us/step - loss: 3382.3292 - val_loss: 3358.2142\n",
      "Epoch 57/100\n",
      "1061/1061 [==============================] - 1s 501us/step - loss: 3381.5257 - val_loss: 3355.5501\n",
      "Epoch 58/100\n",
      "1061/1061 [==============================] - 0s 468us/step - loss: 3379.6348 - val_loss: 3356.6661\n",
      "Epoch 59/100\n",
      "1061/1061 [==============================] - 1s 474us/step - loss: 3380.2314 - val_loss: 3359.2763\n",
      "Epoch 60/100\n",
      "1061/1061 [==============================] - 0s 440us/step - loss: 3378.8508 - val_loss: 3357.3608\n",
      "Epoch 61/100\n",
      "1061/1061 [==============================] - 0s 436us/step - loss: 3379.5484 - val_loss: 3352.7894\n",
      "Epoch 62/100\n",
      "1061/1061 [==============================] - 0s 437us/step - loss: 3377.4432 - val_loss: 3355.9360\n",
      "Epoch 63/100\n",
      "1061/1061 [==============================] - 0s 444us/step - loss: 3378.8390 - val_loss: 3350.3478\n",
      "Epoch 64/100\n",
      "1061/1061 [==============================] - 0s 445us/step - loss: 3377.3321 - val_loss: 3354.2620\n",
      "Epoch 65/100\n",
      "1061/1061 [==============================] - 0s 434us/step - loss: 3377.1184 - val_loss: 3353.5545\n",
      "Epoch 66/100\n",
      "1061/1061 [==============================] - 0s 456us/step - loss: 3376.7630 - val_loss: 3350.3091\n",
      "Epoch 67/100\n",
      "1061/1061 [==============================] - 0s 446us/step - loss: 3374.8862 - val_loss: 3350.0608\n",
      "Epoch 68/100\n",
      "1061/1061 [==============================] - 0s 442us/step - loss: 3373.2936 - val_loss: 3348.0074\n",
      "Epoch 69/100\n",
      "1061/1061 [==============================] - 0s 434us/step - loss: 3372.0935 - val_loss: 3348.1412\n",
      "Epoch 70/100\n",
      "1061/1061 [==============================] - 0s 439us/step - loss: 3372.8395 - val_loss: 3350.4288\n",
      "Epoch 71/100\n",
      "1061/1061 [==============================] - 0s 468us/step - loss: 3371.1048 - val_loss: 3348.7451\n",
      "Epoch 72/100\n",
      "1061/1061 [==============================] - 1s 575us/step - loss: 3370.9901 - val_loss: 3353.6157\n",
      "Epoch 73/100\n",
      "1061/1061 [==============================] - 0s 441us/step - loss: 3372.4446 - val_loss: 3348.0935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "1061/1061 [==============================] - 0s 431us/step - loss: 3369.8240 - val_loss: 3345.3283\n",
      "Epoch 75/100\n",
      "1061/1061 [==============================] - 0s 422us/step - loss: 3369.1913 - val_loss: 3344.2548\n",
      "Epoch 76/100\n",
      "1061/1061 [==============================] - 0s 425us/step - loss: 3369.5818 - val_loss: 3347.1836\n",
      "Epoch 77/100\n",
      "1061/1061 [==============================] - 0s 423us/step - loss: 3368.3470 - val_loss: 3345.3316\n",
      "Epoch 78/100\n",
      "1061/1061 [==============================] - 0s 423us/step - loss: 3368.1188 - val_loss: 3345.9925\n",
      "Epoch 79/100\n",
      "1061/1061 [==============================] - 0s 426us/step - loss: 3369.6246 - val_loss: 3340.9685\n",
      "Epoch 80/100\n",
      "1061/1061 [==============================] - 0s 437us/step - loss: 3368.7060 - val_loss: 3342.6284\n",
      "Epoch 81/100\n",
      "1061/1061 [==============================] - 0s 468us/step - loss: 3366.8832 - val_loss: 3345.7308\n",
      "Epoch 82/100\n",
      "1061/1061 [==============================] - 0s 458us/step - loss: 3369.5735 - val_loss: 3341.6075\n",
      "Epoch 83/100\n",
      "1061/1061 [==============================] - 0s 433us/step - loss: 3365.0557 - val_loss: 3344.6917\n",
      "Epoch 84/100\n",
      "1061/1061 [==============================] - 0s 426us/step - loss: 3364.8233 - val_loss: 3343.0536\n",
      "Epoch 85/100\n",
      "1061/1061 [==============================] - 0s 427us/step - loss: 3363.3810 - val_loss: 3338.4195\n",
      "Epoch 86/100\n",
      "1061/1061 [==============================] - 0s 425us/step - loss: 3363.9769 - val_loss: 3343.8611\n",
      "Epoch 87/100\n",
      "1061/1061 [==============================] - 0s 426us/step - loss: 3364.2884 - val_loss: 3339.8760\n",
      "Epoch 88/100\n",
      "1061/1061 [==============================] - 0s 422us/step - loss: 3363.8587 - val_loss: 3339.2401\n",
      "Epoch 89/100\n",
      "1061/1061 [==============================] - 0s 433us/step - loss: 3363.1370 - val_loss: 3339.1450\n",
      "Epoch 90/100\n",
      "1061/1061 [==============================] - 0s 466us/step - loss: 3363.8854 - val_loss: 3336.7928\n",
      "Epoch 91/100\n",
      "1061/1061 [==============================] - 1s 472us/step - loss: 3363.0826 - val_loss: 3341.8119\n",
      "Epoch 92/100\n",
      "1061/1061 [==============================] - 0s 443us/step - loss: 3362.0463 - val_loss: 3341.6153\n",
      "Epoch 93/100\n",
      "1061/1061 [==============================] - 0s 458us/step - loss: 3362.5475 - val_loss: 3340.9610\n",
      "Epoch 94/100\n",
      "1061/1061 [==============================] - 0s 449us/step - loss: 3361.4258 - val_loss: 3337.8341\n",
      "Epoch 95/100\n",
      "1061/1061 [==============================] - 0s 454us/step - loss: 3361.0052 - val_loss: 3337.6561\n",
      "Epoch 96/100\n",
      "1061/1061 [==============================] - 0s 453us/step - loss: 3359.3753 - val_loss: 3339.9452\n",
      "Epoch 97/100\n",
      "1061/1061 [==============================] - 0s 427us/step - loss: 3360.1545 - val_loss: 3338.0511\n",
      "Epoch 98/100\n",
      "1061/1061 [==============================] - 0s 430us/step - loss: 3359.6999 - val_loss: 3337.2596\n",
      "Epoch 99/100\n",
      "1061/1061 [==============================] - 0s 423us/step - loss: 3357.4548 - val_loss: 3336.2987\n",
      "Epoch 100/100\n",
      "1061/1061 [==============================] - 0s 427us/step - loss: 3358.9633 - val_loss: 3334.6313\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcae4aab668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_model(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "decoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
