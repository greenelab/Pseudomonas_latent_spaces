{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee (July 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with 2-hidden layers\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# To ensure reproducibility using Keras during development\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "randomState = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Files\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "data_file =  os.path.join(os.path.dirname(os.getcwd()), \"data\", \"cipro_treatment\", \"train_model_input.txt.xz\")\n",
    "rnaseq = pd.read_table(data_file,sep='\\t',index_col=0, header=0, compression='xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "kappa = 0.01\n",
    "\n",
    "original_dim = rnaseq.shape[1]\n",
    "intermediate_dim = 100\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"cipro_treatment\", \"tybalt_2layer_{}latent_stats.csv\".format(latent_dim))\n",
    "hist_plot_file =os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"cipro_treatment\", \"tybalt_2layer_{}latent_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file =os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"cipro_treatment\", \"train_input_2layer_{}latent_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"cipro_treatment\", \"tybalt_2layer_{}latent_encoder_model.h5\".format(latent_dim))\n",
    "weights_encoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"cipro_treatment\", \"tybalt_2layer_{}latent_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"cipro_treatment\", \"tybalt_2layer_{}latent_decoder_model.h5\".format(latent_dim))\n",
    "weights_decoder_file =os.path.join(os.path.dirname(os.getcwd()), \"models\", \"cipro_treatment\", \"tybalt_2layer_{}latent_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "#\n",
    "# Based on publication by Greg et. al. \n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728678/\n",
    "# https://github.com/greenelab/tybalt/blob/master/scripts/vae_pancancer.py\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function for reparameterization trick to make model differentiable\n",
    "def sampling(args):\n",
    "\n",
    "    # Function with args required for Keras Lambda function\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    # Draw epsilon of the same shape from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=tf.shape(z_mean), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "\n",
    "    # The latent vector is non-deterministic and differentiable\n",
    "    # in respect to z_mean and z_log_var\n",
    "    z = z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "    return z\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer that learns and performs the training\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # https://keras.io/layers/writing-your-own-keras-layers/\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * \\\n",
    "                              metrics.binary_crossentropy(x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded -\n",
    "                                K.square(z_mean_encoded) -\n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "\n",
    "\n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # Behavior on each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Data initalizations\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Split 10% test set randomly\n",
    "test_set_percent = 0.1\n",
    "rnaseq_test_df = rnaseq.sample(frac=test_set_percent, random_state = randomState)\n",
    "rnaseq_train_df = rnaseq.drop(rnaseq_test_df.index)\n",
    "\n",
    "# Create a placeholder for an encoded (original-dimensional)\n",
    "rnaseq_input = Input(shape=(original_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:74: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Architecture of VAE\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ENCODER\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "hidden_dense_linear = Dense(intermediate_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "hidden_dense_batchnorm = BatchNormalization()(hidden_dense_linear)\n",
    "hidden_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "# Note:\n",
    "# Normalize and relu filter at each layer adds non-linear component (relu is non-linear function)\n",
    "# If architecture is layer-layer-normalization-relu then the computation is still linear\n",
    "# Add additional layers in triplicate\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "z = Lambda(sampling,\n",
    "           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "\n",
    "# DECODER\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_model = Sequential()\n",
    "decoder_model.add(Dense(intermediate_dim, activation='relu', input_dim=latent_dim))\n",
    "decoder_model.add(Dense(original_dim, activation='sigmoid'))\n",
    "rnaseq_reconstruct = decoder_model(z)\n",
    "\n",
    "\n",
    "# CONNECTIONS\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer()([rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1065 samples, validate on 118 samples\n",
      "Epoch 1/100\n",
      "1065/1065 [==============================] - 1s 1ms/step - loss: 3675.5156 - val_loss: 3567.5889\n",
      "Epoch 2/100\n",
      "1065/1065 [==============================] - 1s 961us/step - loss: 3521.1647 - val_loss: 3602.8569\n",
      "Epoch 3/100\n",
      "1065/1065 [==============================] - 1s 939us/step - loss: 3495.1654 - val_loss: 3596.0941\n",
      "Epoch 4/100\n",
      "1065/1065 [==============================] - 1s 965us/step - loss: 3481.5152 - val_loss: 3507.1283\n",
      "Epoch 5/100\n",
      "1065/1065 [==============================] - 1s 1ms/step - loss: 3474.3220 - val_loss: 3472.4469\n",
      "Epoch 6/100\n",
      "1065/1065 [==============================] - 1s 931us/step - loss: 3469.4837 - val_loss: 3477.3594\n",
      "Epoch 7/100\n",
      "1065/1065 [==============================] - 1s 1ms/step - loss: 3465.1734 - val_loss: 3456.3904\n",
      "Epoch 8/100\n",
      "1065/1065 [==============================] - 1s 1ms/step - loss: 3462.7182 - val_loss: 3445.9832\n",
      "Epoch 9/100\n",
      "1065/1065 [==============================] - 1s 915us/step - loss: 3454.6435 - val_loss: 3449.7283\n",
      "Epoch 10/100\n",
      "1065/1065 [==============================] - 1s 900us/step - loss: 3453.6157 - val_loss: 3446.7951\n",
      "Epoch 11/100\n",
      "1065/1065 [==============================] - 1s 940us/step - loss: 3450.7848 - val_loss: 3433.9846\n",
      "Epoch 12/100\n",
      "1065/1065 [==============================] - 1s 916us/step - loss: 3444.9037 - val_loss: 3439.8921\n",
      "Epoch 13/100\n",
      "1065/1065 [==============================] - 1s 915us/step - loss: 3445.4096 - val_loss: 3432.6797\n",
      "Epoch 14/100\n",
      "1065/1065 [==============================] - 1s 910us/step - loss: 3441.9745 - val_loss: 3432.8082\n",
      "Epoch 15/100\n",
      "1065/1065 [==============================] - 1s 921us/step - loss: 3439.2288 - val_loss: 3426.4071\n",
      "Epoch 16/100\n",
      "1065/1065 [==============================] - 1s 932us/step - loss: 3437.5424 - val_loss: 3433.4216\n",
      "Epoch 17/100\n",
      "1065/1065 [==============================] - 1s 931us/step - loss: 3435.2359 - val_loss: 3426.1902\n",
      "Epoch 18/100\n",
      "1065/1065 [==============================] - 1s 911us/step - loss: 3432.5428 - val_loss: 3426.9563\n",
      "Epoch 19/100\n",
      "1065/1065 [==============================] - 1s 914us/step - loss: 3431.6350 - val_loss: 3415.3958\n",
      "Epoch 20/100\n",
      "1065/1065 [==============================] - 1s 931us/step - loss: 3429.2956 - val_loss: 3422.0045\n",
      "Epoch 21/100\n",
      "1065/1065 [==============================] - 1s 924us/step - loss: 3426.7067 - val_loss: 3419.6156\n",
      "Epoch 22/100\n",
      "1065/1065 [==============================] - 1s 924us/step - loss: 3426.8021 - val_loss: 3416.2103\n",
      "Epoch 23/100\n",
      "1065/1065 [==============================] - 1s 918us/step - loss: 3422.8770 - val_loss: 3412.9994\n",
      "Epoch 24/100\n",
      "1065/1065 [==============================] - 1s 965us/step - loss: 3421.9042 - val_loss: 3412.0251\n",
      "Epoch 25/100\n",
      "1065/1065 [==============================] - 1s 924us/step - loss: 3419.4865 - val_loss: 3411.6493\n",
      "Epoch 26/100\n",
      "1065/1065 [==============================] - 1s 931us/step - loss: 3419.3380 - val_loss: 3406.4178\n",
      "Epoch 27/100\n",
      "1065/1065 [==============================] - 1s 942us/step - loss: 3419.1639 - val_loss: 3407.2740\n",
      "Epoch 28/100\n",
      "1065/1065 [==============================] - 1s 919us/step - loss: 3416.3194 - val_loss: 3405.8680\n",
      "Epoch 29/100\n",
      "1065/1065 [==============================] - 1s 938us/step - loss: 3414.5145 - val_loss: 3408.5655\n",
      "Epoch 30/100\n",
      "1065/1065 [==============================] - 1s 899us/step - loss: 3412.6631 - val_loss: 3402.3327\n",
      "Epoch 31/100\n",
      "1065/1065 [==============================] - 1s 926us/step - loss: 3411.4290 - val_loss: 3400.4578\n",
      "Epoch 32/100\n",
      "1065/1065 [==============================] - 1s 930us/step - loss: 3408.5698 - val_loss: 3399.7070\n",
      "Epoch 33/100\n",
      "1065/1065 [==============================] - 1s 915us/step - loss: 3407.3454 - val_loss: 3400.0879\n",
      "Epoch 34/100\n",
      "1065/1065 [==============================] - 1s 926us/step - loss: 3405.5947 - val_loss: 3397.5420\n",
      "Epoch 35/100\n",
      "1065/1065 [==============================] - 1s 919us/step - loss: 3406.2966 - val_loss: 3398.3642\n",
      "Epoch 36/100\n",
      "1065/1065 [==============================] - 1s 928us/step - loss: 3402.6510 - val_loss: 3394.9674\n",
      "Epoch 37/100\n",
      "1065/1065 [==============================] - 1s 1ms/step - loss: 3402.3263 - val_loss: 3392.6705\n",
      "Epoch 38/100\n",
      "1065/1065 [==============================] - 1s 1ms/step - loss: 3401.9601 - val_loss: 3391.6119\n",
      "Epoch 39/100\n",
      "1065/1065 [==============================] - 1s 894us/step - loss: 3400.7854 - val_loss: 3394.3602\n",
      "Epoch 40/100\n",
      "1065/1065 [==============================] - 1s 917us/step - loss: 3398.6933 - val_loss: 3392.8966\n",
      "Epoch 41/100\n",
      "1065/1065 [==============================] - 1s 931us/step - loss: 3397.4813 - val_loss: 3389.9539\n",
      "Epoch 42/100\n",
      "1065/1065 [==============================] - 1s 937us/step - loss: 3397.9022 - val_loss: 3389.0514\n",
      "Epoch 43/100\n",
      "1065/1065 [==============================] - 1s 915us/step - loss: 3394.8399 - val_loss: 3388.6393\n",
      "Epoch 44/100\n",
      "1065/1065 [==============================] - 1s 917us/step - loss: 3392.4709 - val_loss: 3387.3028\n",
      "Epoch 45/100\n",
      "1065/1065 [==============================] - 1s 902us/step - loss: 3392.0262 - val_loss: 3387.1214\n",
      "Epoch 46/100\n",
      "1065/1065 [==============================] - 1s 923us/step - loss: 3391.8034 - val_loss: 3383.0353\n",
      "Epoch 47/100\n",
      "1065/1065 [==============================] - 1s 949us/step - loss: 3390.5617 - val_loss: 3383.3989\n",
      "Epoch 48/100\n",
      "1065/1065 [==============================] - 1s 908us/step - loss: 3386.7962 - val_loss: 3382.9176\n",
      "Epoch 49/100\n",
      "1065/1065 [==============================] - 1s 922us/step - loss: 3388.2544 - val_loss: 3379.5922\n",
      "Epoch 50/100\n",
      "1065/1065 [==============================] - 1s 961us/step - loss: 3388.3463 - val_loss: 3381.3149\n",
      "Epoch 51/100\n",
      "1065/1065 [==============================] - 1s 916us/step - loss: 3387.0316 - val_loss: 3379.0529\n",
      "Epoch 52/100\n",
      "1065/1065 [==============================] - 1s 939us/step - loss: 3386.2803 - val_loss: 3379.9448\n",
      "Epoch 53/100\n",
      "1065/1065 [==============================] - 1s 943us/step - loss: 3385.2063 - val_loss: 3378.4535\n",
      "Epoch 54/100\n",
      "1065/1065 [==============================] - 1s 937us/step - loss: 3384.0702 - val_loss: 3378.2745\n",
      "Epoch 55/100\n",
      "1065/1065 [==============================] - 1s 908us/step - loss: 3383.7023 - val_loss: 3372.3869\n",
      "Epoch 56/100\n",
      "1065/1065 [==============================] - 1s 909us/step - loss: 3382.4468 - val_loss: 3376.7212\n",
      "Epoch 57/100\n",
      "1065/1065 [==============================] - 1s 933us/step - loss: 3383.3978 - val_loss: 3379.6605\n",
      "Epoch 58/100\n",
      "1065/1065 [==============================] - 1s 929us/step - loss: 3382.3422 - val_loss: 3372.4552\n",
      "Epoch 59/100\n",
      "1065/1065 [==============================] - 1s 905us/step - loss: 3380.5146 - val_loss: 3375.5898\n",
      "Epoch 60/100\n",
      "1065/1065 [==============================] - 1s 937us/step - loss: 3381.7763 - val_loss: 3378.0029\n",
      "Epoch 61/100\n",
      "1065/1065 [==============================] - 1s 929us/step - loss: 3377.5256 - val_loss: 3369.7244\n",
      "Epoch 62/100\n",
      "1065/1065 [==============================] - 1s 982us/step - loss: 3377.7494 - val_loss: 3371.5783\n",
      "Epoch 63/100\n",
      "1065/1065 [==============================] - 1s 915us/step - loss: 3376.8759 - val_loss: 3372.4858\n",
      "Epoch 64/100\n",
      "1065/1065 [==============================] - 1s 915us/step - loss: 3375.5830 - val_loss: 3371.9424\n",
      "Epoch 65/100\n",
      "1065/1065 [==============================] - 1s 900us/step - loss: 3377.6144 - val_loss: 3380.4086\n",
      "Epoch 66/100\n",
      "1065/1065 [==============================] - 1s 922us/step - loss: 3374.1135 - val_loss: 3368.2330\n",
      "Epoch 67/100\n",
      "1065/1065 [==============================] - 1s 934us/step - loss: 3373.7306 - val_loss: 3369.1055\n",
      "Epoch 68/100\n",
      "1065/1065 [==============================] - 1s 922us/step - loss: 3373.6743 - val_loss: 3366.8186\n",
      "Epoch 69/100\n",
      "1065/1065 [==============================] - 1s 920us/step - loss: 3372.4442 - val_loss: 3368.4499\n",
      "Epoch 70/100\n",
      "1065/1065 [==============================] - 1s 917us/step - loss: 3372.8897 - val_loss: 3366.6607\n",
      "Epoch 71/100\n",
      "1065/1065 [==============================] - 1s 970us/step - loss: 3370.9707 - val_loss: 3364.8825\n",
      "Epoch 72/100\n",
      "1065/1065 [==============================] - 1s 936us/step - loss: 3370.7953 - val_loss: 3366.3006\n",
      "Epoch 73/100\n",
      "1065/1065 [==============================] - 1s 918us/step - loss: 3369.9022 - val_loss: 3364.5878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "1065/1065 [==============================] - 1s 892us/step - loss: 3369.9907 - val_loss: 3364.9688\n",
      "Epoch 75/100\n",
      "1065/1065 [==============================] - 1s 895us/step - loss: 3370.1718 - val_loss: 3364.1434\n",
      "Epoch 76/100\n",
      "1065/1065 [==============================] - 1s 929us/step - loss: 3366.7975 - val_loss: 3364.5692\n",
      "Epoch 77/100\n",
      "1065/1065 [==============================] - 1s 994us/step - loss: 3369.5893 - val_loss: 3363.5172\n",
      "Epoch 78/100\n",
      "1065/1065 [==============================] - 1s 929us/step - loss: 3366.9937 - val_loss: 3361.1385\n",
      "Epoch 79/100\n",
      "1065/1065 [==============================] - 1s 912us/step - loss: 3365.8551 - val_loss: 3363.9195\n",
      "Epoch 80/100\n",
      "1065/1065 [==============================] - 1s 912us/step - loss: 3365.5958 - val_loss: 3361.3495\n",
      "Epoch 81/100\n",
      "1065/1065 [==============================] - 1s 916us/step - loss: 3366.8624 - val_loss: 3364.3202\n",
      "Epoch 82/100\n",
      "1065/1065 [==============================] - 1s 941us/step - loss: 3365.3043 - val_loss: 3364.1176\n",
      "Epoch 83/100\n",
      "1065/1065 [==============================] - 1s 920us/step - loss: 3365.4803 - val_loss: 3359.7606\n",
      "Epoch 84/100\n",
      "1065/1065 [==============================] - 1s 945us/step - loss: 3363.3862 - val_loss: 3360.3589\n",
      "Epoch 85/100\n",
      "1065/1065 [==============================] - 1s 915us/step - loss: 3364.3625 - val_loss: 3361.3859\n",
      "Epoch 86/100\n",
      "1065/1065 [==============================] - 1s 923us/step - loss: 3363.6646 - val_loss: 3361.9830\n",
      "Epoch 87/100\n",
      "1065/1065 [==============================] - 1s 932us/step - loss: 3361.9521 - val_loss: 3361.2736\n",
      "Epoch 88/100\n",
      "1065/1065 [==============================] - 1s 908us/step - loss: 3362.0834 - val_loss: 3361.4961\n",
      "Epoch 89/100\n",
      "1065/1065 [==============================] - 1s 915us/step - loss: 3363.7994 - val_loss: 3360.4235\n",
      "Epoch 90/100\n",
      "1065/1065 [==============================] - 1s 975us/step - loss: 3362.4327 - val_loss: 3356.7954\n",
      "Epoch 91/100\n",
      "1065/1065 [==============================] - 1s 1ms/step - loss: 3362.9773 - val_loss: 3359.2820\n",
      "Epoch 92/100\n",
      "1065/1065 [==============================] - 1s 1ms/step - loss: 3360.4947 - val_loss: 3356.7599\n",
      "Epoch 93/100\n",
      "1065/1065 [==============================] - 1s 931us/step - loss: 3361.8962 - val_loss: 3354.3720\n",
      "Epoch 94/100\n",
      "1065/1065 [==============================] - 1s 897us/step - loss: 3361.3335 - val_loss: 3356.7523\n",
      "Epoch 95/100\n",
      "1065/1065 [==============================] - 1s 904us/step - loss: 3360.4036 - val_loss: 3356.7906\n",
      "Epoch 96/100\n",
      "1065/1065 [==============================] - 1s 910us/step - loss: 3357.5453 - val_loss: 3356.4055\n",
      "Epoch 97/100\n",
      "1065/1065 [==============================] - 1s 952us/step - loss: 3358.6798 - val_loss: 3355.3321\n",
      "Epoch 98/100\n",
      "1065/1065 [==============================] - 1s 917us/step - loss: 3357.7419 - val_loss: 3356.2535\n",
      "Epoch 99/100\n",
      "1065/1065 [==============================] - 1s 948us/step - loss: 3358.0565 - val_loss: 3358.6973\n",
      "Epoch 100/100\n",
      "1065/1065 [==============================] - 1s 961us/step - loss: 3359.6090 - val_loss: 3354.5209\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# fit Model\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit(np.array(rnaseq_train_df), shuffle=True, epochs=epochs, batch_size=batch_size,\n",
    "               validation_data=(np.array(rnaseq_test_df), None),\n",
    "               callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Use trained model to make predictions\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)\n",
    "\n",
    "encoded_rnaseq_df = encoder.predict_on_batch(rnaseq)\n",
    "encoded_rnaseq_df = pd.DataFrame(encoded_rnaseq_df, index=rnaseq.index)\n",
    "\n",
    "encoded_rnaseq_df.columns.name = 'sample_id'\n",
    "encoded_rnaseq_df.columns = encoded_rnaseq_df.columns + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3fc167da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Visualize training performance\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Output\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save latent space representation\n",
    "encoded_rnaseq_df.to_csv(encoded_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_model(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "decoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
